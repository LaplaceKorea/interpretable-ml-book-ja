[["index.html", "Interpretable Machine Learning A Guide for Making Black Box Models Explainable. 要約", " Interpretable Machine Learning A Guide for Making Black Box Models Explainable. Christoph Molnar 2021-02-16 要約 機械学習は、製品や処理、研究を改善するための大きな可能性を秘めています。 しかし、コンピュータは通常、予測の説明をしません。これが機械学習を採用する障壁となっています。 本書は、機械学習モデルや、その判断を解釈可能なものにすることについて書かれています。 解釈可能性とは何かを説明した後、決定木、決定規則、線形回帰などの単純で解釈可能なモデルについて学びます。 その後の章では、特徴量の重要度 (feature importance)やALE(accumulated local effects)や、個々の予測を説明するLIMEやシャープレイ値のようなモデルに非依存な手法(model-agnostic methods)について焦点を当てていきます。 すべての解釈手法は、深く説明され、批判的に議論されます。 それらの手法はどのように機能しているのか？ それらの長所と短所は何か？ それらの出力はどのように解釈できるのか？ この本を読めば、機械学習プロジェクトに最も適した解釈手法を選択し、正しく適用できるようになるでしょう。 本書では、テーブルデータ（リレーショナルデータや構造化データとも呼ばれる）の機械学習モデルに焦点を当てており、コンピュータビジョンや自然言語処理のタスクにはあまり焦点を当てていません。 機械学習の実務家、データサイエンティスト、統計学者、その他、機械学習モデルを解釈可能なものとすることに興味のある人は、本書を読むことをお勧めします。 PDFと電子書籍版（epub、mobi）の購入はこちら on leanpub.com. 印刷版の購入はこちら on lulu.com. 著者について: 私の名前はChristoph Molnar、統計学者であり機械学習者です。 私の目標は、機械学習を解釈可能なものにすることです。 Mail：christoph.molnar.ai@gmail.com Website: https://christophm.github.io/ Follow me on Twitter! @ChristophMolnar Cover by @YvonneDoinel クリエイティブ・コモンズ・ライセンス この本のライセンスは以下の通りです。 Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["著者による序文.html", "著者による序文", " 著者による序文 本書は、私が臨床研究で統計学者として働いていたときに、サイドプロジェクトとして始めたものです。 週に4日働き、「休みの日」にはサイドプロジェクトに取り組んでいました。 最終的に解釈可能な機械学習は私のサイドプロジェクトの1つになりました。 最初は本を書くつもりはありませんでした。 その代わり、私は単に解釈可能な機械学習についてもっと理解したいと思っていて、学ぶための良いリソースを探していました。 機械学習の成功と解釈可能性の重要性を考えると、このトピックに関する書籍やチュートリアルはたくさんあるだろうと予想していました。 しかし、私が見つけたのは関連する研究論文と、インターネット上に散らばっているわずかなブログ記事だけで、概要がよくわかるものはありませんでした。 本、チュートリアル、概要の論文は何もありませんでした。 このギャップに触発されて、この本を書き始めました。 私は結局、解釈可能な機械学習の研究を始めたときに利用できることを願っていた本を書くことになりました。 私がこの本を書いた意図は2つあります。それは自分自身のために学ぶためと、この新しい知識を他の人と共有するためです。 私はドイツのミュンヘン大学（LMU Munich）で統計学の学士号と修士号を取得しました。 機械学習に関する私の知識のほとんどは、オンラインコース、コンペティション、サイドプロジェクト、専門的な活動を通じて独学で習得しました。 私の統計学のバックグラウンドは、機械学習、特に解釈可能性の分野に入るための優れた基礎となりました。 統計学では、解釈可能な回帰モデルを構築することに主に焦点を当てています。 統計学の修士号を取得した後、私は博士号を取得しないことにしました。 修士論文を書くのが好きではなかったからです。 書くことがものすごくストレスだったのです。 そこで私は、Fintechのスタートアップ企業でデータサイエンティストとして、また臨床研究の統計学者として仕事をしました。 この3年間の業界での仕事の後、私はこの本を書き始め、数ヶ月後には解釈可能な機械学習の博士号を取得しました。 この本を書き始めることで、書くことの楽しさを取り戻し、研究への情熱をもつことができるようになりました。 本書は、解釈可能な機械学習の多くの手法を網羅しています。 最初の章では、解釈可能性の概念を紹介し、なぜ解釈可能性が必要なのかを動機付けています。 ショートストーリーもあります! この本では、説明の性質の違いや、人間が考える良い説明とは何かを論じています。 そして、本質的に解釈可能な機械学習モデル、例えば回帰モデルや決定木などについて論じます。 この本の主な焦点は、モデル非依存(model-agnostic)な解釈手法です。 モデル非依存な方法とは、どのような機械学習モデルにも適用可能であり、モデルが訓練された後に適用可能です。 モデルの独立は、モデル非依存な手法を非常に柔軟で強力なものにします。 いくつかの手法では、Local interpretable model-agnostic explanations(LIME)やシャープレイ値のように、個々の予測がどのように行われたかを説明します。 他の手法は、データセット全体のモデルの平均的な振る舞いを説明します。 ここでは、partial dependence plot, accumulated local effects, permutation feature importanceをはじめ、その他多くの手法についても学びます。 特別なカテゴリとして、データ点を説明として生成する例示による説明手法があります。 対事実的説明(Counterfactual explanations)、プロトタイプ(prototypes)、influential instances、敵対的サンプル(adversarial examples)が、本書で紹介されている例示による説明手法です。 本書は、解釈可能な機械学習の将来がどのようなものになるかについての考察で締めくくられています。 最初から最後まで読まなくても、行ったり来たりしながら、自分が最も興味を持った手法に集中して読むことができます。 私がお勧めするのは、序章と解釈可能性の章から始めることだけです。 ほとんどの章は似たような構成で、1つの解釈手法に焦点を当てています。 最初の段落でその手法を要約します。 そして、数式に頼らず、その方法を直感的に説明するようにしています。 それから、その手法の理論を見て、その方法がどのように働くのかを深く理解します。 理論には数式が含まれているでしょうから、ここでは惜しむことはありません。 私は、新しい方法は例を使って理解するのが一番だと考えています。 そのため、それぞれの手法を実際のデータに当てはめていきます。 統計学者は非常に批判的な人だと言う人がいます。 私にとっては、各章にそれぞれの解釈法の長所と短所について批判的な議論が含まれているので、その通りだと思います。 本書は手法の宣伝ではありませんが、ある手法があなたの応用に適しているかどうかの判断材料にはなるはずです。 各章の最後のセクションでは、利用可能なソフトウェアの実装が議論されています。 機械学習は、研究や産業界の多くの人々から大きな注目を集めています。 機械学習はメディアで過剰に宣伝されることもありますが、実際には影響力のあるアプリケーションがたくさんあります。 機械学習は、製品、研究、自動化のための強力な技術です。 近年では、機械学習は、例えば、詐欺的な金融取引を検出したり、視聴する映画をおすすめしたり、画像を分類したりするために使用されています。 そこで、機械学習モデルが解釈可能であることは非常に重要です。 解釈可能性は、開発者がモデルをデバッグして改善したり、モデルの信頼性を向上させたり、モデルの予測を正当化し、洞察を得るために役立ちます。 機械学習の解釈可能性の必要性が高まっているのは、機械学習の利用が増えたことによる自然な結果です。 本書は多くの人にとって貴重なリソースとなっています。 講師の方は、生徒に解釈可能な機械学習の概念を紹介するために本書を使えます。 私は、様々な修士課程や博士課程の学生から、この本が論文の出発点であり、最も重要な参考文献であることを教えてくれるメールを受け取りました。 この本は、生態学、金融、心理学などの分野で機械学習を使ってデータを理解しようとする応用研究者に役立ってきました。 産業界のデータサイエンティストは、『解釈可能な機械学習』の本を仕事に使っていて、同僚にも勧めていると話してくれました。 多くの人がこの本の恩恵を受け、モデル解釈の専門家になれることを嬉しく思います。 機械学習モデルをより解釈しやすくするための技術の概要を知りたい実務家の方にお勧めしたい一冊です。 また、このトピックに興味を持っている学生や研究者（他の誰にでも）にもお勧めします。 本書の恩恵を受けるためには、機械学習の基本的な理解をすでに持っている必要があります。 加えて、本書の理論と公式を理解できるように、大学入学レベルの数学的素養を持っている必要があります。 しかし、各章の冒頭にある手法の直感的な説明は数学なしで理解できるはずです。 ぜひ本書をお楽しみください。 "],["intro.html", "Chapter 1 イントロダクション", " Chapter 1 イントロダクション 本書は（教師あり）機械学習モデルを解釈可能にする手法について解説しています。 文中に数式が出てくることもありますが、数式を理解できなくても手法の裏にある考え方を理解できるでしょう。 本書は機械学習を一から学ぼうとしている人のための本ではありません。 機械学習に初めて触れる人には、基礎を学ぶための教材が多くあります。 書籍では &quot;The Elements of Statistical Learning&quot; by Hastie, Tibshirani, and Friedman (2009) 1、オンライン学習プラットフォームなら coursera.com の Andrew Ng's &quot;Machine Learning&quot; online course がお勧めです。 これらの教材は無料で利用できます。 機械学習モデルの解釈性に関する新しい手法がとてつもない速さで次々に公開されています。 公開された全ての手法についていくのは狂気の沙汰ですし単純に不可能でしょう。 従って、本書では斬新で風変わりな手法を取り上げることはせず、確立された手法や機械学習の解釈可能性の基本的な考え方について説明しています。 これらの基礎知識は機械学習モデルを解釈可能にするための手助けとなります。 本書を読み始めて (若干誇張気味ですが) 5分もあれば、解釈性の基本的な考え方を習得し、arxiv.org に掲載された解釈性に関する論文をよりよく理解し評価できるようになります。 本書は、(ディストピア的な) short stories から始まります。本書を理解するためには必須ではありませんが、あなたを楽しませ、考えさせてくれるでしょう。 次に、解釈可能な機械学習 の概念について紹介します。 どのような時に解釈可能性が重要となるか、どのような説明手法があるかを議論します。 本書で使われている用語は 専門用語 の章で確認できます。 本書で取り上げるモデルや手法の多くは、データセット の章で説明する現実のデータ例を用いて解説します。 機械学習モデルを解釈可能にするひとつの方法は、線形モデルや決定木のような 解釈可能なモデルを用いることです。 もうひとつの選択肢は、どんな教師あり学習モデルにも適用できる モデル非依存の手法 を用いることです。 この章では partial dependence plots と permutation feature importance といった手法を取り上げます。 モデル非依存の手法は、機械学習モデルの入力を変えた時に予測結果がどのように変化するかを計算します。 例示による説明 の章では、モデルの説明としてデータインスタンスを出力するモデル非依存な手法について解説します。 モデル非依存な手法は、全インスタンスに渡るモデルの大域的な挙動について説明するか、個々の予測について説明するかによって、分類できます。 モデルの大域的な挙動について説明する手法には Partial Dependence Plots, Accumulated Local Effects, Feature Interaction, Feature Importance, Global Surrogate Models そして Prototypes and Criticisms のようなものがあります。 モデルの個々の予測については、 Local Surrogate Models, Shapley Value Explanations, Counterfactual Explanations といった手法があり 、関連する Adversarial Examples についても解説します。 モデルの大域的な挙動と個々の推論の両面を説明できる手法として Individual Conditional Expectation や Influential Instances のようなものもあります。 最後に、解釈可能な機械学習の未来の章で機械学習の将来について楽観的な見通しを示します。 本書は最初から順番に読んでも良いし、興味のある手法のところだけ読んでも良いように書かれています。 楽しんで読んでいただけると幸いです。 Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. &quot;The elements of statistical learning&quot;. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009).↩ "],["storytime.html", "1.1 物語の時間", " 1.1 物語の時間 まずは短い物語をいくつか紹介しましょう。 それぞれの物語は解釈可能な機械学習のためにいささか誇張されたものです。 もし急いでいるなら、これらの物語は読み飛ばしても大丈夫です。 もし楽しみたいとか、やる気を出したい（ときに失くしたい）ならば、ぜひ読んでみてください！ 話の構成は Jack Clark の Import AI Newsletter に掲載されている技術小話から影響を受けています。 もしこれらの物語が気に入って、AI に興味を持ったならば、そのニュースレターに登録しておくことをオススメします。 稲妻は二度と打たない 2030年：スイスの医療ラボ 「絶対に、医療ミスなんかじゃありませんでしたよ！」トムは起こった悲劇の中から少しでもマシなことがないか探すかのように言った。 彼は患者の静脈からポンプを引き抜いた。 「彼は医療ミスのせいで亡くなったんじゃないわ」レナが言った。 「この狂ったモルヒネポンプのせいですよ！ 余計な仕事を増やしやがって！」トムはポンプの裏板のネジを外しながら不平を言った。 ネジをすべて外し終えると、彼はその板を持ち上げて脇に置いた。 彼はケーブルを診断装置に繋いだ。 「仕事自体に文句を言ったわけじゃないわよね？」レナは彼にからかうような笑みを向けた。 「違いますよ。えぇ違いますとも！」彼は皮肉っぽい低い声で言い返した。 彼はポンプのコンピュータを起動した。 レナはケーブルのもう一方の端を彼女のタブレットに接続した。 「いいわ。診断装置は動作してる」彼女は言った。 「なにがいけなかったのかすごく興味がある」 「John Doe 氏は完全に御陀仏ですね。 この高濃度のモルヒネです。 なんてこった。なんというか……こんなこと初めてです。そうですよね？ 普通、壊れたポンプというのはほんのちょっとしか供給しないか、何もしないものです。 決して、いや起こったけど、こんな狂った量の投与はしません」トムは説明した。 「ええ、言わなくてもわかってるわ……ねぇ、これを見て」レナは彼女のタブレットを持ち上げた。 「ここにピークがあるのが見える？これが鎮痛剤を打った効能よ。 見て！このラインは基準値を示してる。 このかわいそうな男性は、彼を17回以上殺せる量の鎮痛剤を血管系に注入されたの。 この私たちのポンプによって。 それに……」彼女は画面をスワイプした。「ここに患者が亡くなった瞬間が記録されてる」 「それで、何が起こったかわかりそうですか、ボス？」トムは彼の上司に尋ねた。 「うーん……センサーは正常なようね。 脈拍、血中酸素濃度、血糖値……データは正常に記録されている。 血中酸素濃度のデータにいくらか欠損値が見受けられるけど、めずらしいことじゃない。 ここを見て。 センサーはモルヒネ誘導体やその他の鎮痛剤によって引き起こされる脈拍の低下とコルチゾール濃度の極端な低下を検知してる」 彼女は診断レポートを次々とスワイプした。 トムは食い入るように画面をじっと見つめた。 それは彼にとって初めての機材故障の調査だった。 「オーケー。これがパズルの最初のピースね。 システムは病院の通信チャンネルに警告を送信するのに失敗している。 警告は発動したのに、プロトコルレベルで拒否された。 それは私たちの落ち度かもしれないけど、病院側の落ち度でもありうるわ。 そのログをITチームに送って」レナはトムに言った。 トムは画面をじっと見つめたまま頷いた。 レナは続けた。 「奇妙ね。 その警告を受けてポンプのシャットダウンも実行されるはずだった。 でも明らかにシャットダウンに失敗している。 きっとバグに違いないわ。 品質管理チームが見逃した。 それもめちゃくちゃ酷いやつ。 多分、プロトコルの問題に関係あるわね」 「それで、ポンプの緊急停止システムがどうにかして故障したのはいいとして、なぜポンプは John Doe 氏にこんなにたくさんの鎮痛剤を注入したのですか？」トムは疑問に思った。 「いい質問ね。 あなたの言う通りよ。 プロトコルの不慮の失敗はさておき、そんなに大量の薬物を投与すべきではまったくなかった。 コルチゾール濃度の低下や他の警告を受けて、アルゴリズムはもっと早くに自分自身で停止するはずだった」レナは説明した。 「雷に打たれるような、万に1つの不運だったかもしれない、ということですか？」トムは彼女に尋ねた。 「いいえ、トム。 もし私が送ったドキュメントに目を通していたなら、このポンプが、センサーからの入力に基づいて完璧な量の鎮痛剤を注入できるよう、最初は動物実験、次に人間で訓練されたことを知っていたかもしれないわね。 ポンプのアルゴリズムは不透明で複雑かもしれないけど、ランダムではない。 それはつまり、同じ状況に遭遇したらポンプは再びまったく同じように動作するということ。 私たちの患者がまた死ぬかもしれない。 センサーからの入力の組み合わせか、望ましくない相互作用かが、ポンプの誤作動を引き起こしたに違いないわ。 だからこそ、私たちはもっと深く掘り下げて、ここで何が起こったのかを明らかにする必要があるの」レナは説明した。 「そうですねぇ……」トムは深く考え込みながら応えた。 「どの道、この患者はそう長くなかったんですよね？ガンかなにかで」 レナは分析レポートを読みながら頷いた。 トムは立ち上がって窓に寄った。 彼は外の、遠くの一点を見つめた。 「おそらく、その機械は彼の痛みから解放されたいという願いを聞いてやったんでしょう。 もう苦しまないように。 雷に打たれるようなものだったかもしれませんが、むしろ幸運だったのかも。 宝くじみたいなもので、ランダムじゃない。 なにか意味があったんです。 もし僕がポンプだったら、同じことをしたでしょう」 彼女はようやく顔を上げ、彼を見た。 彼はまだ外のなにかを見ている。 少しの間、二人は何も言わなかった。 レナは再び画面に目を落とし、解析を続けた。 「いいえ、トム。これはバグよ……ただのクソッタレなバグ」 信用失墜 2050年：シンガポールの地下鉄駅で 彼女は地下鉄 Bishan 駅に駆け込んだ。 頭の中ではとっくに仕事を始めていた。 新しいニューラルアーキテクチャのテストはもうできたはずだ。 彼女は政府の納税義務者の脱税を予測するシステムの再設計に着手した。 彼女のチームはみごとな実装を考案していた。 うまくいけば、システムは税務署だけではなく、テロリスト対策用警鐘システムや、営利法人の登記などさまざまな場面に導入されるだろう。 ゆくゆくはそういった予測を市民の信用度計算システムに統合できるかもしれない。 市民の信用度計算システムは文字通り、それぞれの人が信用できるか判定するものだ。 ローンの採否からパスポート発行までにかかる時間まで、人々の生活のあらゆる面に影響を及ぼすだろう。 エスカレーターをおりる途中で、彼女はチームで開発しているシステムを信用度を計算にいかに統合するか考えていた。 彼女はいつも通り、歩みを緩めることなく、RFID 読み取り機に手をかざした。 良い気分だったが、期待と現実の不一致が彼女の中で警鐘を鳴らした。 遅かった。 彼女は地下鉄の改札に向かったが、鼻をぶつけて尻餅をついてこけた。 開くはずのドアは開かなかった。 驚きながら立ち上がり、改札脇のスクリーンに目をやった。 スクリーンには笑顔のキャラクターと「もう一度お試し下さい」の一言。 彼女のことなど気にせず、他の人は読み取り機に手をかざしてすり抜けていく。 ドアは開いて行ってしまった。 また閉じた。 彼女は鼻をこすった。痛むが出血はない。 彼女はもう一度ドアを開けようとしたが、またしても拒否されてしまった。 おかしい。 公共交通機関を利用するための預金が不足しているのかもしれない。 彼女はスマートウォッチで残高を確認してみる。 「ログインに失敗しました。市民相談所にお問い合わせください」と彼女の腕時計に表示されていた。 お腹を殴られでもしたかのような吐き気が彼女を襲った。 何が起きているのか考えた。 試しに「スナイパーギルド」という携帯ゲームを始めた。 すると予想通りにアプリは自動終了してしまった。 眩暈がして彼女は床にへたりこんだ。 この状況を説明できる方法は1つしかない。 市民の信用度が落ちたのだ。 がっくりと。 ちょっと落ちたくらいなら、飛行機でファーストクラスに乗れないとか、公的文書の取り寄せに通常より時間がかかるとか、少々の不便で済むのだ。 信用度が低いということは、彼女が社会的に危険な存在として認識されたということだ。 そういった人への対策の1つは、地下鉄のような公共施設からの締め出しだ。 政府は信用度が低い人の金銭取引も制限する。 更にはソーシャルメディアでの活動を監視し始め、暴力的なゲームなど一部のコンテンツの利用制限すら課す。 一度落ちた信用度は急激に回復が難しくなってしまう。 どん底まで落ちると這い上がれない。 彼女には信用度が落ちた理由がわからなかった。 信用度は機械学習に基づいて計算されている。 このシステムは、よくオイルを行きわたらせたエンジンのように社会を動かしている。 信用度計算システムの性能は常にモニタリングされていた。 今世紀初頭に比べて機械学習は非常に発展している。 市民の信用度に基づく判断はとても効率的で議論の余地はなかった。 完全無欠なシステムなのだ。 彼女は悲嘆にくれて笑うしかなかった。 完全無欠なシステム。 そうならよかった。 ほとんど失敗しない。 でも失敗した。 彼女は自分がそんな特別な一例だと思った。 システムのエラーだ。 おかげで、社会から見捨てられてしまった。 誰もシステムを疑おうなどとしない。 システムは政府と密に連携していて、社会にまで溶け込んでいて、疑う余地なんてない。 民主主義国家の中には、反民主主義的な活動を禁じている国が少数ながらある。 それは別に危険性が高いからではなく、今あるシステムを不安定にしてしまう可能性があるからだ。 同じ理屈が、今では一般的になった人工知能による統治にも適用される。 現状を危機にしてしまうようなアルゴリズムへの敵対は禁止されているのだ。 アルゴリズムによる信頼度の算出は、社会的に要請されて生まれた。 公益のため、稀な信頼度の誤判定はそれとなく許容された。 スコアの算出には何百もの予測システムやデータベースが利用されていて、彼女のスコアがなぜ落ちたのか説明できなくなっていた。 彼女は自分の足元に大きくて真っ暗な穴が開いたように感じた。 彼女は恐れ、虚空を見つめていた。 彼女の脱税者予測システムが市民の信用度計算システムに取り込まれた、しかし、彼女はそれについて知ることはなかった。 フェルミのペーパー・クリップ AMS（火星定住歴）612年の火星の博物館で 「歴史って面白くないな」、Xola は友達にこぼした。 青い髪をした少女の Xola は、屋内で彼女の左側を飛ぶプロジェクタ搭載のドローンにだらだらとついていった。 先生は彼女を見て「歴史は重要ですよ」と取り乱し気味に言った。 彼女はまさか先生に聞かれていると思っていなかった。 「Xola、今、何を学びましたか？」と先生は尋ねた。 彼女は慎重に「昔の人は惑星 Earther の資源を使い尽して死んだんでしたっけ？」と聞き返した。Lin という名の女の子が続いた。 「違うよ。彼らは気候温暖化を招いたの。正確には人ではなくコンピューターと機械がね。で、その惑星の名前は Earth（地球）よ。Earther じゃないよ」 Xola はなるほどと頷いた。 先生は少し誇らしげな様子で微笑みながら頷いた。 「どちらも正しいです。ではなぜそんなことが起きたのでしょうか？」 Xola は「人が短絡的で強欲だったからかな？」と首をかしげた。 「機械を止められなかったからよ！」と Lin は思わず言ってしまった。 先生は「今度の答えも両方あってます」と断じた。 「ですが、事態はもう少し複雑だったのです。 当時のほとんどの人は何が起きているか理解していませんでした。 急激な変化に気付いた人もいましたが、取り返しがつきませんでした。 この時代を理解する最も有名な鍵として、書き手不明の詩があります。 この事態に起きていたことを最も明瞭に記しているので、よく聞いていてくださいね。」 先生は詩を読み始めました。 たくさんのドローンが子供たちの前に移動して、それぞれの目に映像を投影し始めた。 映像の中には、切り株だけが残された森の中に立つスーツを着た人がいた。 彼はこう語り始めました。 機械は計算し、予測した 人々は予測の結果に基づいて前進した 人々は機械が学んだ最適解を辿った 最適解は1次元で、局所的で、制約がないものだった シリコンと人類は指数関数を追い求めた 発展こそ我らが本懐 すべての報酬が得られたとき、 副作用は無視される すべてのコインが採掘されたとき 自然は置き去りにされる そして災厄が訪れる 指数的な成長は水泡に帰す泡沫となって弾ける コモンズの悲劇が繰り広げられる 爆発する 目前で 冷淡な計算と容赦ない強欲が 地球を熱で満たす 何もかもを襲う死に 我々はなす術はない 目隠しされた馬のように、我々は自ら生んだレースの中で競争する 限界に向かって だから我々は執拗に行進する 機械の一部のように 崩壊も受け入れて 「暗い過去ですね」と呟き、先生は静寂を破った。 「みなさんのライブラリにあげておきますね。 宿題として来週までに暗記しておいてください。」 Xola は溜息をついた。 彼女は小型ドローンを1機つかまえた。 ドローンは CPU とエンジンのおかげで熱を持っていた。 Xola は自身の手に伝わる熱を心地良く感じていた。 "],["機械学習とは何か.html", "1.2 機械学習とは何か？", " 1.2 機械学習とは何か？ 機械学習とは、コンピュータがデータから予測や動作をしたり、それらを改善するために用いる手法全般のことを指します。 例えば、家の価値を予測するためには、コンピュータは過去の家の売値からパターンを学習するでしょう。 この本は機械学習の中でも、教師あり学習に焦点を当てています。 教師あり学習は、関心のある結果(過去の住宅価格など)が既知のデータセットがある状況で新しいデータに対する予測の仕方を学習したいというような場合の全ての予測問題に相当します。 教師あり学習以外のものは、例えば、クラスタリング問題 (= 教師なし学習)があります。 これは、関心のある結果が手元にない状況で、データ点の集まり (クラスタ)を知りたいという問題設定です。 強化学習という、テトリスをプレイするコンピュータなど、ある環境下での行動に対して、得られる報酬を最大化するように学習する方法も扱わないこととします。 教師あり学習の目的は、データの特徴量(敷地面積、場所、床の種類など）と結果（家の価格など）を対応させる予測モデルを学習することです。 予測対象がカテゴリカルな場合、それはクラス分類と呼ばれ、予測対象が連続の数値である場合は回帰と呼ばれます。 機械学習アルゴリズムにおける学習とは、重みのようなパラメータを推定するか、グラフの木のような構造を決定することをいいます。 そのアルゴリズムはスコアまたは損失関数を最小化するように動きます。 住宅価格の例では、機械は予測した住宅価格と実際の住宅価格の差を最小化します。 こうして訓練された機械学習モデルを用いることで、新しいインスタンスに対して予測できるようになります。 住宅価格予測、商品のレコメンド、道路標識検出、債務不履行予測、不正検知、これらのすべての例は機械学習で解決できるという共通した点があります。 それぞれのタスクは異なりますが、アプローチは一緒です。 ステップ1：データを収集します。 多ければ多いほど良いです。 データには予測したい結果と予測するための追加情報が含まれていなければなりません。 道路標識検出の場合（「画像に道路標識はあるのか？」）、道路の画像を集めて標識が含まれているか否かのラベルを付けます。 債務不履行予測の場合、過去の債務情報、顧客が債務不履行をしたかの情報、及び収入や債務不履行歴など予測に役立つデータが必要です。 住宅価格自動予測プログラムの場合、過去の住宅売買情報や広さ・立地など不動産に関する情報が収集可能です。 ステップ2：これらの情報を道路標識検出モデル、債務評価モデル、住宅価格予測モデルなどを生成する機外学習のアルゴリズムに入力します。 ステップ3：新しいデータに対してモデルを使用します。 モデルを自動運転、債務申請手続き、不動産市場のウェブサイト、などの商品や手続きに統合します。 機械は、チェス（または最近では囲碁）、天気予測など多くのタスクにおいて人間を上回っています。 機械が人間と同じくらい優れている、もしくは少し劣っていた場合でも、スピード、再現性、スケールの観点で大きな利点があります。 一度導入された機械学習のモデルは、人間よりもはるかに速くタスクを完遂でき、一貫した結果を確実に提供し、複製も無限にできます。 機械学習モデルを別の機械に複製するのはとても早くて安価です。 あるタスクのために人間を訓練させるために数十年かかることがあり（特に若い人）、多くの費用がかかります。 機械学習を使用する時の大きな欠点は、データや機械が解決する問題に関する洞察が、ますます複雑になるモデルの中に隠されてしまうことです。 ディープニューラルネットワークを説明するためには数百万の数値が必要であり、モデル全体を理解する方法はないのです。 ランダムフォレストのような他のモデルでは、多数の決定木を用いて”多数決”を行うことで予測します。このとき、どのように意思決定がなされたかを理解するためには、投票がどのように行われたかと、それぞれの決定木の構造を調べる必要があります。しかし、それはあなたがどんなに賢くて、記憶力が優れていてもうまくいかないでしょう。 最も性能のいいモデルは大抵、いくつかのモデルをブレンドしたもの (アンサンブル)であり、たとえ個々のモデルが解釈可能なものであったとしても全体として解釈ができないものとなってしまいます。もし、あなたが性能のみに注目するのであれば、自動的により不透明なモデルとなるでしょう。 機械学習コンペティションプラットフォーム kaggle.com の優勝者インタビューをご覧ください。 優勝したモデルは、ほとんど boosted trees やディープニューラルネットワークのような、とても複雑なモデルのアンサンブルモデルです。 "],["terminology.html", "1.3 専門用語", " 1.3 専門用語 曖昧さによる混乱を避けるために、この本で使用する用語の定義をいくつか紹介します。 アルゴリズム (Algorithm) とは、特定のゴール2を達成するために機械が従うルールの集まりのことです。アルゴリズムは、入力と出力、および入力から出力を得るために必要な全てのステップを定義するレシピのようなものと見なすことができます。 料理のレシピは、食材を入力、調理された食品を出力、準備や調理手順がアルゴリズムの指示であるようなアルゴリズムと言えます。 機械学習 (Machine Learning) はコンピュータがデータから学習して予測 (例えば、がん、売り上げ、債務不履行) を行い、改善することを可能にする手法の集まりのことです。 機械学習によって、全ての命令を明示的にコンピュータに与える必要がある”従来のプログラミング”から、データを提供することで行われる”間接的なプログラミング”へのパラダイムシフトが起こりました。 学習者 (Learner) または 機械学習アルゴリズム はデータから機械学習モデルを学習するためのプログラムのことを言います。別の名前は、”inducer” (例えば, “tree inducer”)とも言います。 機械学習モデル (Machine Learning Model)とは、入力に対して予測を対応づける学習されたプログラムのことを言います。これは、線形モデルやニューラルネットワークの重みの集合とも言えます。 この曖昧な単語である“モデル”の別の言い方として、”predictor” または、タスクに応じて “classifier” や “regression model” と言うこともあります。 FIGURE 1.1: ラベル付きの学習データから学習者がモデルを学習する様子 モデルは予測を行うために使用される。 ブラックボックスモデル (Black Box Model)とは、内部の機構が明らかになっていないシステムのことを言います。機械学習の文脈では、”ブラックボックス”は、例えばニューラルネットワークのような、学習された重みを見ても人間が理解できないモデルのことを指しています。 ブラックボックスの反対は”ホワイトボックス”であり、この本では interpretable model として紹介されています。 解釈可能な機械学習 (Interpretable Machine Learning) は、機械学習システムの振る舞いや予測を人間にとって理解可能なものにするための手法やモデルのことを言います。 データセット (Dataset) は機械が学習するデータを含むテーブルのこととします。 データセットは特徴量 (features) と予測の目的値を持っています。 モデルを学習する際に使われたデータセットのことを学習データと呼びます。 インスタンス (Instance) はデータセットの行のことを言います。 インスタンスの別の言い方は、データ点 (data point)、例 (example)、観測 (observation)です。 インスタンスは特徴量の値\\(x^{(i)}\\)と、もし既知なら、目的値の\\(y_i\\)からなります。 特徴量 (Features) は予測やクラス分類に使われる入力のことです。 特徴量はデータセットの列に対応します。 この本を通して、特徴量そのものは解釈可能、つまり、気温や日、人間の身長など、意味が簡単に理解可能なものであるとします。 なぜなら、仮に入力特徴量の理解が困難であれば、学習されたモデルの理解もまた難しくなるからです。 全ての特徴量からなる行列をXと呼び、\\(x^{(i)}\\)は1つのインスタンスのこととします。 すべてのインスタンスに対する1つの特徴量を並べたベクトルを\\(x_j\\)として、インスタンス i に対する特徴量 j の値は \\(x^{(i)}_j\\) とします。 目標値 (Target) とは機械が学習する予測の情報 のことです。 数式の中では、目標値は y や 1つのインスタンスに対しては \\(y_i\\) と呼ばれます。 機械学習タスク (Machine Learning Task) はデータセットの特徴量と目標値の組み合わせのことです。目標値のタイプにしたがって、タスクは例えばクラス分類、回帰、生存分析、クラスタリング、異常検知となります。 予測 (Prediction) は、与えられた特徴量に対して、機械学習モデルが 目標値がどうあるべきか“推測” した結果のことを言います。 この本では、モデルの予測は \\(\\hat{f}(x^{(i)})\\) または \\(\\hat{y}\\) と表記します。 &quot;Definition of Algorithm.&quot; https://www.merriam-webster.com/dictionary/algorithm. (2017).↩ "],["interpretability.html", "Chapter 2 解釈可能性", " Chapter 2 解釈可能性 解釈可能性に数学的な定義はありません。 Miller (2017)3が提言した個人的に好きな(数学的でない)定義は、 解釈可能性とは、人間が決断の要因を理解できる度合いです。 そしてもうひとつが、 解釈可能性とは、人間がモデルの結果を一貫して予測できる程度です 4。 機械学習モデルの解釈可能性が高ければ高いほど、その決定や予測がなされた理由を理解しやすくなります。他のモデルよりも人間が理解しやすい予測をするモデルは、より良い解釈性を持つモデルと言えます。 本書では、解釈可能と説明可能を互換性のある用語としてどちらも使用します。 Miller (2017)と同様に、私は「解釈可能性/説明可能性」と「説明」という用語を区別することには意味がある考えます。 この本では、「説明」という用語を各予測の説明として使用することにします。 人間にとって良い説明と考えられるものを学ぶには 説明についての節を参照してください。 Miller, Tim. &quot;Explanation in artificial intelligence: Insights from the social sciences.&quot; arXiv Preprint arXiv:1706.07269. (2017).↩ Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. &quot;Examples are not enough, learn to criticize! Criticism for interpretability.&quot; Advances in Neural Information Processing Systems (2016).↩ "],["interpretability-importance.html", "2.1 解釈可能性の重要性", " 2.1 解釈可能性の重要性 機械学習のモデルがうまく動作しているなら、なぜそのままモデルを信用して、決定がなされた理由を無視してはいけないのでしょうか。 &quot;問題は、分類精度などの単一の評価方式では、多くの現実世界の問題を表現するには不完全なものであるということです。&quot; (Doshi-Velez and Kim 2017 5) なぜモデルの解釈可能性がそれほど重要なのか、もう少し考えてみましょう。何らかの予測モデルを構築する場合、そこに発生するトレードオフについて考える必要があります: あなたは顧客を集める可能性や、薬がどれだけ患者に効果的か、といったそのモデルが予測する結果についてのみ知りたいのでしょうか。それともたとえ予測性能が下がったとしても予測がなされた理由が知りたいのでしょうか。確かに、一部の事例では予測がなされた理由は必要なく、テストデータに対する予測性能のみを知ることができれば十分だと思います。しかし、この理由について知ることはその問題やデータに対する理解を深め、モデルが判断を誤る際にもその原因を探ることに役立ちます。また、映画のレコメンデーションといった失敗した際にの影響がそれほど大きくないリスクの低い環境で扱われる場合や、文字認識などといった、用いる手法がすでに広範にわたって研究され評価されている場合などでは、モデルの解釈はそれほで必要ではないでしょう。説明可能であることの必要性は、問題の不完全な定式化によって生じるのです(Doshi-Velez and Kim 2017)。これは、ある特定の問題や課題に対しては、予測（ 結果）を得るだけでは不十分であるということです。このような場合は、正確な予測は本来の問題の一部のみを解決しただけであり、モデルが予測に至った経緯（ 理由）も説明されるべきなのです。モデルの解釈・説明は次のような観点からも必要とされます（Doshi-Velez and Kim 2017 and Miller 2017）。 人間の好奇心と学び: 私たち人間は自分の周囲の環境に対する心理的なモデルを一人一人が持っており、何か予測していないことが起こった際、このモデルを更新しています。ただし、このモデルの更新はこれらの出来事の原因がわかって初めて行われます。例えば、もし、思いがけず病気にかかってしまったとすると、なぜ病気にかかってしまったのかを考えるでしょう。その後、赤い木の実を食べた後に毎回具合が悪くなることに気付きます。このとき赤い木の実が具合を悪くさせること、また今後赤い木の実は食べないようにするよう、あなたは自分のモデルを更新します。これに対し、不透明なモデルが研究に使用された場合、科学的な知見は全く得られません。問題に対する理解を深め、自身の好奇心を満たすためには、モデルの説明可能性や解釈可能性は不可欠なのです。もちろん、私たちは現実に起こりうる全ての物事に対し説明が必要であるわけではありません。多くの人はコンピュータが動く原理を理解しなくても全く問題ないでしょう。しかし、それでも予想外の出来事というのは私たちの好奇心をそそります。考えてみてください、なにが原因でコンピュータは突然シャットダウンしてしまうのでしょうか。 学びに大きく関係しているのが、人間の世の中の意味を見出したいという願望です。私たちは出来事と自分の持っている知識との矛盾や不一致を調和させようとします。「なぜうちの犬はさっき自分のことを噛んできたのだろうか？今までそんなことはなかったのに。」このように考えるわけです。ここには飼い犬の過去の振る舞いと、つい先ほど噛まれたという不快な事実という矛盾があります。 獣医の説明は、次のように矛盾を解決します: 「その犬はストレスに晒されていたのでしょう。」 機械の判断が人間の生活により影響を及ぼすようになるのであれば、機械自身がその判断について説明できることがより重要となってきます。もし機械学習モデルがローンの申し込みを拒否するよう判断したならば、これは申し込んだ側にとって予想外の出来事でしょう。 この予想と実際の結果の矛盾を埋めるには何らかの説明が必要です。この説明では状況を完璧に説明する必要はないでしょうが、主要な原因については言及する必要があるでしょう。 別の例としては、アルゴリズムを元にした商品のレコメンデーションです。個人的な話ですが、私はある商品や映画がなぜアルゴリズム的に自分に推薦されたのか、いつも考えてしまいます。 多くの場合は明らかです。直近に洗濯機を購入していたとすると、次の数日間は洗濯機に関する広告が表示されるでしょう。冬の帽子をすでに買い物かごに入れている場合に手袋がおすすめされるのも納得がいきます。 また映画のレコメンデーション機能は他のユーザーが気に入った映画に基づいて映画を提案します。最近では、インターネット事業を手掛けるより多くの会社がおすすめ機能に説明を加えています。分かりやすい例としては、他ユーザーが合わせて購入する商品をもとにした商品のおすすめ機能が挙げられます。 FIGURE 2.1: よく一緒に購入されている商品 社会学や哲学など、多くの科学分野において分析の手法は定性的なものから定量的なものへと変化しています。中には生物学や遺伝学など、機械学習へと手法が移っている分野もあります。 科学の目標は知識を得ることですが、多くの問題は大規模データとブラックボックスな機械学習モデルで解かれてしまっています。 モデルはそれ自身がデータの代わりに知識の源となります。 解釈可能性はこのモデルから得られる付加的な知識を抽出することを可能にします。 機械学習モデルは、安全対策とテストが必要な実務課題を引き受けるようになります。 モデルを読み解くと、学習によって得られた自転車の最も重要な特徴が2つの車輪を認識することだとしましょう。すると、このモデルによる説明から、車輪の一部を覆うサイドバッグをつけた自転車のようなコーナーケースを考えるのに役立ちます。 デフォルトでは、機械学習モデルは学習データに内在するバイアスを反映してしまいます。これは機械学習モデルを、保護されるべきグループを差別する人種差別主義者に変えうるものです。 解釈可能性は、機械学習モデルのバイアス検出のための便利なデバッグツールになります。 例えば、クレジット申請を自動で承認、拒否をするために学習したモデルが、少数派を差別するケースが考えられます。 企業の最大の目標はきちんと返済する人にだけローンを貸すことです。 この例における問題の定式化の不備は、ローンの不履行を最小化するだけではなく、特定の人口統計に基づいて差別をしない義務があることです。 低リスクでなるべく多くの人を受け入れるようにローンを貸し出すという問題の定式化には、機械学習モデルが最適化する損失関数でカバーできていない追加の制約を加える必要があります。 機械とアルゴリズムを私たちの日常生活に取り入れるプロセスとして、社会的受容を高めるための解釈可能性が必要です。 人間は、信念や欲求、意図などが物体にあると考えます。 有名な実験で、Heider and Simmel（1944）6は、円が「ドア」を開いて「部屋」（ただの長方形）に入るようなビデオを参加者に見せました。 参加者は、人間の行動を説明するのと同様に図形の行動を説明し、意図、さらには感情や性格までもを図形に割り当てました。 私が&quot;Doge&quot;と名付けたロボット掃除機のように、ロボットはその良い例です。 例えばもし Doge が停止していると、私はこう考えるでしょう。 「Doge は掃除を続けたがっているが、動けなくなってしまったから私に助けを求めてきたぞ。」 しばらくして Doge が掃除を終え、充電台を探していれば私はこう考えます。 「Doge は充電したがっていて、そのために台を見つけるつもりなんだ。」 また、私は Doge に個性をも見出すでしょう。 「Doge は少し抜けているが、そこが可愛いんだ。」 これらは私の考えですが、特に Doge が真面目に家を掃除しようとして、植物を倒してしまった時などにそう感じます。 予測の理由を説明できる機械やアルゴリズムはより広く受け入れられるでしょう。 説明は社会的プロセスであると主張している説明についての章も参照してください。 説明は社会的相互作用を管理するのに使われます。 説明者は何かしらの意図を共有することで、説明を受ける人の行動や感情、信念に影響をもたらします。 機械が私たちが互いに影響し合うためには、人間の感情や信念を形にする必要があるかもしれません。 機械が意図した目標を達成するためには、私たちを&quot;説得&quot;する必要があります。 ロボット掃除機がその動作をある程度説明できなければ、私は完全に受け入れることはできないでしょう。 掃除機は何も言わず単に停止するのではなく、その理由（例えばバスルームのカーペットに引っかかるといった「事故」など）を説明することで、共通の理解を生み出します。 興味深いことに、説明する機械側の目的（信頼を築く）と受け手側の目的（予測、あるいは動作を理解する）の間に乖離が生じる可能性があります。 ひょっとしたら、Doge が動かなくなった本当の理由は、バッテリーがとても少ないこと、タイヤの１つが故障していること、障害物があるにも関わらず同じ場所を何度も往復するバグがあることかもしれません。 これらの理由（もしくは他のいくつかの理由）によってロボット掃除機は停止しましたが、私が彼の動きを信頼しその事故の共通の意味を理解するには、何かが邪魔をしていることを説明すれば十分でした。 ちなみに、Doge はバスルームでまた立ち往生しました。 Dogeに掃除させる前に、毎回カーペットを退けておく必要があったみたいです。 FIGURE 2.2: 掃除機 Doge が停止している。事故の説明として、Doge は平らな表面上に居なければならないと教えてくれた。 機械学習モデルは解釈可能な場合にのみデバッグや検査ができます。 映画推薦のようなリスクが低い場合や、デプロイ後に限らず研究・調査段階であったとしても解釈可能性は価値があります。 モデルを製品の中で使用する場合、後になってから問題が発生することがあります。 誤った予測に対する解釈はエラーの原因を理解するために役立ちます。 これによって、システムをどのように修正するかという方向性が得られます。 ハスキーとオオカミの分類器において一部のハスキーをオオカミと誤分類する例を考えてみます。 解釈可能な機械学習手法を用いると、誤分類が画像上の雪によって生じていることがわかりました。 分類器は画像をオオカミと分類するための特徴として雪を学習しました。学習データにおいて、ハスキーとオオカミを分類するという観点からは意味があったかもしれませんが、実世界では、意味がありません。 機械学習モデルが判断の根拠を説明できるようになれば、以下の特性も簡単に確かめることができます(Doshi-Velez and Kim 2017)。 - 公平性: 予測に偏りがなく、保護されるグループに対して明示的または暗黙的に差別しない。解釈可能なモデルでは、ある人物が融資を受けるべきでないと決定した理由がわかり、その理由が学習した人口統計(人種など)の偏りに基づいているかどうかを人間が判断しやすくなります。 - プライバシー: データ内の機密情報が保護されていること。 - 信頼性または頑健性: 入力の小さな変化が予測に大きな変化をもたらさないこと。 - 因果関係: 因果関係だけが取得されていること。 - 信頼: ブラックボックスよりも自身の決定を説明できるシステムの方が人間に信頼されやすいこと。 解釈可能性を必要としない場合 以下のシナリオは機械学習モデルの解釈可能性がいつ必要とされないか、あるいはいつ求められないかを説明します。 モデルが重大な影響力を持たないならば、解釈可能性は必要とされません。 フェイスブックのデータを元に友人が次の休暇にどこへ行くのかを予測する機械学習のプロジェクトで働くマイクという人物を想像してみてください。 マイクは友人が休暇中にどこへ行くのかを学習によって推測することで単に友人を驚かせるのが好きなのです。 もしモデルが間違えたとしても(最悪、マイクが少し恥ずかしい思いをするだけで)実害はありませんし、マイクがモデルの予測結果を説明できなくても問題はありません。 この場合では解釈可能性がなくても全く問題ないのです。 もしマイクが休暇中の行き先予測に関するビジネスを始めるならば、状況は変わります。 もしモデルが間違えれば、ビジネスでお金を失うか、あるいは人種に関する偏見を学習することでモデルが一部の人々にとって悪く働くかもしれません。 経済的であれ社会的であれ、モデルが重大な影響力を持つとすぐに、解釈可能性は重要になります。 問題が十分に研究されているならば、解釈可能性は必要ありません。 一部のアプリケーションは十分に研究されているため、モデルに関する十分な実務経験があり、モデルの問題は時間をかけて解決されてきました。 その良い例は、封筒の画像を処理して住所を抽出する光学式文字認識の機械学習モデルです。 これらのシステムには長年の経験があり、それらが機能することは明らかです。 加えて、このタスクについて追加の洞察を得ることには関心がありません。 解釈可能性は人間やプログラムがシステムを操作することを可能にします。 システムを欺く問題はモデルの使用者と開発者間の目的の不一致によって発生します。 クレジットの信用スコアはそのようなシステムであり、銀行は返済能力のある申請者だけに融資されるようにしたい一方で、申請者はたとえ銀行が融資を望まない場合でも融資されることを目的とします。 この両者の目的の不一致は、申請者が融資を得る可能性を高めるためにシステムを操る動機となります。 ２枚より多くのクレジットカードを所持していると数値に悪影響があると申請者が知っていれば、数値を向上させるために３枚目のカードを一旦返却し、融資が成立した後に新たなカードを発行するでしょう。 これは数値が改善される一方で、融資を返済する実際の可能性は変わりません。 このシステムは、入力が因果的特徴の代理となっている場合であって、出力との実際の因果関係とは異なる場合に操られるでしょう。 モデルをゲーム化させないためにも、代理的な特徴は避けるべきです。 例えば、グーグルはインフルエンザの流行を予測するために Google Flu Trends と呼ばれるシステムを開発しました。 そのシステムは Google 検索とインフルエンザの流行を関連付けましたが、機能は低下してしまいました。 検索クエリの分布が変化したことで、Google Flu Trends は多くのインフルエンザの流行を見逃してしまいました。 グーグル検索はインフルエンザを引き起こす原因ではないのです。 人々が”発熱”のような症状を検索する時と、実際のインフルエンザの流行とは単なる相関関係にすぎません。 理想的なのは、ゲーム化しないようにモデルが因果的な特徴だけを使用することです。 Doshi-Velez, Finale, and Been Kim. &quot;Towards a rigorous science of interpretable machine learning,&quot; no. Ml: 1–13. http://arxiv.org/abs/1702.08608 ( 2017).↩ Heider, Fritz, and Marianne Simmel. &quot;An experimental study of apparent behavior.&quot; The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944).↩ "],["解釈可能な手法の分類.html", "2.2 解釈可能な手法の分類", " 2.2 解釈可能な手法の分類 機械学習の解釈方法は様々な指標で分類できます。 本質的 (intrinsic) か後付けか (post-hoc) この指標は、解釈性の獲得を、機械学習モデルの複雑度を制限することで行う本質的 (intrinsic)と、モデルを訓練した後に分析することで行う後付け(post-hoc)に分かれます。 本質的な解釈は機械学習モデルの単純な構造によるものであり、たとえば、単純な決定木やスパース制約をかけた線形モデルが該当します。 後付けの解釈はモデルを学習した後に解釈するための手法です。 例えば、Permutation Feature Importance は後付けの解釈方法です。 後付けの手法は本質的に解釈可能なモデルに対しても適用できます。 例として、Permutation Feature Importance は決定木に適用できます。 本書はこの分類方法を用いて本質的に解釈可能なモデルと後付けでモデル非依存の解釈方法の2つの章を設けています。 解釈方法の結果 解釈方法は様々ですが、その結果に応じて大まかに分類可能です。 特徴量の要約統計量: 解釈方法の多くは、特徴量ごとに要約統計量を算出します。 特徴量重要度のように特徴量ごとに1つの値を計算するものもあれば、 2項間の相互作用の強さ(pairwise feature interaction strengths)のように特徴量のペアごとに計算するものもあります。 特徴量の視覚的要約: 特徴量の要約統計量は大抵、可視化できます。 中には特徴量の Partial dependence のように、解釈に表が適さず、可視化が頼りになるものもあります。 Partial dependence plots は、注目したい特徴量の値と平均的な予測の結果との関係を表します。そのため、Partial dependenceの表現方法は、座標を記載するのではなく、実際に曲線を描くことです。 モデルの内部(例：学習後の重み): 本質的に解釈可能なモデルはこの分類に属し、例えば、線形モデルの重みや学習された木構造 (分割のための特徴量と閾値) があります。 特徴量の要約統計量との境界は曖昧で、線形モデルの重みはモデルの本質であると同時に、特徴量の要約統計量でもあります。 他にもモデルの中身を出力する方法として、畳み込みニューラルネットワークで検出した特徴を可視化する手法があります。 モデルの中身を出力する解釈方法は、本質的にモデル専用の方法です(次項参照)。 データ点: モデルを解釈するために既存の、あるいは新しく作ったデータ点を出力するすべての方法がこのカテゴリに属します。 一例としてcounterfactual explanationsを挙げます。 ある観測値から得た予測を解釈する時、予測結果（たとえば分類結果）が変わるように特徴量の一部を改変します。 他の例として、特定の予測結果を得る典型的な特徴量（prototype）を特定する方法があります。 利便性の観点から、新しいデータ点を出力する解釈方法は、データ点そのものの解釈可能性が求められます。 この方法は画像やテキストに向いている一方で、何百もの特徴量から成るテーブルデータには向いていません。 本質的に解釈可能なモデル: ブラックボックスなモデルを解釈する1つの方法として、モデルを大局的ないし局所的に解釈可能なモデルで近似してしまう手があります。 解釈可能なモデルは、モデル内部のパラメータや特徴量の要約統計量を確認することで解釈が可能です。 モデル専用か汎用か モデル専用の解釈方法は、特定のモデルやクラスに限定されています。 線形モデルの重みの解釈はモデル固有の解釈方法であり、定義から、本質的に解釈可能なモデルの解釈方法は常にモデル特有の方法と言えます。 例えば、ニュートラルネットワークの解釈のみに使える手法もモデル専用です。 モデルに依存しない手法はいかなる機械学習モデルにも適用でき、学習済みモデルにも使えます(post hoc)。 これらの汎用手法は、たいてい、入力特徴量と出力の組を分析することで機能します。 定義より、これらの手法は重みや構造の情報といったモデル内部へはアクセスできません。 局所的か大局的か 解釈方法が個々の予測を説明するか、モデル全体の挙動を説明するか、はたまたその中間でしょうか。 この分類に関しては次節で説明します。 "],["解釈可能性の範囲.html", "2.3 解釈可能性の範囲", " 2.3 解釈可能性の範囲 アルゴリズムは予測をするためにモデルを学習します。 各段階において、透明性や解釈可能性に関して評価できます。 2.3.1 アルゴリズムの透明性 アルゴリズムはどのようにしてモデルを作成するか。 アルゴリズムの透明性とは、アルゴリズムがどのようにデータからモデルを学習させるか、どのような関係性を見出せるかについてです。画像の分類にCNNを用いる場合、低階層のレイヤーにおいてはエッジの検出及び抽出が行われている、と説明できます。これはアルゴリズムがどのように動くかということに対する理解であり、モデルが最終的に何を学習したのか、個々の予測に対してどのように予測をしたのかとは関係がありません。 アルゴリズムの透明性はデータや学習済みのモデルへの知識ではなく、アルゴリズムに対する理解のみを必要とします。 なお、この本ではアルゴリズムの透明性よりもモデルの解釈可能性に焦点を当てています。線形モデルに対する最小二乗法のようなアルゴリズムは既に広範にわたって研究され理解が深められていることから、アルゴリズムの透明性は高いといえます。対して、深層学習で何百万もの重みを勾配降下法を用いて求める手法は動作が詳細には理解されておらず、この内部の動作は現在研究の対象となっています。このようなアルゴリズムは透明性が低いといえます。 2.3.2 全体的なモデルの解釈可能性 学習済みのモデルはどのようにして予測するか。 もしモデルが一目見て概要を掴めるようなものだった場合、そのモデルは解釈可能だといえるでしょう(Lipton 20167)。モデルの出力の全体を説明しようとするならば、学習済みモデル、アルゴリズムに対する知識、及びデータが必要となります。このレベルの解釈可能性は、特徴量や重みなど学習可能なパラメータ、その他のハイパーパラメータ、モデルの構造など、全ての要素から、モデルの決定がいかにしてなされるのか、ということを理解することだといえます。 どの特徴量が重要で、どのような相互作用が発生しているのでしょうか。このような問いに対し、モデルの全体的な解釈可能性は特徴量に基づいた出力の分布の理解の助けとなります。しかし、モデルの全体的な理解をすることは実際は困難です。パラメータや重みの多いモデルは人間の短い記憶には収まりません。個人的な見解ですが、人間は線形回帰モデルですら特徴量が5つもあれば、5次元の空間に超平面を想像することとなり、頭の中にイメージできなくなるでしょう。そもそも3次元以上の空間は人間には想像できません。このため、モデルの理解には線形モデルの重みなどモデルの一部のみを考えることが一般的です。 2.3.3 モジュールレベルのモデルの全体的な解釈可能性 モデルの一部はどのように予測に影響しているのか 何百もの特徴量を持つナイーブベイズモデルは大きすぎて、すべてを頭の中に記憶することは困難です。たとえ、全ての重みを記憶できたとしても、新しいデータに対して、どのように判断されるか素早く答えることはできないでしょう。それに加えて、特徴量の重要度や各特徴量が平均して予測に与える影響を測るため全ての特徴量に対する同時確率分布も把握しておく必要もあります。このようなことは不可能です。しかし、1つの重みならば簡単に理解できるでしょう。 モデルを全体的に見て解釈することは通常不可能ですが、モジュール単位で見たときに、いくつかのモデルは理解できます。全てのモデルがパラメータを用いて解釈できるわけではありません。線形回帰モデルでは重みが解釈可能な要素であり、決定木ならば分岐において選ばれた特徴量と分岐点、及び葉での予測が解釈可能な要素となるでしょう。ただし、線形モデルなどの場合、一見これらは要素レベルで完全に解釈ができるように思えますが、この重みは他の全ての重みと連動しています。重みの解釈は他の特徴量が常に同じ値であることを前提としていますが、これは多くの現実のタスクには当てはまりません。例として、家のサイズ及び部屋の数を特徴量として家の価値を予測する線形モデルを考えます。このとき、線形モデルは部屋の数に対する重みとして負の値を持つかもしれません。これは家の大きさと部屋の数の相関が大きいときに起こりえます。人々が大きい部屋を好む場合、同じ大きさの家では部屋の数が少ないほうが価値があると言えます。このように、重みはモデルの他の特徴量を考慮に入れて初めて意味を成します。ただし、それでも線形モデルの重みはニューラルネットの重みよりもはるかに解釈しやすいでしょう。 2.3.4 単一の予測に対する局所的な解釈 あるインスタンスに対して、なぜモデルがそのような予測をしたのか 単一のインスタンスに対して注目して、この入力に対してモデルが何を予測するのかを調査することで、その理由を説明できます。 個々の予測についてみてみると、他の複雑なモデルの振る舞いもすっきりとするかもしれません。予測は、複雑な依存関係があったとしても、局所的にはいくつかの特徴量の線形、もしくは単調な関係に従うとみなすことができます。 例えば、住宅の価格は家のサイズに対して非線形に従うかもしれません。 ただし、100平方メートルの家に限定してみると、その付近のデータでは、予測が家のサイズに線形に従っている可能性があります。これは、サイズを10平方メートル増減させたときに予測価格がどのように変化するかをシミュレーションすることで明らかにできます。 それゆえ、局所的な説明は大域的な説明よりも、より正確になります。 この本では、モデル非依存(model-agnostic)の方法の章で、個々の予測をより解釈可能にするための手法を紹介しています。 2.3.5 予測のグループに対する局所的な解釈 インスタンスのグループに対して、なぜモデルがそのような予測をしたのか 複数のインスタンスに対するモデルの予測は、大域的なモデル解釈の方法（モジュールレベル）または、個々のインスタンスの説明によって説明可能です。 Lipton, Zachary C. &quot;The mythos of model interpretability.&quot; arXiv preprint arXiv:1606.03490, (2016).↩ "],["解釈可能性の評価.html", "2.4 解釈可能性の評価", " 2.4 解釈可能性の評価 機械学習における解釈性に関しての総意はありません。 それを測定する方法も明確ではありません。 しかし、これに関するいくつかの先行研究や、評価のための定式化の試みが行われているため、以下ではそれについて紹介します。 Doshi-Velez と Kim (2017) は解釈可能性を評価するための3つの主要なレベルを提案しています。 アプリケーションレベルの評価 (真の作業) 製品に説明書を同梱して、エンドユーザーに試用してもらいます。 機械学習によってX線画像から骨折箇所を見つけて印をつける骨折検出ソフトウェアを想像してください。 アプリケーションレベルでは、放射線科医が骨折検出ソフトウェアを直接試用してモデルを評価します。 これには優れた実験設定と品質評価の方法に関する理解が必要とされます。 そのための適切な基準は、同様の決定を人間が説明する際に毎回どのくらい優れているかということです。 人間レベルの評価 (単純な作業) は単純化されたアプリケーションレベルの評価です。 これらの実験間の違いは、人間レベルの評価実験が分野の専門家によってではなく、素人によって行われることです。 これによって実験が(分野の専門家が放射線科医の場合は特に)安価になり、さらに多くの試験車を探しやすくなります。 実験の例としては、ユーザーにそれぞれ異なるいくつかの説明を見せて、一番良いものを選んでもらう方法があります。 機能レベルの評価 (代理的な作業) は人間を必要としません。 これは、使用されるモデルのクラスがすでに誰かによって、人間レベルで評価されている場合に最もいい方法です。 例えば、エンドユーザーが決定木を理解していると分かっている場合があります。 この場合、評価の質を表すのは木の深さかもしれません。 より短い木はより説明可能性の数値を高めるでしょう。 木の予測性能が良好なままで、より大きな木と比較してもそれほど性能が低下しないという制約を追加することには意味があるでしょう。 次の章では、機能レベルでの個々の予測に対する説明の評価に焦点を当てます。 説明に対する評価を検討する上で関連する性質は何でしょうか？ "],["properties.html", "2.5 説明に関する性質", " 2.5 説明に関する性質 機械学習モデルの予測を説明するために、いくつかの説明を生成するためのアルゴリズムに頼る必要があります。 一般的に、説明とは、インスタンスの特徴量とモデルの予測結果を人間にわかりやすい形で関連づけることをいいます。 他には、k近傍法などのように、いくつかのデータを用いて解釈する手法もあります。 例えば、がんのリスクを SVM を用いて予測し、local surrogate method を用いて決定木を構築することで予測の解釈する方法や、サポートベクタマシンの代わりに線形回帰モデルを用いる方法があります。線形回帰モデルは重みから予測結果の解釈できます。 予測の説明及び、説明方法の性質について、もう少し詳しく見てみましょう (Robnik-Sikonja and Bohanec, 20188)。これらの性質はモデルの解釈手法とその解釈の良し悪しを決めるために用いることができます。ただし、これらの性質をどのように算出するかは定まっておらず、これらを計算可能とするための定式化が1つの課題となっています。 説明方法の性質 表現力 (Expressive Power)は、その手法が生成できる「言語」や説明の構造を指します。 説明方法は、IF-THEN規則や、決定木、加重和、自然言語を生成できます。 透光性 (Translucency)は、その手法がどの程度モデルのパラメータなどを参照しているかを表します。 説明手法が、線形回帰モデルのような本質的に解釈可能なモデルに対する特有の方法であるとき、透光性が高いと言えます。逆に、説明がモデルへの入力と出力の変化のみに基づいている場合、その説明手法は透光性がないといえます。 状況によって、求められる透光性のレベルは異なります。 高い透光性のメリットは、より多くの乗法をもとに説明を生成することが可能になる一方で、低い透光性のメリットは、モデルの種類にかかわらず、その説明手法を適用することが可能になることです。 汎用性 (Portability)は、説明手法が適用可能なモデルの範囲を表します。 透光性の低い説明手法はモデルをブラックボックスのように扱うため、汎用性が高くなります。サロゲートモデルは説明手法の中でもかなり高い汎用性があり、一方で、再帰型ニューラルネットワークなど特定のモデルにのみ適用できる手法は汎用性は低いと言えます。 アルゴリズムの複雑さ (Algorithmic Complexity)はモデルの説明を計算する際の計算量を表します。これは説明の生成の計算時間がボトルネックになるような場合に重要な性質となります。 個々の説明に対する性質 正確性 (Accuracy)：見たことのないデータに対する予測がどの程度うまく説明できるか？ 高い正確性は、機械学習のモデルの代わりに説明自体が予測のために用いられる場合に特に重要とです。ただし、モデルの精度自体がそれほど高くない場合や、ブラックボックスモデルを説明することが目的である場合、正確性はそれほど重要ではありません。 このような場合は、次の忠実さが重要となります。 忠実性 (Fidelity)：その説明が、ブラックボックスモデルの予測をどの程度近似しているか？ 忠実性の高さは最も重要な指標の1つです。なぜなら忠実性の低い説明は機械学習モデルを説明する上で意味を為さないためです。正確性と忠実性は密接に関係しています。ブラックボックスモデルの精度が高く、かつ説明の忠実性も高い場合、その説明は正確性も高いと言えます。いくつかの説明には、局所的に忠実性を持つもの、つまりデータの一部においてモデルの予測をよく近似しているもの(e.g. local surrogate models) や、個々のインスタンスに対してのみ忠実である場合(e.g. シャープレイ値)があります。 一貫性 (Consistency)：同じ問題に対し学習され、同じような予測をする複数のモデルに対し、説明がどの程度異なるか？ 例えば、サポートベクタマシンと線形回帰モデルを同じタスクに対し学習し、それらがとても似た予測をするとしましょう。これらに対して、何らかの手法で説明を与え、得られた説明がどの程度異なっているのかを計算します。このとき、説明がとても似ているのであれば、その説明は高い一貫性を持つと言います。 ただしこの指標には注意点があり、これらのモデルが異なる特徴に基づいて同じ予測をしているような場合があります (&quot;羅生門効果&quot; )。このとき、解釈は全く異なったものになるべきであり、一貫性は低い方が望ましいです。逆に複数のモデルの予測が同じ特徴に基づいている場合、一貫性は高いことが望ましいです。 安定性 (Stability)：似たインスタンスに対して、説明がどの程度似たものになるか？ 一貫性がモデルごとの説明を比較するのに対し、安定性は同じモデルの似たインスタンスごとの説明を比較します。安定性が高いとは、あるデータの特徴が多少変化した場合においても、予測に大きな影響がなければ説明が大きく変化しないことを言います。安定性に欠ける場合、説明の方法が大きく異なるでしょう。 言い換えれば、説明の方法は説明されるインスタンスの特徴量のわずかな変化に強く影響されてしまいます。 安定性の欠如は、local surrogate methodで使われているようなデータをサンプリングするステップなどの、非決定性（ランダム性）に基づく説明の手法によって起こされます。 高い安定性は常に求められています。 理解のしやすさ (Comprehensibility)：説明が人間にとってどの程度理解可能か？ この指標は多くの指標の中の1つに思えますが、定義や計算することが困難でありながら、正しく求めることが極めて重要であるという点で、非常に面倒な指標です。理解のしやすさは、長袖に依存するという点は多くの人が同意するところでしょう。 理解のしやすさを測る方法として、説明のサイズ（線形モデルでの非ゼロの重みの数、決定規則の数など）を測る方法や、説明からモデルの振る舞いをどの程度予測できるかを測る方法などが考えられます。また、説明で使用されている特徴量の理解度も考慮する必要があります。 特徴量に複雑な変換を施してしまうと、元の特徴量よりも理解が難しくなってしまいます。 確信度 (Certainty)：モデルの確信度がどの程度説明に反映されているか？ 機械学習モデルの多くは予測のみを出力し、その予測が正しいと確信している度合いについては何も出力しません。モデルがある患者に対しがんの可能性が4％であると予測した場合、これは特徴量の値の異なる別の患者の4%と同じくらい正しいと考えられるでしょうか。モデルの予測の確信度を含む説明はとても有用です。 重要度 (Degree of Importance)：特徴量の重要度、または、説明の一部を、どの程度説明に反映できているか？ 例えば、決定規則による説明が個々の予測から生成された場合、どの規則が最も重要が明白でしょうか。 新規性 (Novelty)：説明されるべきインスタンスが学習データの分布から遠く離れた新しい領域からのものであるかを説明に反映しているか？ このような場合、モデル自体があまり正確でなく、説明も役に立たなくなる場合があります。新規性の概念は確信度と関連しています。新規性が高くなるほど、データ不足によりモデルの確信度は低くなります。 表現力 (Representativeness)：説明はどれほどのインスタンスをカバーしているか？ 説明はモデル全体をカバー (例: 線形回帰モデルの重みの解釈)するものや、個々の予測にのみ(例: シャープレイ値) しか行えないものもあります。 Robnik-Sikonja, Marko, and Marko Bohanec. &quot;Perturbation-based explanations of prediction models.&quot; Human and Machine Learning. Springer, Cham. 159-175. (2018).↩ "],["explanation.html", "2.6 人間に優しい説明", " 2.6 人間に優しい説明 私たち人間にとっての「良い」説明についてや、解釈可能な機械学習との密接な関係についてさらに深く掘り下げてみましょう。 それには人文科学が参考になるでしょう。 Miller(2017) は、{explanations}についての出版物に対して、膨大な調査をしました。この章はその要約に基づいています。 この章では、以下のことを説明します。 ある出来事の説明として、人間はその出来事が起こらなかったであろう状況と対比するような、1つか2つの要因でできた端的な説明を好みます。 特に、異常の原因は良い説明となります。 説明は、説明する人と説明を受ける人の間の社会的な相互作用なので、社会的な文脈は実際の説明の内容に大きな影響を与えます。 特定の予測や行動に対して全ての要因を用いた説明をする必要がある場合は、人間に分かりやすい説明が求められている時ではなく、完全な因果関係が求められる場合です。 例えば、機械学習モデルをデバッグする場合や、特徴量の全ての影響を特定することが法的に求められている場合、おそらく因果属性が必要になるでしょう。このような場合は、以下のことは無視してください。そうでない場合、つまり一般の人々や時間のない人々へ説明する場合、次のセクションはきっと興味深いものとなります。 2.6.1 説明とはなにか 説明とは原因を問う疑問への解答です。(Miller 2017) どうして治療が患者に効かなかったのか？ どうして私のローン申請は却下されたのか？ どうして私たちはまだエイリアンからコンタクトを受けていないのか？ はじめの２つの疑問は「日常」の説明で答えることができる一方、３つ目の疑問は、より一般的な科学現象と哲学的疑問のカテゴリーから生じています。 ここでは、解釈可能な機械学習に関係している「日常」タイプの質問に絞って考えます。 「どのように」で始まる疑問は、「なぜ」で始まる疑問に言い換えることができます。 例えば、「どのように私のローン申請は却下されましたか？は「なぜ私のローン申請は却下されたのですか？」に変換できます。 以降では、「説明」という用語は説明の社会的、及び認知的なプロセスだけでなく、これらのプロセスの産物も含まれます。 説明をする人は、人間の場合も機械の場合もありえます。 このセクションでは、「良い」説明に関する Miller の要約をさらに凝縮し、解釈可能な機械学習に対する具体的な意味を付け加えます。 説明は対照的です(Lipton 19909)。 人間は普段、ある予測をした理由について尋ねませんが、その予測が他の予測の代わりにされた理由を尋ねます。 私たち人間は、事実に反する事例、つまり「もし入力Xが違っていたら、予測はどうなっていたか」を考える傾向があります。 例えば、住宅価格の予測については、住宅所有者は予測価格が予想よりも高かった場合、その理由に興味があるでしょう。 もしローン申請が却下されたなら、却下の原因に対する一般論ではなく、自分がローンを獲得するために変える必要がある申請の要素に興味があるでしょう。 ただ単に、却下された申請と、おそらく通るであろう申請の違いについて知りたいのです。 こうした対照的な説明が重要であるという認識は、説明可能な機械学習にとっても重要な発見です。 ほとんどの解釈可能なモデルは、１つのデータに対する予測と、人工的なデータもしくはデータの平均に対する予測とを暗黙的に対比する説明をするが出来ます。 医師は、「どうして薬は患者に効かなかったのか」と尋ねるかもしれません。 この場合は、医師が必要とするのは、薬が効いた患者と、似た状況で薬が効かなかった患者との比較による説明です。 比較による説明は完全な説明よりも理解が簡単です。 なぜ薬が効かないのかという医師の質問への完全な説明には次のようなものがあります。 「患者は10年間この病気にかかっていて、11の遺伝子が過剰発現しているため、体内で薬が非常に素早く効果のない化学物質に分解してしまい...」というように、比較による説明は、より簡潔です。 「薬の効く患者と対照的に、薬の効かない患者は薬の効果を下げる特定の遺伝子の組み合わせを持っています。」 最良の説明というのは、興味のある対象と、参照する対象の最も大きな違いを強調できるものです。 解釈可能な機械学習の意味: 人間は予測についての完璧な説明は求めませんが、違いが何であったかを他のインスタンスの予測（これは人工的でもかまいません）と比較したいと考えます。 対照的な説明をするには、比較のためのデータ点が必要となるため、アプリケーションに依存します。また、これは説明されるデータ点だけでなく、説明を受けるユーザーにも依存する可能性があります。 対照的な説明を自動的に生み出すための手法として、データ内にプロトタイプ、または、典型を見つける方法もあります。 説明は選択的です。 人々は、イベントの原因の完全なリストを網羅するような説明は期待していません。 むしろ人間は、考慮され得る様々な原因から1つまたは2つを説明として選択することに慣れています。 その証拠として、テレビニュースを考えてください。 「株価の下落は、最新のソフトウェアアップデートの問題によって同社の製品に対する反発が高まっていることが原因です。」 「Tsubasa と彼のチームはディフェンスが弱かったので試合に負けました。対戦相手が戦略を実行する余地を与え過ぎたのです。」 「国立機関と政府に対する不信の高まりが原因で、投票率が下落しました。」 ある出来事がさまざまな原因で説明できることを羅生門効果と呼びます。 羅生門は、武士の死について別の矛盾した物語（説明）を伝える日本の映画です。 機械学習モデルの場合、別の特徴量を用いたとしても適切な予測ができれば有利です。 異なる特徴（異なる説明）を持つ複数のモデルを組み合わせるアンサンブル学習は、これらの「物語」を平均化してよりロバストかつ正確に予測するため、ほとんどの場合で性能が よくなります。しかし、それは特定の予測が行われた理由の説明となる選択肢が複数あることも意味してます。 解釈可能な機械学習に対する意味： 世界がより複雑であっても、1つから3つの理由だけを用いて端的な説明をしてください。 LIME という手法を用いると、上記のことができます。 説明は社会的です。 説明は説明する人と説明を受ける人との会話や相互作用の一部です。 社会的な文脈が説明の内容と性質を決定します。 デジタル暗号通貨がなぜそれほど価値があるのか​​を技術者に説明したい場合、次のように説明するでしょう： 「中央のエンティティでは制御できない非中央集権的な分散型ブロックチェーンを基盤とした台帳は、富を安全に保持したい人々の共感を呼んでいます。これによって、需要と価格が高くなるのです。」 しかし、祖母に説明する場合には： 「ほら、おばあちゃん：暗号通貨はコンピューター上の金のようなものなんだ。みんな金が好きで、金のためにお金を払うけれど、若い人はコンピュータ上の金が好きでお金を払っているんだ。」 解釈可能な機械学習に対する意味： 機械学習アプリケーションの社会的な位置付けと対象とする利用者に注意を払ってください。 機械学習モデルの社会的な部分を正しく把握することは、アプリケーションに完全に依存します。 人文科学の専門家（心理学者や社会学者など）を見つけて支援を受けてください。 異常に焦点を当てた説明 人々は出来事を説明するために、より正常でない原因に焦点を当てます (Kahnemann and Tversky, 198110)。 これらは、確率が低いにも関わらず発生してしまった原因です。 これらの正常でない原因を排除することで、結果は大きく変わるでしょう（反事実的説明）。 人々はこれらの「異常な」原因を良い説明とみなします。 Štrumbelj と Kononenko (2011)11の次のような例があります： 先生と生徒間のテストに関するデータセットがあると仮定します。 生徒は授業に出席し、プレゼンテーションに成功するとその授業に合格できます。 先生は、生徒の知識をテストする試験を追加で行う選択肢があります。 これら試験の質問に答えられない生徒はその授業に落第します。 生徒はさまざまなレベルで準備できます。これは、先生の試験に正しく答えるための様々な確率へと変換できます（テストを実施する場合）。 生徒が授業に合格するかどうかを予測し、その予測の理由も説明してみましょう。 先生が追加の試験を実施しなければ合格率は100％です。そうでなければ、合格率は生徒の準備レベルと質問に正しく答える結果の確率に依存します。 シナリオ1： 先生は、100回のうち95回というように、常に生徒に対して追加の試験を実施するとしましょう。 （質問の部分に合格する可能性が10％の）勉強しなかった生徒は、幸運な生徒ではなく、正解できない追加の試験を受けます。 なぜ生徒は授業に落第したのでしょうか？ それは、生徒が勉強しなかったからです。 シナリオ2： 先生は、100回のうち2回というように、稀にしか試験を実施しないとしましょう。 試験の勉強をしていない生徒でも、試験が実施される可能性が低いため、授業に合格する可能性が高いと予測されます。 ある生徒は試験の準備をしていないので、10％の確率でしか試験に合格できません。 そして、不運にも、先生は彼が答えられない追加の試験を実施したので、授業に落第しました。 彼はなぜ落第してしまったのでしょうか？ この場合、より適した説明は「先生が生徒に対して試験を実施したから」だと言えます。 先生が試験を実施する可能性は低いので、先生は普通ではない行動をしたと言えます。 解釈可能な機械学習に対する意味： 予測の入力特徴の1つが（カテゴリ特徴のうち、ごく稀なカテゴリなど）何らかの意味で異常であり、その特徴が予測に影響を与えた場合、たとえ他の「正常な」特徴が予測に対して同じ影響を与えたとしても、説明に含めるべきです。 住宅価格予測の例の異常な特徴として、極めて高価な家には2つのバルコニーがあることかもしれません。 2つのバルコニーがあるということが、いくつかの方法で、家の大きさ、良好な近隣住民、または最近リフォームされたことが同じくらい価格差に寄与していることがわかったとしても、異常な特徴である「2つのバルコニー」という特徴は、その家が高価格となっている理由の最良の説明となるでしょう。 説明は真実。 適切な説明は実際に（他の状況で）真実であることを証明します。 しかし気がかりなことに、これは「適切な」説明にとって最も重要な要素ではありません。 例えば、選択性は真実性よりも重要であると考えられます。 たった1つか2つの原因のみによる説明が、関連する原因全てを網羅することはめったにありません。選択性は一部の真実を省略します。 例として、1つか2つの要因だけで株式市場の暴落を引き起こすことはありません。真実としては、何百万もの人々が最終的に暴落を引き起こす行動をとるように影響を与えた何百万もの原因があるのです。 解釈可能な機械学習に対する意味： 説明は出来事をできるだけ正確に説明する必要があります。これは、機械学習において忠実性 (fidelity)と呼ばれることがあります。したがって、2つのバルコニーが家の価格を上げると主張する場合、それは他の家（あるいは少なくとも同じような家）にも当てはまるべきです。 人間にとって、説明の忠実性は、その選択性、対照性、社会的側面ほど重要ではありません。 よい説明は、説明対象者の事前の信念と一貫している。 人間は、自分が信じていることと矛盾する情報を無視する傾向があります。 この効果は確証バイアスと呼ばれます (Nickerson 199812)。 説明はこの種の偏見から免れることができません。 人間は自分の信念と矛盾する説明を軽んじたり無視したりする傾向があります。 信念は人によって異なりますが、政治的世界観など集団に基づくものもあります。 解釈可能な機械学習に対する意味： よい説明は人間が信じていることと一貫しています。 これを機械学習に統合するのは難しく、統合できたとしても、おそらく予測性能を大幅に損なうことになります。 家の大きさが予測価格に与える影響について、我々は家が大きいほど価格が高くなると信じています。 いくつかの家の予測価格に対して、家のサイズがマイナスの影響を与えていたとモデルが示していたとします。 モデルは予測性能を向上させるために、（いくつかの複雑な相互作用の影響によって）このように学習しましたが、この振る舞いは我々が信じていることと強く矛盾しています。 （特徴量が一方向の予測にのみ影響を与えるような）単調性制約を適用するか、この性質を持つ線形モデルなどを使用できます。 適切な説明は一般的でもっともらしいものである 多くの出来事を説明できる原因は非常に一般的であり、よい説明と見なされます。 ただし、これは普通でない原因がよい説明であるという主張と矛盾していることに注意してください。 著者の考えでは、異常な原因による説明は一般的な原因による説明を上回っています。 異常な原因は定義上、特定の文脈や状況において稀なものです。 異常な出来事がない場合、一般的な説明はよい説明であると見なされます。 人間は同時に起こる出来事の確率を誤解する傾向があることを忘れないでください。 （Joeは司書です。彼は恥ずかしがり屋である可能性が高いですか、それとも本を読むのが好きな恥ずかしがり屋である可能性が高いですか？） 「家が大きいのでその価格は高い」というのは分かりやすい例です。これは非常に一般的で、家が高いあるいは安い理由をよく説明しています。 解釈可能な機械学習に対する意味： 一般性は、特徴量のサポートによって簡単に測定できます。これは、説明が適用されるインスタンスの数をインスタンスの総数で割ったものです。 Lipton, Peter. &quot;Contrastive explanation.&quot; Royal Institute of Philosophy Supplements 27 (1990): 247-266.↩ Kahneman, Daniel, and Amos Tversky. &quot;The Simulation Heuristic.&quot; Stanford Univ CA Dept of Psychology. (1981).↩ Štrumbelj, Erik, and Igor Kononenko. &quot;A general method for visualizing and explaining black-box regression models.&quot; In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).↩ Nickerson, Raymond S. &quot;Confirmation Bias: A ubiquitous phenomenon in many guises.&quot; Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998).↩ "],["data.html", "Chapter 3 データセット", " Chapter 3 データセット この本を通して、全てのモデルや手法は、オンライン上に無料で公開されているデータセットに対して適用されています。 それぞれのタスク(クラス分類、回帰、テキスト分類)で様々なデータセットを使用しています。 "],["bike-data.html", "3.1 自転車レンタル (回帰)", " 3.1 自転車レンタル (回帰) このデータセットには、ワシントンDCにある自転車レンタル会社の、日毎の自転車の貸し出し数が含まれていて、それに加え、天気と季節の情報があります。 データはCapital-Bikeshareによって、親切にももすべての人が使用できるように作られました。Fanaee-T and Gama (2013)13によって、天気と季節の情報が加えられました。目標は、季節や日毎に自転車がどれほど貸し出されるのかを予測することです。データは、 UCI Machine Learning Repositoryからダウンロードできます。 新しい特徴量がデータセットに追加されました、しかし、この本では全ての特徴量を例として使っている訳ではありません。 以下に今回使われた特徴量の一覧を記しておきます。 自転車のレンタル台数は回帰の問題の中でターゲット（目標）として使われています。 自転車のレンタル台数は未登録ユーザと登録されたユーザを含んでいます。 季節(春、夏、秋、冬) その日が祝日であったかどうか 年(2011年または2012年) 2011年の1月1日（この日がデータセットの中で最初の日）からの日数 この特徴量は時間変化に関するトレンドを考慮するために導入されました。 その日が平日であったか週末であったか その日の天候。下記の情報をそれぞれ1つずつ 晴天、少々曇り、少しだけ曇り、曇り 霧＋雲、霧＋所々曇り、霧＋少しの雲、小雨＋さざれ雲 豪雨＋凍雨＋雷雨＋霧、雪＋霧 気温（摂氏） 比較的温度（0 - 100％） 風速（km/h） この本で使用する具体例のために、データは少し処理されています。データ処理のためのRのスクリプトは、GitHubのリポジトリで取得できます。処理後のデータは、ここから取得できます。 Fanaee-T, Hadi, and Joao Gama. &quot;Event labeling combining ensemble detectors and background knowledge.&quot; Progress in Artificial Intelligence. Springer Berlin Heidelberg, 1–15. doi:10.1007/s13748-013-0040-3. (2013).↩ "],["spam-data.html", "3.2 YouTube スパムコメント (テキスト分類)", " 3.2 YouTube スパムコメント (テキスト分類) テキスト分類の例として、5つのYouTubeビデオについた1956件のコメントを用います。ありがたいことに、このデータセットを使用したスパム分類に関する論文の著者がデータを自由に利用可能な状態にしてくれました。(Alberto, Lochter, and Almeida (2015)14) コメントは2015年の上半期の再生回数上位10件のうちの5つからYouTube APIを用いて収集されました。5件すべてはミュージックビデオで、そのうちの1つは、韓国のアーティスト Psy による ”カンナムスタイル” です。その他のアーティストは、Katy Perry, LMFAO, Eminem, Shakira です。 コメントのいくつかを見てみましょう。 コメントは、手動でスパムかそうでないかラベルがつけられています。 スパムは 1 というラベルで表され、スパムでない場合は 0 で表されます。 CONTENT CLASS Huh, anyway check out this you[tube] channel: kobyoshi02 1 Hey guys check out my new channel and our first vid THIS IS US THE MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment and please subscribe!!!! 1 just for test I have to say murdev.com 1 me shaking my sexy ass on my channel enjoy ^_^ 1 watch?v=vtaRGgvGtWQ Check this out . 1 Hey, check out my new website!! This site is about kids stuff. kidsmediausa . com 1 Subscribe to my channel 1 i turned it on mute as soon is i came on i just wanted to check the views... 0 You should check my channel for Funny VIDEOS!! 1 and u should.d check my channel and tell me what I should do next! 1 実際にYouTube にアクセスしてコメントを確認できます。 しかし、YouTube の地獄に巻き込まれて、猿がビーチで観光客からカクテルを盗んで飲む動画を見ないようにしてください。 Google のスパム検知は 2015 年から大きく変更されているかもしれません。 視聴率記録を更新した「Gangnam Style」の動画はこちら. もしこのデータで遊びたいのならば、この本のGitHubレポジトリに RDataファイル があります。また、便利な関数はここR-script にあります。 Alberto, Túlio C, Johannes V Lochter, and Tiago A Almeida. &quot;Tubespam: comment spam filtering on YouTube.&quot; In Machine Learning and Applications (Icmla), Ieee 14th International Conference on, 138–43. IEEE. (2015).↩ "],["cervical.html", "3.3 子宮頸がんのリスク要因(クラス分類)", " 3.3 子宮頸がんのリスク要因(クラス分類) 子宮頸がんデータセットは、女性が子宮頸がんにかかるか否かを予測するための指標とリスク要因を含んでいます。 特徴量は、人口統計データ(年齢など)、生活スタイル、病歴を含みます。 データは Fernandes, Cardoso, Fernandes(2017)15 によって作成されており、UCI機械学習リポジトリ からダウンロードできます。 この本で例として使われる特徴量のサブセットは以下のとおりです。 年齢 性交渉の相手の人数 初めての性交渉の年齢 妊娠の回数 喫煙の有無 喫煙の継続年数 ホルモン避妊薬の使用有無 ホルモン避妊薬の使用年数 子宮内避妊器具(IUD)の使用有無 子宮内避妊器具(IUD)の使用年数 性感染症(STD)の感染歴の有無 STDの診断数 最初にSTDと診断された時点 最後にSTDと判断された時点 生検結果「健康」または「がん」(目的変数) 生検は子宮頸がん診断において精度が高いため、広く容認された手法として用いられます。 この本の例では、生検結果は目的変数として使われます。 各列での欠損データは、値が無いということ自体が確率に相関性を持ち得るため良くない方法かもしれませんが、最頻値（最も頻繁に登場する値）で補完されます。 質問は非常にプライベートな性質のものであるため、バイアスがあるかもしれません。 しかしこの本は欠損値の補完に関する本ではないため、最頻値補完は例としては十分でしょう。 このデータセットのこの本での例を再現するには、この本のGitHubリポジトリ内を探して下さい。 前処理のR-scriptと最終のRDataファイル Fernandes, Kelwin, Jaime S Cardoso, and Jessica Fernandes. &quot;Transfer learning with partial observability applied to cervical cancer screening.&quot; In Iberian Conference on Pattern Recognition and Image Analysis, 243–50. Springer. (2017).↩ "],["simple.html", "Chapter 4 解釈可能なモデル", " Chapter 4 解釈可能なモデル 解釈可能性を手に入れる最も簡単な方法は、 解釈可能なモデルを作るようなアルゴリズムのみを使うことです。 線形回帰や、ロジスティック回帰、決定木は一般的に用いられる解釈可能なモデルです。 以下の章ではこれらのモデルについて紹介します。 既に多くの本や動画、チュートリアル、論文、その他閲覧可能なものが存在するので、細部には拘らず、基本的なことのみに留めます。 特に、モデルの解釈方法に焦点を当てます。 本書では線形回帰、ロジスティック回帰、その他の線形回帰の拡張、決定木、決定規則、RuleFitについてより詳細に記述されています。 また、その他の解釈可能なモデルもいくつか挙げています。 本書で説明されている全ての解釈可能なモデルは、k近傍法を除いてモジュールレベルで解釈可能です。 後述の表は解釈可能なモデルの種類と特性を示しています。 特徴量と目的変数との関係が線形にモデル化されている場合，そのモデルは線形であると言います。 単調制約を持つモデルは、特徴量の全域にわたって、特徴量と目的変数が常に同じ方向に変化することを保証します。 特徴量が増加すると、目的変数は常に増加する、または減少するかのどちらかになります。 単調性は関係性を理解しやすくするため、モデルの解釈に有用です。 一部のモデルでは、目的変数の出力を予測するための特徴量間の交互作用を自動的に含めることができます。 手動で特徴量の交互作用を含めることで、あらゆる種類のモデルに交互作用を含めることができます。 交互作用項を導入することで、予測性能を改善できますが、多すぎるもしくは複雑すぎる交互作用は解釈性を損なう可能性があります。 いくつかのモデルは、回帰または、分類のみであり、どちらも対応可能なモデルもあります。 この表から、回帰 (regr) もしくは分類 (class) のタスクに対して、適切な解釈可能モデルを選べます。 アルゴリズム 線形性 単調性 交互作用 タスク 線形回帰 あり あり なし regr ロジスティック回帰 なし あり なし class 決定木 なし いくつか あり class, regr RuleFit あり なし あり class, regr 単純ベイズ法 なし あり なし class k近傍法 なし なし なし class, regr "],["limo.html", "4.1 線形回帰", " 4.1 線形回帰 線形回帰モデルは予測値を特徴量の重み付き和として表します。 学習された関係の線形性が解釈を簡単にしてくれます。 線形回帰モデルは長い間、定量的な問題に取り組む統計学者や計算機科学者たちによって使用されてきました。 線形モデルでは、特徴量 x が目的変数 y にどれくらい依存するかをモデリングできます。 学習された関係は線形で、1つのデータ i に対して次のように表すことができます。 \\[y=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}+\\epsilon\\] 予測結果は p 個の特徴量の重み付き和です。 \\(\\beta_{j}\\) は学習された特徴の重み、つまり係数を表します。 1つ目の重み (\\(\\beta_0\\)) は切片と呼ばれ、特徴量と乗算はしません。 \\(\\epsilon\\) は予測と実際の結果との差、つまり誤差です。 これらの誤差値はガウス分布に従うと仮定されます。誤差が正の方向にも負の方向にも存在して、小さい誤差は多く、大きい誤差は少ないと仮定するという意味です。 最適な重みを推定するために様々な方法が用いられます。 実際の結果と推定結果との差の二乗和を最小化するような重みを求めるために、最小二乗法がよく用いられます。 \\[\\hat{\\boldsymbol{\\beta}}=\\arg\\!\\min_{\\beta_0,\\ldots,\\beta_p}\\sum_{i=1}^n\\left(y^{(i)}-\\left(\\beta_0+\\sum_{j=1}^p\\beta_jx^{(i)}_{j}\\right)\\right)^{2}\\] 具体的に最適な重みがどう求められるかはここでは論じませんが、興味があれば、&quot;The Elements of Statistical Learning&quot; (Friedman, Hastie and Tibshirani 2009)16 の 3.2 章や他の線形回帰に関するオンライン資料を参考にしてください。 線形回帰の最大の利点は線形性です。 線形性により予測手順が簡単になります。最も重要なことは、これらの一次方程式がモジュールレベル（つまり重み）で理解しやすくなります。 これが、線形モデルおよび同様のモデルが、医学、社会学、心理学の学術分野および他の多くの定量的研究分野で広く普及している主な理由の 1つです。 例えば、医療分野では、患者の臨床成果の予測だけでなく、薬の影響を定量化し、同時に性別、年齢、その他の特徴を解釈可能な方法で考慮することが重要です。 推定される重みには信頼区間があります。 信頼区間とは、ある信頼度で「真」の重みを含むような重み推定の範囲です。 例えば、重み2に対する信頼度 95％ の信頼区間は1から3の範囲になる可能性があります。 この信頼区間は、「回帰モデルが与えられるデータに正しいという仮定のもとで、新しくサンプリングされたデータに対して推定を 100 回繰り返したときに、この信頼区間は 100 回中 95 回真の重みを含む」と解釈できます。 モデルが「正しい」かはデータ中の関係が、線形性、正規性、等分散性、独立性、固定された特徴量、また、多重共線性の欠如 というこれらの仮定を満たしているかどうかによります。 線形性 線形回帰モデルは予測が特徴量の線形結合になることを要請しますが、これは線形回帰の最大の強みでもあり、最大の制約でもあります。 線形性は解釈可能なモデルに繋がります。 線形効果は定量化しやすく、説明しやすいです。 線形的に足されるため、効果を分離することも簡単です。 特徴量の相互作用や、特徴量と目的値の間の非線形性が疑われる場合は、相互作用項を追加するか、スプライン回帰を使うことができます。 正規性 与えられた特徴量に対する目的値は正規分布に従うと仮定されます。 もしこの仮定が破れると、特徴量に対する重みの推定信頼区間は意味を持たなくなります。 等分散性 誤差項の分散は、特徴空間全体において一定であると想定されます。 平方メートルで与えられる居住面積に基づいて住宅の価格を予測するとします。 家の大きさに関わらず、予測値における誤差の分散が一定であると想定する線形モデルを作ります。 この仮定は、実際には現実に反します。 住宅の例でいうと、家の価値が高いほど価格変動の余地が大きく、価格の予測値における誤差項の分散が大きくなるかもしれません。 線形回帰モデルの平均誤差（予測価格と実際の価格との差）が 50,000 ユーロであるとしましょう。 等分散性を想定すると、平均 50,000 の誤差が 100 万ユーロの家と 4 万ユーロの家とに対して同じであるとすることになります。 これは家の価格がマイナスになりうるということになり、合理的ではありません。 独立性 各データは互いに独立であることが想定されます。 患者ごとに血液検査を複数回行うなど、繰り返して測定する場合、データ点は互いに独立になりません。 従属なデータに対しては混合効果モデルや GEE などの特殊な線形回帰モデルが必要です。 このような場合に、「標準的な」線形回帰モデルを使ってしまうと、モデルから誤った結論を導き出してしまう恐れがあります。 固定された特徴量 入力特徴量は「固定されている」と見なします。 固定されているとは、入力特徴量が統計的な変数でなく、「与えられた定数」として扱われるということです。 これは測定誤差がないことを意味します。 これは、かなり非現実的な仮定です。 しかし、この仮定なしでは、入力特徴の測定誤差を説明するために非常に複雑な測定誤差モデルを使う必要があります。 通常、そんなことはしたくないでしょう。 多重共線性の欠如 強い相関関係にある特徴量は、重みの推定を台無しにするので望ましくありません。 2つの特徴量が強く相関している場合、特徴量の効果が相加的であり、相関する特徴量のどちらが影響を与えているのか不定となるため、重みを推定するときに確定できません。 4.1.1 解釈 線形回帰モデルの重みの解釈は、対応する特徴量のタイプに依存します。 量的特徴量 (Numerical feature): 量的特徴量の値が1単位分増えると、推定結果が重み分変化します。量的特徴量の一例としては、家の大きさが挙げられます。 バイナリ特徴量 (Binary feature): それぞれのインスタンスごとに、2つの値のどちらか一方のみを取る特徴量のことです。 そのような特徴量の一例として、庭付きの家であるかどうかが挙げられます。値のうちの1つは&quot;庭付きでない&quot;のような参照カテゴリ（プログラミング言語によっては 0 と符号化されます）として数えられます。ある特徴量の参照カテゴリが別のカテゴリに変化すると、推定結果は特徴量の重み分変化します。 複数のカテゴリを含んだカテゴリカル特徴量: 取りうる値の数が固定された特徴量のことです。一例として、&quot;カーペット&quot;、&quot;ラミネート&quot;、&quot;寄木細工&quot;というカテゴリを取りうる”床の種類”という特徴量が挙げられます。 多くのカテゴリを扱う解決策の1つとして、one-hot エンコーディングというものがあります。one-hot エンコーディングとは、それぞれのカテゴリごとに 0, 1 の値を取るカラムを設定するエンコーディングの手法です。 L 個のカテゴリがあるカテゴリカル特徴量に対しては、L-1 個のカラムがあれば十分です。なぜならば、L 番目のカラムは冗長な情報だからです（例えば、ある1つのインスタンスに対して 1 番目から L-1 番目のカラムが全て 0 であるとき、そのインスタンスのカテゴリは L 番目のものであることが分かるからです）。 各カテゴリに対する解釈はバイナリ特徴量に対する解釈と同じです。 R のような言語では、この章で後ほど説明するような様々な方法でカテゴリカル特徴量をエンコードすることが出来ます。 切片\\(\\beta_0\\): 切片は、全てのインスタンスについて常に1をとる”定数特徴量”に対する重みであるとみなすことが出来ます。 大抵のソフトウェアパッケージは切片を推定するために定数特徴量を自動的に追加します。 これに対する解釈は以下の通りです。 全ての量的特徴量が0であり、カテゴリカルデータの値が参照カテゴリであるインスタンスの場合には、モデルの予測値は切片の重みになります。 そのような、全ての特徴量の値が0であるようなインスタンスは多くの場合意味をなさないため、切片の解釈は通常関係がありません。 切片の解釈は、全ての特徴量が平均0、標準偏差1に標準化されているときにのみ意味があるものとなります。 そのような場合、切片は全ての特徴量が平均値であるようなインスタンスの予測結果を反映しています。 線形回帰モデルの特徴量の解釈は以下のテンプレートを用いて自動化できます。 量的特徴量の解釈 特徴量 \\(x_{k}\\) が 1 だけ増えて、その他の特徴量は固定されている場合、予測結果 y は \\(\\beta_k\\) だけ増えます。 カテゴリカル特徴量の解釈 特徴量 \\(x_{k}\\) が参照カテゴリから他のカテゴリに変化し、その他の特徴量は固定されている場合、予測結果 y は \\(\\beta_{k}\\) だけ増えます。 もう1つの重要な指標は、決定係数 \\(R^2\\) です。 \\(R^2\\) はモデルによって、どの程度、目的値の全てのばらつきが説明されているかを教えてくれます。 \\(R^2\\) が高くなればなるほど、そのモデルはデータを説明していることになります。 \\(R^2\\) の計算方法は以下の通りです。 \\[R^2=1-SSE/SST\\] SSE は、二乗和誤差 (Squared Sum of the Error)です。 \\[SSE=\\sum_{i=1}^n(y^{(i)}-\\hat{y}^{(i)})^2\\] SST は、データの分散の二乗和です。 \\[SST=\\sum_{i=1}^n(y^{(i)}-\\bar{y})^2\\] SSE は線形回帰で学習した後、どの程度のばらつきが残っているかを教えてくれます。これは、予測と実際の目的値の間の二乗誤差を計測することで求めます。 SST はデータ自体の目的値の全体の分散です。 \\(R^2\\) は、線形モデルによって、どの程度ばらつきが説明できているのかを教えてくれます。 \\(R^2\\) は 0 のとき、モデルはデータを全く説明できていないのに対して、1 のときは、データのばらつきを完全に説明できていることを意味します。 実は、たとえ目的値に対する情報を全く含んでいない特徴量を付け加えたとしても、モデルの特徴量の数を増やすと \\(R^2\\) の値を増加させることができます。 そのため、モデルで使われている特徴量の数を反映した、自由度調整済みの決定係数 \\(R^2\\) を使用することが推奨されます。 その計算方法は次のようになります。 \\[\\bar{R}^2=1-(1-R^2)\\frac{n-1}{n-p-1}\\] ただし、 p は特徴量の数で、n はインスタンスの数です。 (自由度調整済み)決定係数 \\(R^2\\) がとても低いモデルに対しては、そもそもモデルがデータをうまく説明できていないため、そのモデルの解釈をしても意味がありません。 どのように重みの解釈しても、意味がないでしょう。 特徴量重要度 (Feature Importance) 線形回帰モデルの特徴量の重要度は t 統計量の絶対値で計測できます。 t 統計量は標準誤差でスケーリングされた推定重みです。 \\[t_{\\hat{\\beta}_j}=\\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\\] この式は何を意味しているのでしょうか。 特徴量の重要度は、重みが増えると増加します。 これは理にかなっています。 推定された重みに、よりばらつきがあると (正しい値に関しての確証が小さくなる)、その特徴量に対する重要度は小さくなります。 これも理にかなっています。 4.1.2 例 この例では、線形回帰モデルを使って、天気や日付情報が与えられたときの自転車レンタル台数 の予測をします。 解釈のために、回帰の重みについて調べます。 特徴量は、量的特徴量、カテゴリカル特徴量のどちらもがあります。 各特徴に対して、推定された重み、推定値の標準誤差 (SE)、及び t 統計量の絶対値(|t|)を表に示します。 Weight SE |t| (Intercept) 2399.4 238.3 10.1 seasonSUMMER 899.3 122.3 7.4 seasonFALL 138.2 161.7 0.9 seasonWINTER 425.6 110.8 3.8 holidayHOLIDAY -686.1 203.3 3.4 workingdayWORKING DAY 124.9 73.3 1.7 weathersitMISTY -379.4 87.6 4.3 weathersitRAIN/SNOW/STORM -1901.5 223.6 8.5 temp 110.7 7.0 15.7 hum -17.4 3.2 5.5 windspeed -42.5 6.9 6.2 days_since_2011 4.9 0.2 28.5 量的特徴量の解釈について (temperature): 摂氏が1度上昇したとき、他の特徴量は全て同じとすると、自転車のレンタル台数の予測は 110.7 だけ増加します。 カテゴリカル特徴量の解釈について (weathersit): 自転車のレンタル数の予測値は、天気の良い日と比べて、雨や雪や嵐のとき -1901.5 だけ変化します。ただし、このときも他の特徴量は変化しないことを想定しています。 晴れた日に比べて、霧が濃い時は、自転車のレンタル台数の予測値は -379.4 だけ変化します。 全ての解釈では必ず、&quot;他の特徴量は変化させない&quot;という脚注がついています。 これは、線形回帰モデルの性質です。 予測値は、重み付けされた特徴量の線形和です。 推定された線形方程式は特徴量/予測値空間の超平面です (単一の特徴量の場合単純な直線になります)。 重みは各方向の超平面の傾き(勾配)を指定しています。 良い点としては、加法性によって個々の特徴量の効果を他の全ての特徴量から分離できることです。 これが可能なのは、全ての特徴量の効果(重みと特徴量の積)が和によって組み合わされているからなのです。 悪い点としては、この解釈は特徴量の同時分布を無視していることです。 ある特徴量を増やすが、他の特徴量は変化させないとき、非現実的か好ましくないデータが得られるかもしれません。 例えば、家の大きさを広くすることなく、部屋の数を増加させることは現実的ではないでしょう。 4.1.3 可視化による解釈 種々の可視化手法を用いることで、線形回帰モデルを簡単かつ素早く把握できます。 4.1.3.1 重みプロット (Weight Plot) 重みの情報（重みと分散の推定値）は重みプロットを用いることで簡単に可視化できます。 以下のプロットでは、先ほどの線形回帰の結果を可視化しています。 FIGURE 4.1: 重みが点として、95%信頼区間が線として表示されている。 重みプロットは、自転車のレンタル台数を予測するにあたって、雨、雪、嵐といった天気は大きな負の影響を及ぼすことを示しています。 稼働日数の重みはほぼ 0 になっており、かつ 95% 信頼区間に 0 は含まれています。従って、この特徴量は統計的に重要でないことが示されます。 気温についても同様のことが言える可能性があります。 重みプロットにおける問題は、各特徴量が異なるスケールで計測されているということです。 天気の推定された重みは晴れ、雨、嵐、雪のそれぞれの天気の違いを反映していますが、気温は 1 度の変化による影響を反映しています。 特徴をスケーリングすることで、線形回帰モデルを学習させる前に、推定された重みを比較できるようになります。（例えば標準化など） 4.1.3.2 影響力プロット (Effect Plot) 線形回帰モデルの重みは特徴量と掛け合わせることで、より意味のある分析になり得ます。重みは特徴量のスケールに依存し、例えば身長の単位をメートルからセンチメートルに変更すればまた異なる重みになります。 ただ、重みは変化したとしても、重みが実際にデータに及ぼす影響は変化しません。 また特徴量の分布を知ることは重要です。なぜなら、ある特徴量の分散が非常に低い場合、ほとんどのインスタンスがその特徴量から同じような影響を受けることを意味するからです。 影響力プロットは、重みと特徴の組み合わせがどれほど予測に貢献しているかを理解する助けになります。 影響力を計算するのは、各特徴ごとの重みと、特徴量をインスタンスごとに掛け合わせます。 \\[\\text{effect}_{j}^{(i)}=w_{j}x_{j}^{(i)}\\] 影響力は箱ひげ図を用いて可視化できます。 箱ひげ図内の各箱は、全データのうち半分のデータに対する影響力の範囲を表します（第1四分位点から第3四分位点まで）。箱内の縦線は中央値を表します。つまり、予測時に半数のインスタンスはこれよりも低い影響力をもち、もう半数は高い影響力を持つということです。 横線は \\(\\pm1.5\\text{IQR}/\\sqrt{n}\\) まで引かれていて、IQR は四分位範囲（inter quartile range）（第3四分位数から第1四分数を引いたもの）のことです。 FIGURE 4.2: 特徴量の影響力プロットは、特徴量ごとのデータに対する影響 (特徴量と重みの積) の分布を示している。 図から、自転車のレンタル台数の予測に大きく寄与している特徴量は、気温と日数の特徴であることがわかります。この特徴量は、自転車レンタルのトレンドを捉えていると言えます。 気温が予測にどれだけ寄与しているかの範囲はかなり広くなっています。日数の特徴量は 0 から大きな正の値に渡って寄与していることがわかります。これは、データセット内での初日（2011/1/1）ではトレンドの影響は非常に小さく、かつ推測された重みは正の値 (4.93) だったからです。これが意味するのは、影響力は毎日上昇し続け、データセット内の最終日（2013/12/31）に最大値を迎えるということです。 重みが負の値のとき、あるインスタンスが正の影響力を持っていたとすると、特徴量が負であるということに注意してください。 例えば、風速が大きく負の影響力を持っているような日は、風速がかなり強い日であるということです。 4.1.4 個々の予測に対する説明 あるインスタンスの各特徴量はどれだけ予測に貢献したのでしょうか。 この疑問への回答はインスタンスに対する効果を計算することで得られます。 インスタンスに固有の効果の解釈は各特徴量に関する効果の分布を比較することでのみ意味を持ちます。 自転車データセットの6番目のインスタンスに対する線形モデルの予測について説明します。 このインスタンスは以下の特徴量をもっています。 Feature Value season SPRING yr 2011 mnth JAN holiday NO HOLIDAY weekday THU workingday WORKING DAY weathersit GOOD temp 1.604356 hum 51.8261 windspeed 6.000868 cnt 1606 days_since_2011 5 このインスタンスの特徴量の効果を知るため、その特徴量と、それに対応する線形回帰モデルの重みの積を求めなくてはなりません。 特徴量 &quot;workingday&quot; の値 &quot;WORKING DAY&quot; に対する効果は 124.9 となります。 気温 1.6 度の効果は 177.6 です。 これらの個々の効果をデータ全体への効果の分布を示す図にX印としてプロットします。 これにより個々の効果とデータ全体の効果が比較できます。 FIGURE 4.3: 1つのインスタンスに対する影響力プロットは、影響力の分布を示し、興味のあるインスタンスの効果のハイライトする。 訓練データのインスタンスに対する予測を平均すると、4504 の平均となります. それと比較すると、6 番目のインスタンスによる自転車の台数予測は 1571 しかないことから、小さいといえます。 影響力プロットはその理由を明らかにしています。 箱ひげ図はデータセットにおける全インスタンスの効果分布を表し、X印は 6 番目のインスタンスの効果を示しています。 6 番目のインスタンスは、この日の気温が 2 度であり、その他のほとんどの日と比べて気温が低いことから、気温による影響は小さいと言えます ( 気温に対する重みは正であることに注意)。 また、トレンド特徴量である &quot;days_since_2011&quot; の効果も、このインスタンスが2011初頭(5 days) のものであることと、トレンドに対する重みが正であることから、影響力は小さいと言えます。 4.1.5 カテゴリカル特徴量のエンコーディング カテゴリカル特徴量をエンコーディングする方法は複数あり、選択した方法によって重みの解釈に影響が生じます。 線形回帰モデルにおいて標準となるのは treatment coding で、これは多くの場合で十分に機能します。 異なるエンコーディングの手法は1つのカテゴリカル特徴量から異なる計画行列を作り出すことに相当します。 この節では3つの異なるエンコーディングの方法について紹介しますが、他にも多くの手法が知られています。 使用する例では、6つのインスタンスと3つのカテゴリを持つカテゴリカル特徴量を持ちます。 最初の2つのインスタンスでは、特徴量はカテゴリAを取ります。 3番目、4番目のインスタンスでは、カテゴリーBを取り、最後2つのインスタンスはカテゴリーCを取ります。 Treatment coding treatment coding では、カテゴリごとの重みは、対応するカテゴリと参照カテゴリ間の予測の差の推定値とします。 線形モデルの切片は参照カテゴリの平均値です。(他の全ての特徴量が変わらない場合) 計画行列の最初の列は切片で常に 1 となります。 2 列目はインスタンス i がカテゴリ B かどうかを示しており、3 列目はインスタンスがカテゴリ C かどうかを示しています。 線形方程式が過剰になり、重みに対して一意な解を見つけることができなくなるため、カテゴリ A に対する列は必要ありません。 インスタンスがカテゴリ B にも C にも属していないことがわかれば十分なのです。 Feature matrix: \\[\\begin{pmatrix}1&amp;0&amp;0\\\\1&amp;0&amp;0\\\\1&amp;1&amp;0\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\1&amp;0&amp;1\\\\\\end{pmatrix}\\] Effect coding カテゴリごとの重みは、対応するカテゴリと全体の平均 (他のすべての特徴がゼロまたは参照カテゴリである場合) の推定された y の差です。 最初の列は切片を推定するために用いられます。 切片に関連づけられた重み \\(\\beta_{0}\\) は、全体の平均を表し、2 列目に対する重み \\(\\beta_{1}\\) は、 全体の平均とカテゴリ B との間の差となります。 したがって、カテゴリ B の全体の効果は \\(\\beta_{0}+\\beta_{1}\\) となります。 カテゴリ C に対する解釈も同様です。 参照カテゴリ A に対しては、\\(-(\\beta_{1}+\\beta_{2})\\)が全体の平均を表し、\\(\\beta_{0}-(\\beta_{1}+\\beta_{2})\\)が全体の影響となります。 Feature matrix: \\[\\begin{pmatrix}1&amp;-1&amp;-1\\\\1&amp;-1&amp;-1\\\\1&amp;1&amp;0\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\1&amp;0&amp;1\\\\\\end{pmatrix}\\] Dummy coding カテゴリごとの \\(\\beta\\) は各カテゴリに対する推定された y の平均値です。（その他の特徴量はゼロもしくは、参照カテゴリ） ただし、切片は、線形モデルの重みに対して一意な解が見つけられるように、省略されていることに注意してください。 Feature matrix: \\[\\begin{pmatrix}1&amp;0&amp;0\\\\1&amp;0&amp;0\\\\0&amp;1&amp;0\\\\0&amp;1&amp;0\\\\0&amp;0&amp;1\\\\0&amp;0&amp;1\\\\\\end{pmatrix}\\] カテゴリカル特徴量に対するエンコーディング手法にさらに興味がある場合は次のサイトをご覧ください。 概要サイト、 ブログ. 4.1.6 線形モデルは良い説明を与えるか? 人間に優しい説明の章で説明した、良い説明とは何かという観点で見ると、線形モデルは最良の説明を与えるというわけではありません。 これらは対照的ですが、参照インスタンスは全ての量的特徴量が0でありカテゴリカル特徴量は参照カテゴリであるようなデータ点となります。 これは大抵、現実的には起こりそうもない人工的で意味のないインスタンスです。 例外もあります： すべての量的特徴量が平均化（特徴量から特徴量の平均値を引いた値）されており、すべてのカテゴリ特徴量がエンコードされている場合、参照インスタンスは、すべての特徴量が平均値を取るデータ点となります。 これは実在しないデータ点かもしれませんが、少なくとも可能性が高く意味があるものかもしれません。 この場合、特徴量に重みを掛けたもの（feature effects）は、&quot;平均的なインスタンス&quot;と比較したときの予測結果への貢献度を説明しています。 良い説明のもう1つの側面は選択性であり、線形モデルにはおいては、より少ない特徴量を使うこと、もしくはスパースな線形モデルを用いることが挙げられます。 線形モデル自体では選択性を持つ説明は達成できないことに注意してください。 線形モデルは、線形方程式が特徴量と出力結果の関係を表す適切なモデルであるかぎり、正しい説明を与えます。 非線形性や交互作用が多いほど、線形モデルは正確ではなくなり、説明も真実味の欠如が起こります。 線形性はモデルに対する説明を、より一般的に単純にします。 人々が関係性を説明するために線形モデルを使う主な理由は、モデルの線形の性質によるものだと考えられます。 4.1.7 スパースな線形モデル 私が選んだ線形モデルの例は、どれも上手くいきましたよね？ しかし実際のデータでは、ほんの一握りの特徴量ではなく、何百、何千の特徴量を持ってるかもしれません。 そのときに、線形回帰モデルはどうなるでしょうか。 解釈性は下がります。 インスタンスよりも特徴量が多く、標準的な線形モデルでは学習ができないという状況に陥ってしまうこともあるかもしれません。 このような時は、線形モデルにスパース性（＝少数の特徴量）を導入することで解決できます。 4.1.7.1 Lasso Lasso は、線形回帰モデルにスパース性を導入するための便利な方法です。 Lasso は「least absolute shrinkage and selection operator」の略で、線形回帰モデルに適用すると、特徴量の選択と選択された特徴量の重みの正則化を行います。 以下の重みを最適化する最小化問題を考えてみましょう。 \\[min_{\\boldsymbol{\\beta}}\\left(\\frac{1}{n}\\sum_{i=1}^n(y^{(i)}-x_i^T\\boldsymbol{\\beta})^2\\right)\\] Lasso は、この最適化問題に新しく項を付け加えます。 \\[min_{\\boldsymbol{\\beta}}\\left(\\frac{1}{n}\\sum_{i=1}^n(y^{(i)}-x_{i}^T\\boldsymbol{\\beta})^2+\\lambda||\\boldsymbol{\\beta}||_1\\right)\\] \\(||\\boldsymbol{\\beta}||_1\\) の項は、重みに対する L1 ノルムであり、大きな重みに対するペナルティの役割があります。 L1 ノルムを使用しているため、多くの重みは 0 となり、他の重みは小さくなります。 パラメータ \\(\\lambda\\) は正則化効果の強さを制御し、通常はクロスバリデーションによって調整されます。 特に \\(\\lambda\\) が大きいと、多くの重みが 0 になります。 特徴量の重みは、ペナルティ項 \\(\\lambda\\) の関数として可視化できます。 各特徴量の重みは、次の図のように曲線で表すことができます。 FIGURE 4.4: 重みに対するペナルティが大きくなるにつれて、非ゼロの重みを持つ特徴量が少なくなっていきます。これらの曲線は解パス図とも呼ばれます。プロットの上の数字は、非ゼロの重みの数です。 \\(\\lambda\\) にはどのような値を選ぶべきでしょうか？ ペナルティ項をチューニングできるパラメータとして捉えれば、クロスバリデーションでモデル誤差を最小化する \\(\\lambda\\) を求めることができます。 \\(\\lambda\\) をモデルの解釈性を制御するパラメータとして考えることもできます。 ペナルティが大きければ大きいほど、モデルに存在する特徴量が少なくなり（重みがゼロになるので）、モデルの解釈性が良くなります。 Lassoを使用した例 Lasso を使ってレンタル自転車の数を予測してみましょう。 モデルに持たせたい特徴量の数を事前に設定しておきます。 まずは特徴量の数を 2 に設定してみましょう。 Weight seasonSPRING 0.00 seasonSUMMER 0.00 seasonFALL 0.00 seasonWINTER 0.00 holidayHOLIDAY 0.00 workingdayWORKING DAY 0.00 weathersitMISTY 0.00 weathersitRAIN/SNOW/STORM 0.00 temp 52.33 hum 0.00 windspeed 0.00 days_since_2011 2.15 Lasso により重みが 0 にならなかったのは2つの特徴量は、温度（&quot;temp&quot;）と時間トレンド（&quot;days_since_2011&quot;）です。 では、次に、5つの特徴量を選択してみましょう。 Weight seasonSPRING -389.99 seasonSUMMER 0.00 seasonFALL 0.00 seasonWINTER 0.00 holidayHOLIDAY 0.00 workingdayWORKING DAY 0.00 weathersitMISTY 0.00 weathersitRAIN/SNOW/STORM -862.27 temp 85.58 hum -3.04 windspeed 0.00 days_since_2011 3.82 &quot;temp&quot; と &quot;days_since_2011&quot; の重みが、先に示した2つの特徴量を持つモデルとは異なることに注意してください。 この理由は、\\(\\lambda\\) を減少させることで、2つの特徴量を持つモデルで選択された特徴量であっても、ペナルティが少なくなり、より大きな重みが得られる可能性があるからです。 Lasso の重みの解釈は、線形回帰モデルの重みの解釈に対応しています。 重みに影響するため、特徴量が標準化されているかどうかに注意を払う必要があります。 この例では、特徴量はソフトウェアによって標準化されていますが、重みは元の特徴量の尺度と一致するように自動的に変換されています。 線形モデルのスパース性のための他の方法 線形モデルの特徴量の数を減らすために、さまざまな手法があります。 前処理に関する方法 手動による特徴量選択: 専門家の知識・ドメイン知識を使うことで、特徴量の選択ができます。 自動化できないのが大きな欠点で、データを理解している人と協力する必要があります。 単変量選択: 例としては、相関係数があります。 特徴量と目的変数の相関関係が一定の閾値を超えた特徴量のみを選択します。 デメリットは、特徴量を単体でしか考えていないことです。 いくつかの特徴量は、線形モデルが他の特徴量を説明するまで相関を示さないかもしれません。 こういった場合、単変量選択法では見逃してしまいます。 段階的な方法 Forward selection: 1つの特徴量で線形モデルをフィットします。 これを特徴量ごとに行います。 最も性能の良いモデルを選択してください (例: 決定係数 \\(R^2\\) が最大)。 ここでもう一度、残りの特徴量について、現在の最良のモデルに新たな特徴量を1つ追加することで、異なるバージョンのモデルを学習させます。 そして、最も性能の良いモデルを選びます。 この作業をモデル内の特徴量の最大数など、ある基準に達するまで続けましょう。 Backward selection: この手法は Forward selection と似ています。 しかし、特徴量を追加するのではなく、すべての特徴量を含むモデルから始めて、どの特徴を削除すれば最高の性能向上が得られるかを試してみましょう。 これを、ある停止基準に達するまで繰り返します。 Lasso を使うことをお勧めする理由は、すべての特徴量を同時に考慮し、\\(\\lambda\\) を変更することで制御できるからです。 また、Lassoは、分類のためのロジスティック回帰モデルでも使用できます。 4.1.8 長所 予測値を重み付き和としてモデル化することで、予測値がどのように生成されるかの透明性を高くできます。 そして、Lasso を使用することで、使用される特徴量の数を減らすことができます。 多くの人が線形回帰モデルを使います。 これは、多くの場所で、予測モデルや推論実行のために線形モデルが受け入れられていることを意味します。 線形回帰に関して、高度な経験をもつ専門家がいたり、教材やソフトウェアなども豊富にあります。 線形回帰はR、Python、Java、Julia、Scala、Javascript、その他多数で使用できます。 数学的には、重みを推定するのは簡単で、最適な重みを見つけることができることが保証されています（線形回帰モデルのすべての仮定がデータによって満たされている場合）。 重みと一緒に、信頼区間、検定、強固な統計理論を得ることができます。 線形回帰モデルの拡張もたくさん知られています（GLM,GAMなどの章を参照）。 4.1.9 短所 線形回帰モデルは、線形関係しか表現できません。 非線形性や交互作用を考慮するには、手作業で新たに特徴量を作成する必要があります。 線形モデルは、学習できる関係が非常に制限されており、実際には複雑な関係を単純化しすぎているため、予測性能に関してもあまり良くないことが多いです。 重みの解釈は、他のすべての特徴量に依存しているため、直感的ではない場合があります。 出力 y と他の特徴量に対して強い正の相関のある特徴量は、線形モデルにおいては、負の重みとなる可能性があります。なぜなら、他にも相関のある特徴量がある場合、高次元空間において y と負の相関があるためです。 完全に相関のある特徴量がある場合は、線形方程式の一意の解を見つけることが不可能になります。 例: 家の価値を予測するモデルがあり、部屋数や広さなどの特徴量があります。 家の大きさと部屋の数は非常に強い相関関係があります。つまり、家が大きければ大きいほど、部屋数が多くなります。 線形モデルに両方の特徴量を使用した場合、家のサイズがより良い予測指標となり、大きな正の重みを取得することが起こるかもしれません。 そうすると、同じ広さの家でも、部屋数を増やすと価値が下がったり、相関関係が強すぎると線形方程式が安定しなくなったりするので、部屋の数に対する重みは負になるかもしれません。 Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. &quot;The elements of statistical learning&quot;. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009).↩ "],["logistic.html", "4.2 ロジスティック回帰", " 4.2 ロジスティック回帰 ロジスティック回帰は2つの可能な結果を伴う分類問題の確率をモデル化します。これは線形回帰モデルの分類問題への拡張です。 4.2.1 線形回帰モデルを分類のために使うと何がいけないか。 線形回帰モデルは回帰問題ではうまく働きますが、分類問題ではうまくいきません。 なぜでしょうか？ 2クラス分類の場合、あるクラスを 0、もう一方を 1 とラベル付けし、線形回帰モデルを使ったとしましょう。 技術的にはうまく働き、線形モデルは重みを計算します。 しかし、この方法にはいくらか問題があります。 線形モデルは確率を出力せず、クラスを数字として扱い、点との距離を最小化するような最適な超平面に (単一の特徴量に対しては線として) 適合させます。 それは、単に、点の間の補間をしているだけなので、確率として解釈できません。 また、線形モデルは外挿し、0 より小さかったり、1 より大きな値を出力します。 これは分類問題に対する、より良いアプローチがあるかもしれないというよい徴候です。 予測結果が確率ではなく、点の間の線形補間なので、クラスを分けるための、意味のある閾値というものは存在しません。 この問題に関するわかりやすい説明はStackoverflow に示されています。 線形モデルは多クラス分類には拡張できません。 次のクラスを 2 だったり、3 だったりとラベル付けするかもしれません。 クラスの順序に意味のないときもあるでしょうが、線形モデルにおいては特徴量と予測クラスに不自然な関係性を作ることが強制されます。 同じような値に偶然なったクラスが他のクラスと近くなかったとしても、正の重みをもつ特徴量の値が高ければ高いほど、予測されるクラスはより大きな数になりやすくなっていきます。 FIGURE 4.5: 線形モデルで大きさによって腫瘍が良性であるか、悪性であるか分類しています。直線は線形モデルの予測を表しています。左にあるデータでは 0.5 を閾値として用いています。いくつか悪性のケースが入った場合では、回帰直線はシフトし、0.5 はもはや閾値としては機能しなくなっています。表示する点を少しだけ減らして、見やすくしています。 4.2.2 理論 分類のための解決策は、ロジスティック回帰です。 直線や超平面を当てはめる代わりに、ロジスティック回帰モデルでは、ロジスティック関数を使用して、0 と 1 の間へ線形方程式の出力を変形します。 ロジスティック関数は次のように定義されます。 \\[\\text{logistic}(\\eta)=\\frac{1}{1+exp(-\\eta)}\\] 下図のようになります。 FIGURE 4.6: ロジスティック関数。 出力は 0 から 1 の間を取ります。 入力が 0 のとき、出力は 0.5 です。 線形回帰からロジスティック回帰への変換は理解しやすいです。 線形回帰モデルでは、結果と特徴量の関係を線形方程式でモデル化しています。 \\[\\hat{y}^{(i)}=\\beta_{0}+\\beta_{1}x^{(i)}_{1}+\\ldots+\\beta_{p}x^{(i)}_{p}\\] 分類において、値が 0 から 1 の間となるように、右式をロジスティック関数に組み込みます。 この変換によって、出力は 0 から 1 の間の値を取るようにできます。 \\[P(y^{(i)}=1)=\\frac{1}{1+exp(-(\\beta_{0}+\\beta_{1}x^{(i)}_{1}+\\ldots+\\beta_{p}x^{(i)}_{p}))}\\] 腫瘍の大きさの例をもう一度見てみましょう。 線形回帰モデルの代わりにロジスティック回帰モデルを使っています。 FIGURE 4.7: ロジスティック回帰モデルは、腫瘍のサイズに応じて、悪性と良性の間の正しい決定境界を見つけます。 この線は、データに適合するように変形されたロジスティック関数です。 ロジスティック回帰はうまく分類し、両方のケースで 0.5 を閾値として使うことができます。点を追加することは、推定された曲線にそこまで影響を与えません。 4.2.3 解釈性 ロジスティック回帰モデルの出力は 0 から 1 の確率で表現されるため、ロジスティック回帰の重みの解釈は線形回帰モデルの重みの解釈とは異なります。 重みは確率に対して、線形に影響を及ぼすわけではありません。 重みの合計はロジスティック関数によって確率に変換されます。 したがって、解釈のために、方程式の右側の線形項を式変形する必要があります。 \\[log\\left(\\frac{P(y=1)}{1-P(y=1)}\\right)=log\\left(\\frac{P(y=1)}{P(y=0)}\\right)=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\] この log()関数の中の項を &quot;オッズ&quot; (イベントの確率をイベントが起こらない確率で割ったもの) といい、対数をとったものを対数オッズといいます。 この式はロジスティック回帰モデルが対数オッズに対する線形モデルであることを表しています。 すばらしい！ それでは役に立つようには見えません。 少し項を入れ替えると、特徴量 \\(x_j\\) の1つを 1 単位変化した時、どのように予測が変化するか分かります。 このように変換すると、exp() 関数を方程式の両辺にかけることができます。 \\[\\frac{P(y=1)}{1-P(y=1)}=odds=exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\right)\\] そして、1つの特徴量の値を 1 増加させると何が起きるかを比較します。 このとき、差をみるのではなく、2つの予測の比に注目します。 \\[\\frac{odds_{x_j+1}}{odds}=\\frac{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}(x_{j}+1)+\\ldots+\\beta_{p}x_{p}\\right)}{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}x_{j}+\\ldots+\\beta_{p}x_{p}\\right)}\\] 以下のルールを適用します。 \\[\\frac{exp(a)}{exp(b)}=exp(a-b)\\] そして、項を削除します。 \\[\\frac{odds_{x_j+1}}{odds}=exp\\left(\\beta_{j}(x_{j}+1)-\\beta_{j}x_{j}\\right)=exp\\left(\\beta_j\\right)\\] 最終的に、特徴量の重みの exp() のような単純な式を得ることができます。 特徴量の中の 1 単位の変化は、オッズ比を \\(\\exp(\\beta_j)\\) 倍に変化させます。 この式は以下のように解釈できます。 \\(x_j\\) を 1 単位だけ変化させると、対応する重みの値だけ対数オッズ比が増加します。 ほとんどの人は、対数について頭で考えることが困難なため、オッズ比を解釈します。 オッズ比を解釈するには慣れが必要です。 例えば、もしオッズ比として 2 が与えられたら、 y=0 の確率より y=1 の方が 2 倍高いことを意味します。 重み（=対数オッズ比）が 0.7 の場合、それに対応する特徴量を 1 単位増やすと、オッズに exp(0.7)（=約 2）が乗算され、オッズは4に変わります。 しかし、通常はオッズを扱う必要がなく、重みだけをオッズ比として解釈します。 オッズを実際に計算するためには、それぞれの特徴量に値を設定する必要があるためです。 これは、データセットの特定のインスタンスに注目したいときのみ意味を成します。 特徴量の種類に応じて、ロジスティック回帰の解釈は異なります。 数的特徴量 (Numerical feature)の場合: \\(x_{j}\\) を 1 単位だけ増加させると、予測されるオッズは \\(\\exp(\\beta_{j})\\) 倍に変化します。 バイナリ特徴量 (Binary categorical feature) の場合: 特徴量の2つの値のうちの1つは 参照カテゴリ (いくつかのプログラミング言語では、0 とエンコードされる) です。 特徴量 \\(x_{j}\\) が参照カテゴリから他のカテゴリに変化した時、オッズは \\(\\exp(\\beta_{j})\\) 倍になります。 2つ以上のカテゴリを持つ、カテゴリカルデータの場合: 複数のカテゴリを扱う1つの方法は one-hot-encoding です。one-hot-encoding はそれぞれのカテゴリがそれぞれ列を持ちます。 L 個のカテゴリを持つとき、L-1 列のみ必要で、そうでなければ、パラメータが過剰となります。 L 番目のカテゴリは参照カテゴリである必要があります。 他にも、線形回帰で使用可能な任意のエンコード方法を用いることができます。 それぞれのカテゴリの解釈はバイナリ特徴量の解釈と同様です。 切片\\(\\beta_{0}\\): 全ての特徴量がゼロでカテゴリカル特徴量が参照カテゴリの時、予測されるオッズの値は \\(\\exp(\\beta_{0})\\) です。 切片の重みの解釈は、たいてい意味がありません。 4.2.4 例 ここでは、ロジスティック回帰をリスク要因に基づいた子宮頸がんの予測に使用します。 以下の表は推測された重み、オッズ比、予測の標準誤差を表しています。 TABLE 4.1: ロジスティック回帰モデルを子宮頸がんデータセットに適合させた結果。 モデルで使用されている特徴量、それらの推定された重みと対応するオッズ比、および推定された重みの標準誤差が示されています。 Weight Odds ratio Std. Error Intercept -2.91 0.05 0.32 Hormonal contraceptives y/n -0.12 0.89 0.30 Smokes y/n 0.26 1.29 0.37 Num. of pregnancies 0.04 1.04 0.10 Num. of diagnosed STDs 0.82 2.26 0.33 Intrauterine device y/n 0.62 1.85 0.40 量的特徴量の解釈 (&quot;Num. of diagnosed STDs&quot;): STDs (性感染症, sexually transmitted diseases) と診断された回数が増えると、癌かそうでないかのオッズは 2.26 倍変化します。 ただし、他の特徴量を固定した場合です。 相関は因果関係を示しているとは限らないことに注意してください。 カテゴリカル特徴量の解釈(&quot;Hormonal contraceptives y/n&quot;): ホルモン避妊薬を使っている女性に関して、癌かそうでないかのオッズは、ホルモン避妊薬を使っていない人に比べて 0.89 倍低いです。 ただし、他の特徴量を固定した場合です。 線形モデルと同様に、解釈では常に他の特徴量が固定されているという仮定の元で行われます。 4.2.5 長所と短所 線形回帰モデルの長所と短所は、ロジスティック回帰モデルにも当てはまります。 ロジスティック回帰は様々な分野の人々によって広く使用されていますが、その表現力の低さが問題であり（たとえば、相互作用を手動で追加する必要があります）、他のモデルの方が予測性能が優れていることもあります。 ロジスティック回帰モデルのもう1つの欠点は、重みの解釈は乗法的であり、加法的ではないため、解釈が難しくなることです。 ロジスティック回帰は、完全分離に悩まされることがあります。 2つのクラスを完全に分離できる特徴量がある場合、ロジスティック回帰モデルは学習できなくなります。 これは、その特徴量の最適な重みが無限大となり、収束しないためです。 このような特徴量はクラス分類の際に有用なので、これは残念なことです。 しかし、このように2つのクラスを分離する単純なルールがある場合は、機械学習は必要ありません。 完全分離の問題は、重みのペナルティを導入するか、重みの事前分布を定義することで解決できます。 良い面として、ロジスティック回帰モデルは分類モデルであるだけでなく、確率を出力します。 これは、最終的な分類結果しか提供できないモデルに比べて大きな利点と言えます。 インスタンスが、あるクラスに分類される確率が51％ではなく99％であること知れるのは大きな違いがあります。 ロジスティック回帰は、2クラス分類から多クラス分類に拡張できます。 それは多項回帰 (Multinomial Regression) と呼ばれます。 4.2.6 ソフトウェア 上記の例は、すべて R の glm 関数を使用しました。 ロジスティック回帰は、Python、Java、Stata、Matlab など、データ分析に用いられるあらゆるプログラミング言語で実装されています。 "],["extend-lm.html", "4.3 GLM、GAM、その他", " 4.3 GLM、GAM、その他 線形モデルは、予測を特徴量の重み付き和としてモデル化する点が、最大の長所であり短所でもあります。 それに加えて、線形モデルは多くの仮定を必要とします。 悪いニュースは（実際にはニュースではありませんが）、それらの仮定が現実問題には当てはまらないということがしばしば起こるということです。 例えば、結果が正規分布に従わなかったり、特徴量間に相互作用があったり、あるいは、特徴量と結果の間の真の関係が非線形であったりするような場合です。 一方で良いニュースは、専門家のコミュニティが、この線形回帰モデルという単純なものを、スイスのアーミーナイフのように扱いやすくするために様々な改良をしてきたということです。 この章は、線形モデルを拡張するための明確なガイドを提供しようとするものではなく、一般化線形モデル（GLM）や一般化加法モデル（GAM）などの拡張モデルの概要を紹介し、直感的な理解を与えることが目的です。 したがって、この章を読み終わった後に、どのように線形モデルを拡張するのかについてしっかりと理解する必要があるでしょう。 もしあなたが最初に線形回帰モデルについて理解したいのにも関わらず、まだ線形回帰モデルの章を読んでいないのなら、先にその章を読むことをお勧めします。 線形回帰モデルの式を思い出してみましょう。 \\[y=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}+\\epsilon\\] 線形回帰モデルは、結果 y は、p 個の特徴量の重み付き和に正規分布に従う誤差 \\(\\epsilon\\) が付与されたものであると仮定しています。 このようにデータを定式化することで、モデルの高い解釈可能性が得られるわけです。 線形回帰モデルでは、特徴量の効果は加法的で、相互作用はなく、特徴量と出力の関係は線形です。つまり、特徴量が 1 増加すると、直接的に予測結果が増加/減少します。 線形モデルを使用することで、特徴量と予測結果の関係を、1つの数値、すなわち推定された重みとして表現します。 しかし、単純な重み付き和は制約として強すぎるため、現実世界の多くの予測問題にそのまま適用することは出来ません。 この章では、線形回帰モデルの典型的な3つの問題を取り上げ、それらをどう解決するかについて紹介します。 モデル仮定に反する可能性のある問題は他にも多くありますが、次の図に示す3つの問題に焦点を当てます。 FIGURE 4.8: 線形モデルの3つの仮定(左側):特徴量に対する出力が正規分布であること、加法性が成り立つ（=相互作用がない）こと、及び線形関係にあること。現実では通常これらの仮定を満たさない(右側):出力が正規分布に従わなかったり、特徴量間に相互作用が存在したり、非線形の関係があったりするかもしれません。 これらすべての問題に対する解決策があります。 問題: 特徴量が与えられたときの結果 y が、正規分布に従わない。 例: ある日に何分間、自転車に乗るかを予測したいとします。 この場合、特徴量としては曜日や天気などがあります。 しかしながら、線形モデルを使用した場合、正規分布を想定しているため、0 分以下にならないとは限らず、負の時間を予測してしまうかもしれません。 その他の例として、線形モデルを使用して確率を予測した場合、負または1より大きい確率が予測値として得られることになります。 解決策: 一般化線形モデル（GLMs） 問題: 特徴量間に相互作用が存在する。 例: 私は、小雨が降るとサイクリングしたいという気持ちが少し萎えますが、夏のラッシュアワー時には雨を歓迎します。 それは、晴天を好むサイクリストが全員家にいて、自転車道が空くからです。 これは時間と天気の間の相互作用であるため、単純な加法モデルでは捉えることができません。 解決策: 相互作用項を手動で追加する 問題: 特徴量と結果 y の間の真の関係性が線形ではない。 例: 摂氏0度から25度の間では、自転車に乗りたいという私の欲求に対する温度の影響は、線形である可能性があります。 つまり、気温が0度から1度に上昇した場合と、20度から21度に上昇した場合のモチベーションの増加は同じということです。 しかし、その気温よりが高くなれば、暑すぎるときに自転車に乗るのは好きでないので、サイクリングへのモチベーションは横ばいになり、やがて低下します。 解決策: 一般化された加法モデル (GAM); 特徴量の変換 この章では、これら3つの問題の解決策を示します。 線形モデルの拡張は他にも多くありますが、ここでは割愛します。 ここで全てを説明しようとすれば、この章自体がすでに他の多くの専門書の中で説明されている内容を集めた本となってしまうでしょう。 とはいえ、せっかくですので、章の終わりで線形モデル拡張について問題と解決策のちょっとした概要を作成しておきました。 解決策の名前は、自身で詳細を調べるときに役に立つことでしょう。 4.3.1 結果が正規分布に従わない場合 - GLMs 線形回帰モデルは、特徴量が与えられたときの結果は正規分布に従うと仮定としています。 しかし、この仮定が成り立たない場合は多くあります。 結果は、カテゴリ (がん or 健康)、整数 (子供の数)、出来事が起こるまでの時間 (機械が故障するまでの時間)、少数のとても大きな数が存在する偏った出力 (世帯収入) などがあります。 実は、線形回帰モデルはこれらすべてのタイプに拡張できます。 この拡張は一般化線形モデル (Generalized Linear Models)もしくは省略してGLMsと呼ばれます。 この章では、GLMという用語を一般的なフレームワークとこのフレームワークに由来する特定のモデル、両方を表す際に使います。 GLM のコアとなる概念は、「特徴量の重み付き和を保持するが、結果の分布の非正規性を許容し、この分布の平均と重み付き和をある非線型関数で関連づけること」です。 例えば、ロジスティック回帰モデルでは結果が二項分布 (Bernoulli distribution) に従うことを仮定しており、ロジスティック関数を通して分布の平均と重み付き和が関連付けられています。 GLM は特徴量の重み付き和と、仮定された分布の平均を、リンク関数 g を用いて数学的に関連付けます。このとき、 g は結果の種類によって柔軟に決められます。 \\[g(E_Y(y|x))=\\beta_0+\\beta_1{}x_{1}+\\ldots{}\\beta_p{}x_{p}\\] GLM は次の3つの要素からなります。 それは、リンク関数 g、重み付き和 \\(X^T\\beta\\) (線形予測器とも呼ばれる) と、\\(E_Y\\) を定義する指数型分布族に由来する確率分布です。 指数型分布族は、分布の平均、分散、その他のパラメータを持つ指数が含まれた共通の式によって記述できる分布の集合です。 それ自体、とてつもなく広い分野であり、深入りはしたくないので、数学的な詳細まで今回は扱いません。 Wikipedia にはよく整理された指数型分布族に該当する分布の表があります。 このリストの中ならどの分布でも GLM に適用できます。 予測したい結果の種類に応じて、適切な分布を選んでください。 例えば、結果が数 (例：一家庭における子供の数) ならばポアソン分布、 結果が常に正 (例：2つのイベント間の時間) ならば指数分布が良いでしょう。 GLM の特殊な場合として、古典的な線形モデルについて考えてみましょう。 線形モデルにおける正規分布のリンク関数は単に恒等関数となります。 正規分布は平均と分散によって決まります。 平均値は平均的に期待される値、分散は平均の周りでどのくらい値がばらつくかを表す値です。 線形モデルでは、リンク関数は特徴量の重み付き和と正規分布の平均を関連付けます。 GLM のフレームワークでは、この概念は (指数型分布族に由来する) すべての分布とリンク関数に一般化されます。 y が例えば一日に飲むコーヒーの数といったような数であったなら、ポアソン分布とリンク関数である自然対数を用いたGLMでモデル化できます。 \\[ln(E_Y(y|x))=x^{T}\\beta\\] ロジスティック回帰モデルも二項分布を仮定した GLM で、ロジット関数がリンク関数として使われています。 ロジスティック回帰で用いられる二項分布の平均は y が 1 である確率です。 \\[x^{T}\\beta=ln\\left(\\frac{E_Y(y|x)}{1-E_Y(y|x)}\\right)=ln\\left(\\frac{P(y=1|x)}{1-P(y=1|x)}\\right)\\] そして、この式を片方が P(y=1) となるように変形すれば、ロジスティック回帰の公式が得られます。 \\[P(y=1)=\\frac{1}{1+exp(-x^{T}\\beta)}\\] 指数型分布族に属する分布は、分布から数学的に導ける正準リンク関数 (canonical link function) があります。 GLM の枠組みはその分布と関係なくリンク関数を選ぶことができます。 どうやって、適切なリンク関数を選ぶのでしょうか。 そこには、完璧なレシピはありません。 目的値の分布だけでなく、理論的な考察と実際のデータにどれだけ適合しているかを考慮に入れます。 分布の中には、正準リンク関数がその分布に対して無効であるような値につながるものもあります。 指数型分布族の分布の場合、正準リンク関数は負の逆関数であり、指数分布の領域の外側であるような負の予測値を出してしまうことがあります。 ただ、リンク関数は任意に選べるので、単純な解決策は分布の領域に適合するような別の関数を選ぶことです。 例 GLM の必要性を強調するために、一日どれくらいコーヒーを飲むかについてのデータセットについて考えてみました。 一日にコーヒーを飲む量についてのデータを集めたとします。 コーヒーが好きでないのなら、お茶でもかまいません。 コーヒーを飲んだ数とともに、現在のストレスレベル（1から10）、夜どれくらいよく眠れたか（1から10）、その日仕事があったかについて記録します。 目標は200日間のこれらのデータからコーヒーを飲んだ数を予測することです。 ストレスと睡眠については1から10まで均一の分布であり、仕事について yes/no が50%ずつとします（なんて生活でしょうか！）。 コーヒーを飲んだ数はポアソン分布に従い、\\(\\lambda\\)（ポアソン分布の期待値）を睡眠、ストレス、仕事の特徴量の関数としてモデル化します。 この話がどうなっていくか予測できるでしょうか？ 「線形モデルでこのデータをモデル化してみよう...うまくいかないなぁ...じゃあ、ポアソン分布を用いてGLMでやったらどうかな...あ!うまくいったぞ!!!!!!!」 読者のためにあまり話が逸れないようにしなければ...。 一日に飲んだコーヒーの数を目的変数としたときの分布をみてみましょう。 FIGURE 4.9: 200日間のコーヒーを飲んだ量の分布 76 日中 200 日はまったくコーヒーを飲んでおらず、一番飲んだ日は7 杯も飲んでいます。 愚直に線形モデルを用いて、睡眠レベル、ストレスレベル、仕事の有無の特徴量から飲んだ コーヒーの数を予測してみましょう。 誤って正規分布を仮定すると何がおかしくなるのでしょうか? 間違った仮定は推定値、特に重みの信頼区間を無効にしてしまいます。 さらに明らかな問題は、次の図に示すように予測値が真の結果の&quot;許された&quot;領域と合致しないということです。 FIGURE 4.10: ストレス、睡眠、仕事に応じて予測されたコーヒーを飲む量の予測値。線形モデルは負の値を予測しています。 線形モデルは負の値を予測するので、理にかなっていません。 リンク関数と仮定している分布を変更することによって GLM ではこの問題を解決できます。 1つの選択肢は、正規分布は引き続き使い、リンク関数としては、常に正の値しか取らないようにするため、恒等関数の代わりに log-link (expの逆関数) を使用するという方法です。 さらに良いのは、データが生成されたプロセスに従った分布を選び、適切なリンク関数を選ぶことです。 結果は数ですから、ポアソン分布とリンク関数として対数関数を選ぶことが自然です。 今回は、データはポアソン分布から生成されるので、Poisson GLM はベストチョイスです。 学習された Poisson GLM による予測値の分布は次のようになります。 FIGURE 4.11: ストレス、睡眠、仕事に応じて予測されたコーヒーを飲む量の予測値。ポアソン分布とlog link に基づいた GLM はこのデータセットに対する適切なモデルです。 負の値を取らないので、先ほどよりも良さそうです。 GLMの重みの解釈 リンク関数と共に仮定した分布は推定される特徴量の重みがどのように解釈されるかを決めます。 コーヒーの例では、ポアソン分布とlogリンクに基づいたGLMを用いました。これによって、次のような特徴量と予測値の関係が示唆されます。 \\[ln(E(\\text{coffees}|\\text{stress},\\text{sleep},\\text{work}))=\\beta_0+\\beta_{\\text{stress}}x_{\\text{stress}}+\\beta_{\\text{sleep}}x_{\\text{sleep}}+\\beta_{\\text{work}}x_{\\text{work}}\\] 重みを解釈するために、予測された結果の対数ではなく、リンク関数の逆関数をとり、特徴量の影響を理解しやすいように変形します。 \\[E(\\text{coffees}|\\text{stress},\\text{sleep},\\text{work})=exp(\\beta_0+\\beta_{\\text{stress}}x_{\\text{stress}}+\\beta_{\\text{sleep}}x_{\\text{sleep}}+\\beta_{\\text{work}}x_{\\text{work}})\\] すべての重みは、指数関数の中にはいっており、exp(a + b) は exp(a) と exp(b) の積になるので、効果の解釈は加法的ではなく乗法的となります。 解釈の最後の要素は、例の実際の重みです。 以下の表に予測される重みと exp(weight) を 95% 信頼区間とともに挙げています。 weight exp(weight) [2.5%, 97.5%] (Intercept) -0.16 0.85 [0.54, 1.32] stress 0.12 1.12 [1.07, 1.18] sleep -0.15 0.86 [0.82, 0.90] workYES 0.80 2.23 [1.72, 2.93] ストレスレベルが1点上昇すると、予測されるコーヒーの量は 1.12 倍になります。 睡眠の質が1点上昇すると、予測されるコーヒーの量は 0.86 倍になります。 仕事のある日の予測値は仕事のない日と比べて、平均して2.23倍されます。 まとめると、ストレスが多い、睡眠が少ない、仕事がある日に、より多くのコーヒーが飲まれます。 この章では正規分布にターゲットが従わない場合に有効なGLMについて少し学びました。 次は、2つの特徴量の相互作用をどのように線形回帰モデルに取り入れるかを見ていきましょう。 4.3.2 相互作用 線形回帰モデルでは、1つの特徴量がもたらす効果は他の特徴量の値とは関係がない (＝相互作用がない) ことを前提としています。 しかし、多くの場合、特徴量の間には相互作用があります。 例えば、自転車レンタルの数を予測する際、気温と就業日であるかどうかの間に相互作用があるかもしれません。 おそらく、就業日であれば、何があろうとも仕事のために自転車に乗るので、気温はレンタルされる自転車の数に大して影響を与えないでしょう。 しかし休日の場合には、多くの人が娯楽目的に自転車に乗りますが、それは気温が十分に暖かいときだけでしょう。 したがって、レンタル自転車の予測では、気温と就業日の間の相互作用が期待されるかもしれません。 線形モデルで相互作用を考慮するにはどうすれば良いでしょう。 線形モデルを学習する前に、特徴量に相互作用を表現する列を追加します。 この解決策は、線形モデル自体を変更することなく、ただ列をデータに追加するだけで良いという点で優れた手法といえます。 例えば、就業日と気温の例においては、休業日の場合は 0、それ以外は気温の値を持つような特徴量を追加します。 このとき、就業日が参照カテゴリであると仮定します。 データが次のようになっているとします。 work temp Y 25 N 12 N 30 Y 5 以下の表は、相互作用を考慮しない場合のデータ行列を表しており、先ほどのものと少し異なります。 通常、この変換は統計ソフトウェアによって自動的に行われます。 Intercept workY temp 1 1 25 1 0 12 1 0 30 1 1 5 1列目は切片の項です。 2列目は 0 を参照カテゴリ、1 をその他としたようなカテゴリカル特徴量となっています。 そして、3列目には気温が入っています。 線形モデルで気温と就業日の相互作用を考慮したい場合、次のように相互作用の列を追加する必要があります。 Intercept workY temp workY.temp 1 1 25 25 1 0 12 0 1 0 30 0 1 1 5 5 新しい列 'workY.temp' は就業日（work）と気温（temp）間の相互作用を表現します。 この列は work 特徴量が参照カテゴリ（Nである、つまり就業日でない）のときに 0 を、その他の場合には気温の値をとります。 このエンコーディングにより、線形モデルは就業日・休業日両方に対して異なる気温による線形効果を学習できます。 これが2つの特徴量間の相互作用です。 相互作用項がない場合、カテゴリカル特徴量と量的特徴量の複合効果は異なるカテゴリに対して垂直方向にシフトした直線で表現できます。 相互作用を考慮すると、量的特徴量の効果（傾き）が各カテゴリごとに異なる値を持つことができます。 2つのカテゴリカル特徴量の相互作用についても同様です。 カテゴリの組み合わせを表現する特徴量を追加します。 次の表は就業日（work）とカテゴリカルな天候（wthr）を含んだ人工的なデータです。 work wthr Y 2 N 0 N 1 Y 2 次に相互作用項を追加します。 Intercept workY wthr1 wthr2 workY.wthr1 workY.wthr2 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1列目は切片の計算に用いられます。 2列目はエンコードされた就業日の特徴量です。 3列目と4列目は、3つのカテゴリを持つ天候の特徴量であり、1つを参照カテゴリとして、効果を表現するためには2つの重みが必要なので、2つの列が用意されています。 残りの列は相互作用項です。 2つの特徴量の各カテゴリごと（参照カテゴリを除く）に、両方の特徴量が、あるカテゴリであれば 1 を、そうでなければ 0 を持つような列を作ります。 2つの量的特徴量に対しては、相互作用項はさらに簡単に構築できます。単に両方の特徴量の値を掛け合わせれば良いのです。 実は、自動で相互作用項を検出し追加するアプローチはいくつかあります。 そのうちの1つは、RuleFitの章で紹介されています。 RuleFit アルゴリズムでは、最初に相互作用を探索し、これらの相互作用を含めた線形回帰モデルを学習します。 例 線形モデルの章でモデル化した自転車レンタル数の予測タスクに戻りましょう。 今回は、気温と就業日の間の相互作用についても考慮します。 これにより、次の重みと信頼区間が得られます。 Weight Std. Error 2.5% 97.5% (Intercept) 2185.8 250.2 1694.6 2677.1 seasonSUMMER 893.8 121.8 654.7 1132.9 seasonFALL 137.1 161.0 -179.0 453.2 seasonWINTER 426.5 110.3 209.9 643.2 holidayHOLIDAY -674.4 202.5 -1071.9 -276.9 workingdayWORKING DAY 451.9 141.7 173.7 730.1 weathersitMISTY -382.1 87.2 -553.3 -211.0 weathersitRAIN/... -1898.2 222.7 -2335.4 -1461.0 temp 125.4 8.9 108.0 142.9 hum -17.5 3.2 -23.7 -11.3 windspeed -42.1 6.9 -55.5 -28.6 days_since_2011 4.9 0.2 4.6 5.3 workingdayWORKING DAY:temp -21.8 8.1 -37.7 -5.9 追加した相互作用による影響は負 (-21.8) であり、95%信頼区間が 0 を含まないことからわかるように、0 から大きく離れています。 ちなみに、互いに近い日は独立でないため、データは独立同分布 (iid) ではありません。 信頼区間は誤解を招く恐れがあるため、話半分に見てください。 相互作用項は関連する特徴量の重みの解釈を変えます。 就業日であれば気温は負の効果をもたらしているでしょうか。 たとえ表が負の効果を示していたとしても、答えはノーです。 &quot;workingdayWORKING DAY:temp&quot; の相互作用項の重みは単独で解釈できません。 なぜなら、重みの解釈は「他のすべての特徴量を変化させずに、就業日の気温の相互作用効果を増加させると、予測される自転車の数は減少する。」となるからです。 しかし、相互作用の効果は気温による効果に追加されるだけです。 今日が就業日で、気温が1度高かったらどうなるか知りたいとします。 それなら、&quot;temp&quot; と &quot;workingdayWORKING DAY:temp&quot; の両方の重みを合計して、予測値がどれほど増加するかを判断する必要する必要があります。 相互作用は簡単に視覚化できます。 カテゴリカル特徴量と量的特徴量の間の相互作用を導入することにより、気温に対して1つではなく2つの勾配を得ることができます。 休業日（'NO WORKING DAY'）における気温の勾配は、表から直接読み取ることができます (125.4)。 就業日（'WORKING DAY'）における気温の勾配は、両方の気温の重みの合計から得られます (125.4 -21.8 = 103.6)。 'NO WORKING DAY'直線の 気温 = 0 における切片は、線形モデルの切片 (2185.8) で決定されます。 また、'WORKING DAY'直線の 気温 = 0 における切片は、線形モデルの切片 + 就業日の効果 (2185.8 + 451.9 = 2637.7) で決まります。 FIGURE 4.12: 線形モデルのレンタル自転車数の予測に対する気温と就業日の影響 (相互作用を含む)。就業日の各カテゴリに対して2つの気温の勾配が得られている。 4.3.3 非線形効果 - GAM 世界は線形ではありません。 線形モデルにおける線形性とは、インスタンスの特徴量の値がどのような時でも、値を 1 増やすと、常に同じ効果を予測結果に与えるということを意味します。 気温が10度から11度に上がった時と、40度から41度に上がった時とで、自転車のレンタル数に同じ影響があると考えるのは妥当でしょうか？ 直感的には、気温が10度から11度に上がるとレンタル自転車数は増え、40度から41度に上がるとレンタル自転車数は減ると予想されます。これは、この本に出てくる他の例も同様です。 温度という特徴量は、レンタル自転車数に線形なプラスの影響を及ぼしますが、ある時点で平坦になり、高い温度ではマイナスの影響を及ぼします。 線形モデルはこのような影響の変化に関わらず、（ユークリッド距離を最小化することによって）あくまで最適な線形の関係性を見つけようとします。 非線形の関係は以下の手法を用いてモデル化できます。 特徴量の単純な変換（対数変換など） 特徴量のカテゴリカル化 一般化加法モデル（Generalized Additive Model, GAM） それぞれの手法の詳細に入る前に、これら3つについて例を見てみましょう。 レンタル自転車のデータセットを使って、温度の特徴量のみを使用し線形モデルを学習することで、レンタル自転車数の予測をしました。 次の図はそれぞれ、標準的な線形モデル、対数変換した温度による線形モデル、温度をカテゴリカル特徴量として扱った場合の線形モデル、GAM (スプライン回帰) を使用したときの推定された勾配を示しています。 FIGURE 4.13: 温度特徴量のみを用いたレンタル自転車数の予測。線形モデル（左上）はデータにあまり適合していません。1つの解決策は、例えば対数で特徴量を変換（右上）したり、カテゴリカル化する（左下）ことですが、これは通常は悪い判断です。GAMを使うと(右下)、気温に対して滑らかな曲線を自動的に適合できます。 特徴量変換 多くの場合、特徴量の対数変換が使用されます。 対数を使用すると、10倍の温度上昇ごとに自転車の数に同じ線形効果があることが示されます。 したがって、気温が1度から10度に変化するときと、0.1度から1度に変化するときは同じ効果があります（これも間違っているように思われます）。 特徴量変換の他の例は、平方根、二乗関数、および指数関数です。 特徴量変換を使用するということは、データの特定の特徴量の列を対数などで変換したものに置き換えて、通常どおり線形モデルで学習することを意味します。 一部の統計プログラムでは、線形モデルを呼び出す時に変換方法も指定できます。 特徴量の変換は、クリエイティブな行為です。 特徴量の解釈は、選択した変換によって変わります。 対数変換を使用する場合、線形モデルにおける解釈は「特徴量の対数が1増加すると、対応する重みによって予測結果が増加する。」となります。 恒等関数ではないリンク関数でGLMを使用する場合、両方の変換を解釈に組み込む必要があるため、解釈はより複雑になります（logとexpのように互いに打ち消し合う場合は解釈は簡単なので除く）。 特徴量のカテゴリカル化 非線形効果を実現するもう1つの選択肢は、特徴量を離散化し、カテゴリカル特徴量とすることです。 たとえば、温度という特徴量をレベル[-10, -5), [-5, 0), ... の 20 の区間に分割できます。 連続値としての温度の代わりにカテゴリカル化された温度を使用する場合、各区間が独自の推定値をとるため、線形モデルはステップ関数を推定します。 このアプローチの問題は、より多くのデータが必要であり、過学習する可能性が高く、特徴量を意味のある形で離散化する方法が不明確であるということです（等距離区間または分位数？区間の数は？）。 非常に有効な場合にのみ、離散化を使用します。 たとえば、モデルを別の研究と比較できるようにするためなどです。 一般化加法モデル（Generalized Additive Models, GAM） なぜ非線形の関係を学習のために (一般化)線形モデルを 'そのまま' 使用してはいけないのでしょうか？ それがGAMの背後にある動機です。 GAMは、関係は単純な重み付き和でなければならないという制限を緩和し、各特徴量の任意の関数の総和によって結果をモデル化できると仮定します。 数学的には、GAMの関係は次のようになります。 \\[g(E_Y(y|x))=\\beta_0+f_1(x_{1})+f_2(x_{2})+\\ldots+f_p(x_{p})\\] この式は GLM の式に似ていますが、線形項 \\(\\beta_j{}x_{j}\\) がより柔軟な関数 \\(f_j(x_{j})\\) に置き換えられている点が異なります。 GAM の核は依然として特徴量効果の合計ですが、特徴量と出力の間の非線形性を許す余地があります。 線形効果もこのフレームワークでカバーされており、特徴量に対する線形性は、\\(f_j(x_{j})\\) を \\(x_{j}\\beta_j\\) の形に制限することで表現できます。 大きな問題は、この非線形関数をどのように学習するかです。 その答えは「スプライン」または「スプライン関数」と呼ばれます。 スプラインは、任意の関数を近似するために組み合わせることができる関数です。 より複雑なものを構築するためにレゴブロックを積み重ねるのと少し似ています。 これらのスプライン関数を定義するには、おびただしい数の方法があります。 もしスプラインを定義するすべての方法についてもっと知りたいのなら、それは長旅になるでしょう。旅の幸運を祈っています。 ここでは詳細に立ち入らずに、直感的な説明のみに留めます。 スプラインを理解するために個人的に最も役立ったのは、個々のスプライン関数を視覚化し、データ行列がどのように変わったかを調べることでした。 たとえば、スプラインを使用して温度をモデル化するには、データから温度の特徴量を削除し、それぞれがスプライン関数を表す4つの列に置き換えます。 通常、スプライン関数はもっと多くなりますが、説明のために数を減らしました。 これらの新しいスプライン特徴量の各インスタンスでの値は、インスタンスの温度の値によって異なります。 すべての線形効果とともに、GAMはこれらのスプラインの重みも推定します。 GAMはまた、重みをゼロに近づけるためのペナルティ項も導入します。 これにより、スプラインの柔軟性が低下し、過学習が抑制されます。 曲線の柔軟性を制御するために一般的に使用される平滑化パラメータは、交差検定によって調整されます。 ペナルティ項を無視すると、スプラインを使用した非線形のモデリングは高度な特徴量エンジニアリングとなります。 GAMを使用し、温度のみから自転車の数を予測する例では、モデルの特徴量の行列は次のようになります。 (Intercept) s(temp).1 s(temp).2 s(temp).3 s(temp).4 1 0.93 -0.14 0.21 -0.83 1 0.83 -0.27 0.27 -0.72 1 1.32 0.71 -0.39 -1.63 1 1.32 0.70 -0.38 -1.61 1 1.29 0.58 -0.26 -1.47 1 1.32 0.68 -0.36 -1.59 各行は、データからの個々のインスタンス (1日) を表します。 各スプラインの列には、特定の温度の値でのスプライン関数の値が含まれています。 以下に、これらのスプライン関数の様子を図示します。 FIGURE 4.14: 温度効果を滑らかにモデル化するために、4つのスプライン関数を使用します。 各温度の値は、（ここでは）4つのスプライン値にマッピングされます。 インスタンスの温度が30度の場合、最初のスプライン特徴量の値は -1、2番目は 0.7、3番目は -0.8、および4番目は 1.7 です。 GAMは、各温度のスプライン特徴量に重みを割り当てます。 weight (Intercept) 4504.35 s(temp).1 -989.34 s(temp).2 740.08 s(temp).3 2309.84 s(temp).4 558.27 また、推定された重みで重み付けされたスプライン関数の合計から得られる実際の曲線は、次のようになります。 FIGURE 4.15: レンタル自転車数を予測するための温度のGAM特徴量効果（温度のみを特徴量として使用）。 滑らかな効果の解釈には、学習した曲線の視覚的なチェックが必要です。 スプラインは通常、平均予測で中心化されているため、曲線上の点は平均予測との差になります。 たとえば 0度のとき、予測される自転車の数は平均予測よりも3000台少ないということです。 4.3.4 長所 線形モデルのこれらすべての拡張は、それら自身がそれぞれ小宇宙のようなものです。 線形モデルで直面する問題が何であれ、それを修正するための拡張方法が見つかるでしょう。 ほとんどの手法は、何十年もの間使用されてきました。 たとえば、GAMはおおよそ30年前のものです。 多くの研究者や産業界の実務家は、線形モデルの経験がとても豊富であり、それらの手法はモデリング手法の現状として多くのコミュニティで受け入れられています。 それに加えて、モデルの仮定に反していないのであれば、モデルを使用して予測を行い、データに関する結論を導き出すことも可能です。 また、重みの信頼区間、有意差検定、予測区間などを得ることができます。 統計ソフトウェアは通常、GLM、GAM、およびより特別な線形モデルを学習するための非常に優れたインターフェースを備えています。 多くの機械学習モデルの不透明度は、1）スパース性の欠如、つまり多くの特徴量が使用されていること、2）非線形に扱われる特徴量、つまり効果を記述するためには1つ以上の重みが必要、および、3）特徴量間の相互作用のモデリング、の３つに起因します。 線形モデルは高い解釈可能性をもっている一方で、しばしば現実に適合しないということを前提にすれば、この章で説明した拡張は、解釈可能性をある程度維持しながら、より柔軟なモデルへのスムーズな移行を実現する優れた方法を提供しているといえるでしょう。 4.3.5 短所 利点として、線形モデルはそれぞれ独自の宇宙が広がっていると言いました。 初学者でなくても、単純な線形モデルを拡張する方法は非常に多いため圧倒されてしまいます。 実際には、研究者や実務家の多くのコミュニティが、多かれ少なかれ同じことを行う手法に独自の名前を持っているため非常にややこしく、複数の並行宇宙があると言えます。 線形モデルの修正により、そのほとんどのモデルは解釈性が低下します。 恒等関数以外の任意のリンク関数 (GLMにおいて) は、解釈を複雑にしますし、 相互作用も解釈を複雑にします。 また、非線形の特徴量の効果は、直感的ではないか（対数変換のように）、あるいは、もはや単一の数値で要約できなくなります（スプライン関数など）。 GLM や GAM などは、データの生成プロセスに関する仮定に依存しています。 それらの仮定に反した場合、重みの解釈に妥当性はなくなります。 ランダムフォレストや勾配ブースティングなどの決定木ベースのアンサンブル学習モデルは、多くの場合、最も洗練された線形モデルよりも優れたパフォーマンスを示します。 これは、私自身の経験からもいえますし、kaggle.com などのプラットフォームで優勝したモデルの結果からもいえます。 4.3.6 ソフトウェア この章のすべての例は R 言語を用いて作られています。 GAM には gam パッケージが使用されていますが、それ以外にも多くのパッケージがあります。 R には回帰モデルを拡張する驚くほど多くのパッケージがあります。 他の分析用の言語にも負けることはなく、R は、他の考えられる線形モデルの拡張の原点と言えます。 Python でも様々な実装を見つけることができ、pyGAM は GAM の Python 実装ですが、これはまだ成熟していません。 4.3.7 さらなる拡張 約束通り、線形モデルを使う際に遭遇する可能性のある問題と、検索したら解決できるように解決方法の名前をリストで示します。 データが独立同分布 (iid) の仮定に反する場合。 例として、同じ患者の繰り返しの測定がこれに当たります。 このような場合は、混合モデル (mixed models)や一般化推定方程式 (generalized estimating equations)で検索してください。 モデルが不均一な分散の誤差持つ場合。 例として、住宅価格を予想する時、高価な住宅であるほど、予測値の誤差は大きくなりますが、これは線形モデルの等分散性に反します。 ロバスト回帰 (robust regression)で検索してください。 モデルに大きく影響する外れ値がある場合。 ロバスト回帰 (robust regression)で検索してください。 イベントが起きるまでの時間を予測したい場合。 イベントまでの時間のデータでは、大抵、打ち切られた測定値が含まれていますが、これはイベントが起きるまでに十分な時間が無かったことを意味しています。 例えば、ある会社が二年間のデータしか与られていない状態で、製氷機の故障を予測したい場合です。 二年経過しても故障しない機械もありますが、その後故障する可能性もあります。 パラメトリック生存モデル (parametric survival models)、コックス回帰 (cox regression)、 生存時間分析 (survival analysis)で検索してください。 予測の結果がカテゴリカルの場合。 もし、結果が2つのカテゴリの場合、ロジスティック回帰モデルを使用してカテゴリの確率を求めることができます。 さらに多くのカテゴリがある場合、multinomial regressionで検索してください。 ロジスティック回帰と multinomial regression はどちらも GLM です。 順序付きのカテゴリを予測したい場合。 例えば学校の成績です。 比例オッズモデル (proportional odds model)で検索してください。 結果が、家族の中の子供の数のようなカウントの場合。 ポアソン回帰 (Poisson regression)で検索してください。 ポアソンモデルもGLMです。 0 の値の頻度がとても多いという問題があるかもしれません。 そのときは、ゼロ過剰ポアソン回帰 (zero-inflated Poisson regression)やHurdleモデル (hurdle model)で検索してください。 正しい因果関係を導き出すためにどの特徴量をモデルに含めればいいのかわかりません。 例えば、血圧に効果のある薬が知りたいときです。 薬はなんらかの血液量に直接影響を与え、この血液量が結果に影響を与えます。 血液量を回帰モデルに含めるべきでしょうか？ 因果推論 (causal inference)や媒介分析 (mediation analysis)で検索してください。 データに欠損値がある場合。 多重代入法 (multiple imputation)で検索してください。 事前知識をモデルに取り入れたい場合。 ベイズ推定 (Bayesian inference)で検索してください。 最近少し元気がありません。 &quot;Amazon Alexa Gone Wild!!! Full version from beginning to end&quot;で検索してください。 "],["tree.html", "4.4 決定木", " 4.4 決定木 線形回帰とロジスティック回帰モデルは特徴量と結果が非線形の時や特徴量が相互作用する時に失敗します。 この状況こそ決定木が輝く時です！ 木をベースにしたモデルは、特徴量を、あるカットオフ値に基づいて複数回データを分割していきます。 この分岐を通して、データセットは異なる部分集合に分割され、それぞれのインスタンスはこのうちの1つに属します。 最後の部分集合は終端ノード (terminal node) または葉 (leaf node) と呼ばれていて、中間の部分集合は内部ノード (internal node)、または、分岐ノード (split node)と呼ばれています。 それぞれの葉で結果を予測するためには、ノードに含まれる学習データの結果の平均値が使用されます。 決定木は、分類でも回帰でも使われています。 決定木を成長させるためのさまざまなアルゴリズムが知られています。 これらのアルゴリズムでは、決定木の構造（例:ノードあたりの分岐数）、分岐を見つけるための指標、いつ分岐を止めるか、そしてどのようにして葉の中で簡単なモデルを予測するかが異なっています。 CART (Classification And Regression Trees) アルゴリズムは、おそらく決定木を構築するためのもっとも有名なアルゴリズムです。 本章ではCARTに焦点をあてますが、他の木も同様に解釈可能です。 CARTに関して、より詳細を知りたいのであれば、&quot;The Elements of Statistical Learning&quot; (Friedman, Hastie and Tibshirani 2009)17 の本をおすすめします。 FIGURE 4.16: 人工データに対する決定木。 特徴量 x1 が 3 より大きいとき、ノード 5 に割り当てられる。それ以外のインスタンスは特徴量 x2 が 1 を超えるかどうかでのノード 3 かノード 4 に割り当てられる。 下記の方程式は特徴量 x と結果 y の関係を記述しています。 \\[\\hat{y}=\\hat{f}(x)=\\sum_{m=1}^Mc_m{}I\\{x\\in{}R_m\\}\\] それぞれのインスタンスは1つの葉 (= 部分集合 \\(R_m\\)) と対応します。 \\(I_{\\{x\\in{}R_m\\}}\\) は、\\(x\\) が 部分集合 \\(R_m\\) に属していれば 1 を、そうでないなら 0 を返す指示関数です。もし、インスタンスが葉 \\(R_l\\) に対応したとすると、予測による結果は \\(\\hat{y}=c_l\\) となります。ただし、\\(c_l\\) は葉 \\(R_l\\) に対応する全ての学習データのインスタンスの平均値です。 しかし、これらの部分集合はどこからきたのでしょうか？ これはとても簡単です: CARTは回帰であれば y の分散を、分類であれば y のクラス分布のジニ係数を最小にするように、特徴量を選び、カットオフ点を決定します。 分散はノード内の y の値が平均からどの程度広がっているかを教えてくれます。 ジニ係数はノードがどれだけ不純かを教えてくれます。例えば、ノード内に全てのクラスが同じ数あるとき、ノードは不純になります。一方で、ノードのクラスが1つだけの場合、純度が最大になります。 ノードの中のデータ点がとても似た y の値を持つとき、分散やジニ係数は小さくなります。 結果として、最適なカットオフ点は、目標値に関して、2つの結果の部分集合ができるかぎり異なる値となるように選ばれます。 カテゴリカル特徴量に対して、アルゴリズムは各カテゴリごとにまとめる様にして部分集合を作成します。 特徴量ごとの最適なカットオフが決まったあと、アルゴリズムは、その中から分散やジニ係数に関して最適な分岐を与える特徴量を選び、この分岐を木に追加します。 アルゴリズムは、両方の新しいノードに対して、この探索と分岐を停止の基準に到達するまで再帰的に繰り返します。 考えられる停止基準は、分岐の前にノードの中に存在するべき最小のインスタンス数や、終端ノードに含まれるインスタンスの最小の数などがあります。 4.4.1 決定木の解釈 決定木の解釈は単純です: 根のノードから始めて、辺を辿って次のノードへと移っていきます。 葉に到達すると、そのノードから予測結果を得ることができます。 すべての枝は 'AND' で繋がっています。 例として、特徴量 x が 閾値 c より[小さい/大きい] かつ ... のようになります。 そして、予測結果は対応するノードに含まれるインスタンスの y の平均値になります。 特徴量重要度 (Feature importance) 決定木での特徴量の全体の重要度は次のように計算されます。 その特徴量が使われた全ての分岐を見て、それが親のノードに比べてどのくらい分散やジニ係数を減少させているかを計算します。 全部の重要度の和を100にスケーリングします。 これはそれぞれの重要度がモデル全体の重要度に対する寄与率として解釈できることを意味しています。 木の分解 決定木の個々の予測は決定経路を特徴量ごとに1つの要素に分解することで説明可能です。 木に沿って決定を追うことができ、それぞれの決定ノードに与えられた寄与度によって予測を説明できます。 根のノードは始点となります。 根のノードを最終的な予測のために使うのであれば、単に学習データ全ての結果の平均を出力することになります。 次の分岐では、経路上の次のノードによって、この和に項を減らしたり加えたりします。 最終的な結果を得るためには、説明したいデータの経路に従って、式に加え続ける必要があります。 \\[\\hat{f}(x)=\\bar{y}+\\sum_{d=1}^D\\text{split.contrib(d,x)}=\\bar{y}+\\sum_{j=1}^p\\text{feat.contrib(j,x)}\\] 個々のインスタンスの予測は、目的変数の平均に、根のノードからそのインスタンスの属する終端ノードの間で起こる D 回の分岐の全ての和を足したものになります。 ただし、分岐の寄与度ではなく、特徴量の寄与度に関心があります。 1つの特徴量は、2回以上使われるかもしれませんし、1回も使われないかもしれません。 p 個の特徴量それぞれで、寄与度は加算でき、それぞれの特徴量がどれだけ予測に寄与してるかを解釈できます。 4.4.2 例 自転車レンタル数のデータをみてみましょう。 決定木を用いて、ある日の自転車レンタル数を予測してみましょう。 学習した決定木は以下の通りです。 FIGURE 4.17: 自転車レンタル数のデータセットで学習された回帰木。 木の最大深さは 2 に設定されている。トレンド特徴量 (2011年からの経過日数) と気温 (temp) が分割に選ばれている。箱ひげ図は終端ノードにおける自転車レンタル数の分布を示している。 1段目と2段目の1つの分岐は、時間のトレンドの特徴量によって行われていますが、データ収集を開始してからの日数を考慮に入れているので、レンタルサービスがだんだんと人気になっていった様子が表現されています。 105 日目より前のとき、自転車の数の予測は約 1800 台で、106 ~ 430 日目の間は約 3900 台となりました。430 日目以降については、予測は 4600 台 (気温が12度以下のとき)、または、6600 台 (気温が12度以上のとき) となりました。 特徴量重要度を見ると、ある特徴量がノードの純度をどの程度向上させるかが分かります。 ここでは、自転車レンタル数の予測は回帰問題であるので、分散が使用されています。 可視化された決定木によって、温度と時間のトレンドの両方が分岐に使われていることはわかりますが、どの特徴量がどれほど重要かは定量化できていません。 特徴量重要度は時間のトレンドが温度よりも需要であることを示しています。 FIGURE 4.18: 平均的にノードの不純度がどの程度改善されたかによって計算された特徴量重要度 4.4.3 長所 決定木の構造は、データの特徴量間の相互作用を捉えるために理想的です。 データは最終的に個別のグループに分かれるので、線型回帰のような多次元の超平面上の点として理解するよりも簡単です。 決定木の構造は、ノードと辺により自然な描画が可能です。 決定木は&quot;人間に優しい説明&quot;の章で定義されているように、よい説明を与えることができます。 決定木の構造は、自動的に個々のインスタンスに対して反事実的に予測値を考えるように誘導します。 つまり、「この特徴量が分岐点より、大きい(小さい)なら、予測値は y2 ではなくて y1 であったのに。」というようになります。 決定木の説明は対照的です。なぜなら、いつでも予測値と、決定木によって定められた&quot;what if&quot;シナリオ(他のノードの葉)と比べられるからです。 もし、木の深さが1から3のように短かったら、最終的な説明は選択的です。 深さが 3 の決定木は、個々のインスタンスの予測の説明を得るために、最大3つの特徴量と分岐点を必要とします。 予測値の真実性は、決定木の予測性能に依存します。 短い木の説明は非常に単純かつ一般的です。なぜなら、それぞれの分岐において、インスタンスは左右いずれかのノードに分かれていくので、このような二者択一は理解しやすいからです。 特徴量を変換する必要はありません。 線形モデルでは、特徴量の対数をとる必要があるかもしれません。 決定木は、特徴量の任意の単調変換に対して等価な振る舞いをします。 4.4.4 短所 決定木は線形な関係をうまく扱うことができません。 任意の入力特徴量と結果の間の線形な関係は、分岐されて作られたステップ関数で近似されます。 これは効率的ではありません。 これは、滑らかさの欠如と密接に関係しています。 入力特徴量のわずかな変化が、予測結果に大きな影響を与える場合がありますが、これは望ましくありません。 家のサイズを特徴量の1つとした決定木で住宅の価格を予測する場合を考えてみましょう。 100.5 平方メートルで分岐が発生したとします。 この決定木の予測モデルを使ってユーザが家の価格を見積もるとどうなるでしょうか。 家のサイズは 99 平方メートルだったとして、それを入力すると 200, 000 ユーロという予測結果を得ました。 ユーザは 2 平方メートルの倉庫部屋の計算を忘れていたことに気がつきました。 また、その倉庫には斜めの壁があったため、全ての面積を計算するべきか、半分にするべきか確証を持てませんでした。 なので、100.0 平方メートルと 101.0 平方メートルの場合のどちらもを試すことに決めました。 結果として、200, 000 ユーロと 205, 000 ユーロという予測結果が得られましたが、これはユーザの直感に反します。なぜなら、99 平方メートルが 100 平方メートルになっても価格に変化が生じなかったためです。 決定木は、かなり不安定でもあります。 学習データがわずかに変わっただけで、全く異なった決定木が作られることがあります。 これは、それぞれの分岐が親の分岐に依存しているためです。 そのため、もし最初の分岐で異なる特徴量が選択されたとすると、全体の木構造に違いが生じます。 このように、構造が容易に変化するため、モデルに信頼性があるとは言えません。 決定木は木の深さが小さいときは非常に解釈しやすいです。 終端ノードの数は深さにともなって急激に増加します。 木の深さが深くなり、終端ノードの数が増加するにつれて、木の決定規則を理解することがより難しくなります。 深さが 1 のときは2つの終端ノード、深さが 2 のときは最大4つ、深さが 3 のときは最大8つと、最大の終端ノードの数は、2の(木の深さ)乗となります。 4.4.5 ソフトウェア この章の例では、CART (Classification And Regression Tree) の実装は rpart という R パッケージを用いました。 CART はPythonをはじめ、多くのプログラミング言語で実装されています。 間違いなく、CARTはかなり古く、使い古されたアルゴリズムであり、木を学習するためのいくつかの興味深い新しいアルゴリズムがあります。 決定木に関するいくつかの R パッケージの概要は Machine Learning and Statistical Learning CRAN Task View の &quot;Recursive Partitioning&quot; の項目のところに書かれています。 Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. &quot;The elements of statistical learning&quot;. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009).↩ "],["rules.html", "4.5 決定規則", " 4.5 決定規則 決定規則は、条件(前提とも呼ばれる)と予測値からなる単純なIF-THEN文です。 例えば、 もし、今日雨が降っていて4月であるなら(条件)、明日雨が降るだろう(予測)というものです。 予測は、単一の決定規則、もしくは、いくつかの決定規則の組み合わせによって行われます。 決定規則は一般的な構造に従います。 もし、条件を満たしているなら、特定の予測がされます。 決定規則は、おそらく最も解釈しやすい予測モデルです。 IF-THEN 構造は意味的には、自然言語や私たちの考え方に似ています。 ただし、条件がわかりやすい特徴量から構成され、条件の長さは短く(少数のANDで結合されている場合)、規則の数が多すぎない必要があります。 プログラミングでは IF-THEN のルールで書くことはとても自然です。 機械学習の新しい点は、決定規則がアルゴリズムによって学習されることです。 家の価値 ('low', 'medium', 'high') を予測するための決定規則をアルゴリズムによって学習することを想像してください。 このモデルから学習できた1つの決定規則として、もし家が 100 平方メートル以上の広さがあり、庭付きならば、価値は高い。 より正確には、IF 'size&gt;100 AND garden=1' THEN 'value=high' とかけます。 決定規則を分解してみましょう。 'size&gt;100' は、IF部分の第一条件です。 'garden=1' は、IF部分の第二条件です。 2つの条件を 'AND' で合わせて新しい条件を作ります。2つの条件がともに真のときのみ、規則は適用されます。 予測結果(THEN部分)は 'value=high' です。 決定規則は条件の中で少なくとも1つの 'feature=value' のステートメントを使用しますが 'AND' で追加できる数に制限はありません。 例外は、明示的なIF部分を持たないデフォルト規則で、他の規則が適用されない場合に適用されますが、これについては後で 解説します。 決定規則の有用性は通常、サポート (Support) と 正答率 (Accuracy) の2つの数字で表されます。 サポート (Support, 規則の適用範囲) 規則の条件が適用されるインスタンスの割合をサポートと呼びます。 例えば、size=big AND location=good THEN value=high という家の価値を予測する規則を考えてみましょう。 1000 軒中 100 軒が大きく、立地が良い場合、この規則のサポートは 10% となります。 予測部分 (THEN部分) は、サポートの計算には必要ありません。 正答率 (Accuracy, 規則の確信度) 規則の正答率とは、規則の条件が適用されるインスタンスに対して、どの程度、正しいクラスと予測できるかを示す指標です。 例えば、size=big AND location=good THEN value=high という規則が適用される 100 軒の中で、value=high が 85 軒、value=medium が 14 軒、value=low が 1 軒であったとすると、この規則の正答率は 85% となります。 通常、正答率とサポートはトレードオフの関係にあります。 条件に新しい特徴量を追加することで正答率を上げることができますが、サポートは低下します。 家の価値を予測するための良い分類器を作成するためには、1つの規則だけではなく、10 から 20 の規則を学習する必要があるかもしれません。 その時、より複雑なものになり、以下のような問題にぶつかるかもしれません。 規則の重複: 家の価値を予測したい時に、2つ以上の条件が適用され、それらが矛盾した予測結果であったとき、どうすればいいのでしょうか？ 規則の未適用: 家の価値を予測したい時に、どの規則も適用されないとき、どうしたらいいのでしょうか？ 複数の規則を組み合わせるとき、決定リスト(順序付き)、決定集合(順序無し)の2つの主な戦略があります。 両方の戦略は、規則の重複問題に対して、異なる解決策を提示します。 決定リストは決定規則に順序付けを用います。 あるインスタンスに対して、最初の規則が真であれば、予測に最初の規則を用います。 偽であるならば、次の規則に進み、その規則を適用するかどうか確かめ、これを繰り返します。 決定リストは、適用される最初の規則の予測のみを返すことで、規則の重複問題を解決します。 決定集合は、いくつかの規則が高い投票権を持っているかもしれないということを除いては、民主主義の原理に似ています。 集合の中では、規則が互いに排他的であるか、多数決のような重複を解決する戦略が存在し、個々の規則が正答率や他の評価指標によって重み付けされます。 ただし、複数の規則が適用されると、解釈性が損なわれる可能性があるため注意が必要です。 決定リストも決定集合も、あるインスタンスに対して、どの規則も適用されないという問題が起こり得ます。 これは、デフォルト規則 (default rule) を導入することによって解決できます。 デフォルト規則は、どの規則も適用されない場合に適用される規則のことです。 デフォルト規則の予測は、他の規則でカバーされていないデータ点の中で最も頻度の高いクラスとすることが多いです。 規則の集合やリストが、特徴量空間全体をカバーしているとき、網羅的と呼びます。 デフォルト規則を追加することで、決定集合や決定リストは自動的に網羅的になります。 データから規則を学習する方法はたくさん存在しますが、本書ではそれら全てをカバーしていません。 この章では、それらのうちの3つを紹介します。 これらのアルゴリズムは、規則を学習するための一般的な考え方を幅広くカバーするように選ばれたため、これら3つは非常に異なるアプローチとなっています。 OneR は、単一の特徴量から規則を学習します。 OneR の特徴は、単純かつ理解しやすいことであり、ベンチマークとして用いられます。 Sequential covering は、繰り返し規則を学習していき、新しい規則でカバーされるデータ点を削除するという一般的な手法です。 この手法は、多くの規則を学習するアルゴリズムで用いられています。 Bayesian Rule Lists は、ベイズ統計を用いて、あらかじめ発見された頻出パターンを決定リストに結合します。 事前に発見されたパターンを使用することも、多くの規則を学習するアルゴリズムで使用されているアプローチです。 規則を学習するために単一の最も良い特徴量を選ぶという、最も単純なアプローチから始めましょう。 4.5.1 単一の特徴量による規則学習 (OneR) Holte(1993)18によって提案された OneR アルゴリズムは、最も単純な規則を導出するアルゴリズムの1つです。 全ての特徴量から，OneR は興味のある出力について最も情報量をもつ特徴量を選び、その特徴量から決定規則を作成します。 &quot;One Rule&quot; の略である OneR という名前にもかかわらず，このアルゴリズムは1つより多くのルールを生成します。 実際には、選択された最良の特徴量の値ごとに1つの規則を作ります。 したがって、OneFeatureRules のほうがふさわしい名前かもしれません。 このアルゴリズムは単純かつ高速です。 適切な間隔 (intervals) を選ぶことで，連続的な特徴量を離散化 各特徴量に対して、次を実行 特徴量の値と (カテゴリカルな) 出力間でクロステーブルを作成 各特徴量の値ごとに、クロステーブルから読み取れる特定の特徴量を持つインスタンスの、最も頻度の高いクラスを予測するための規則を作成 特徴量に対して、規則の誤差の合計を計算 誤差の合計が最小となる特徴量を選択 OneR は、選択された特徴量の全ての値を使用するため、常にデータセットにおける全インスタンスをカバーします。 欠損値は、追加の特徴量の値として扱うか事前に代入されます。 OneR モデルは分割が1つしかない決定木です。 その分割は、CART のように二分木である必要はなく、ユニークな特徴量の値の数に依存します。 OneR によってどのように最も良い特徴量が選ばれているか、例を見てみましょう。 次の表は、家についての価格、ロケーション、サイズ、ペットの可否の情報を持つ人工的なデータセットを示しています。 家の価格を予測するための単純なモデルを学習してみましょう。 location size pets value good small yes high good big no high good big no high bad medium no medium good medium only cats medium good small only cats medium bad medium yes medium bad small yes low bad medium yes low bad small no low OneR は各特徴と出力との間のクロステーブルを生成します。 value=low value=medium value=high location=bad 3 2 0 location=good 0 2 3 value=low value=medium value=high size=big 0 0 2 size=medium 1 3 0 size=small 2 1 1 value=low value=medium value=high pets=no 1 1 2 pets=only cats 0 2 0 pets=yes 2 1 1 各特徴に対して、1行ごとにクロステーブルを見ていき、各特徴量の値が、ルールにおける IF部分 に相当します。 この特徴量を持つインスタンスの最も一般的なクラスが予測値、つまり、規則の THEN部分 に相当します。 例えば、サイズに対しては、small、medium、big の3つの規則が得られます。 各特徴量に対して，生成された規則の全ての誤差率 (誤差の総和) を計算します。 「ロケーション」の特徴量は、bad と good を取りうる特徴量です。 ロケーションが bad の家の最も出現頻度が高い価格は low ですが、low を予測値としたとき、2つの誤りが生じます。 なぜなら、ロケーションが bad かつ、価格が medium である家が2つ存在するからです。 ロケーションが good の家の予測値を high としても、ロケーションが good かつ価格が medium の家が 2 軒あるため、ここでも2つの誤りが生じます。 ロケーションを特徴量に用いると、その誤差は 4/10、大きさでは 3/10、ペットの可否では 4/10 です。 大きさを特徴量とすると、最も低い誤差を持った規則が生成できるため、これが最終的に OneR モデルに用いられます。 IF size=small THEN value=small IF size=medium THEN value=medium IF size=big THEN value=high OneR は、多くのレベルをもつ特徴量が選ばれる傾向にあります。なぜなら、それらの特徴量を用いると簡単に過学習してしまうためです。 全ての特徴量がランダムな値をもち、目的値に対して有用な値を持たないようなノイズのみを含むデータセットを想定してください。 いくつかの特徴量は他の特徴量より多くのレベルを持っています。 そのような多くのレベルを持った特徴量は過学習が起きやすくなります。 ある特徴量がインスタンスごとに異なるレベルを持っていたとすると、学習データ全体を完全に予測できてしまいます。 この問題に対する解決策は、データを学習用 (training data) と評価用 (validation data)に分けて、学習用のデータを用いて規則を学習し、選ばれた特徴量の評価は評価用のデータを用いて行います。 複数の特徴量が同じ誤差となるときが、もう1つの問題となります。 OneR では、このような場合は、誤差が最小の最初の特徴量を選択する、もしくは、カイ2乗検定の p値 が最小の特徴量を選択するようにします。 例 OneR を実データに適用してみましょう。 OneR アルゴリズムを子宮頸がんの分類タスク に適用してみます。 全ての連続な入力の特徴を5分位に離散化したところ、以下のような規則が作成されました。 Age prediction (12.9,27.2] Healthy (27.2,41.4] Healthy (41.4,55.6] Healthy (55.6,69.8] Healthy (69.8,84.1] Healthy OneR によって年齢の特徴量が最良の特徴量として選択されました。 がんは滅多に起こらないため、各規則はデータ数の多いクラスとなります。 従って、予測されるラベルが常に Healthy となり、これはあまり役に立たない結果と言えます。 このように、不均衡データに対するラベルの予測で使用しても意味がありません。 Age の間隔と Cancer/Healthy の間のクロステーブルに、癌にかかった女性の割合を加味するとより有益です。 # Cancer # Healthy P(Cancer) Age=(12.9,27.2] 26 477 0.05 Age=(27.2,41.4] 25 290 0.08 Age=(41.4,55.6] 4 31 0.11 Age=(55.6,69.8] 0 1 0.00 Age=(69.8,84.1] 0 4 0.00 ただし、解釈を始める前に注意しなければいけないことがあります。 全ての特徴量の全ての値に対する予測は Healthy だったため、全ての特徴量に対する合計の誤差率は同じです。 複数の特徴量で合計の誤差率が等しい場合、基本的には、最も誤差率の低い特徴量の中で、最初のものが使用されます(全ての特徴量は誤差率 55/858)。これがたまたま「Age feature」だったのです。 OneR は回帰問題では使用できません。 しかし、出力をいくつかの区間に分割することで回帰問題を分類問題に落とし込むことができます。 この手法を自転車レンタル台数予測に使ってみましょう。 自転車の数を四分位数(0~25%, 25~50%, 50~75%, 75~100%)で分割することで、OneR を用いて予測します。 OneR モデルで選択された特徴量の表は以下の通りです。 mnth prediction JAN [22,3152] FEB [22,3152] MAR [22,3152] APR (3152,4548] MAY (5956,8714] JUN (4548,5956] JUL (5956,8714] AUG (5956,8714] SEP (5956,8714] OKT (5956,8714] NOV (3152,4548] DEZ [22,3152] 選択された特徴量は月 (month) でした。 月の特徴量は（驚くべきことに！）12段階に分かれており、これは他のほとんどの特徴量よりも多いです。 そのため、過学習の危険性があります。 しかし、より楽観的な立場からすると、月の特徴量は季節のトレンド（例えば、冬はレンタル自転車の人気がなくなるなど）を捉えることができるため、その予測は賢明なのかもしれません。 それでは、単純な OneR アルゴリズムから、より複雑な手順で、いくつかの特徴量からなる複雑な条件を持つ規則を学習するための Sequential Covering に移りましょう。 4.5.2 Sequential Covering Sequential Covering とは、1つの規則を繰り返し学習し、ルールごとにデータセット全体をカバーする決定リスト（または決定集合）を作成する一般的な手続きです。 多くの規則を学習するアルゴリズムは、Sequential Covering の一種です。 この章では、手法の概要を紹介し、例として、Sequential Covering の応用形である RIPPER を使用します。 アイデアはシンプルです。 まずはいくつかのデータに当てはまる良い規則を見つけます。 そして、その規則でカバーされる全てのデータ点を削除します。 データ点がカバーされるのは、条件が適用されたときであり、その点が正しく分類されたかどうかとは関係がないことに注意してください。 この規則を学習し、カバーされた点を削除することを、残りのデータ点がなくなるか、他の停止条件が満たされるまで繰り返します。 その結果、決定リストが得られます。 この、規則の学習とカバーされたデータ点の削除を繰り返す手法を 「separate-and-conquer」と呼びます。 データの一部をカバーする単一のルールを作成できるアルゴリズムをすでに我々は持っているとします。 2つのクラス（positiveとnegative）に対する、sequential covering アルゴリズムは 以下のように動作します。 空の規則のリストから始める（rlist） 規則 r を学習 規則のリストがある閾値を下回っている間（もしくは positive な例がまだカバーされていない間）： 規則 r を rlist に追加 規則 r によってカバーされるデータ点を全て削除 残ったデータに対して、他の規則を学習 決定リストを返す FIGURE 4.19: アルゴリズムは単一の規則で特徴空間を順次カバーし、それらのルールで既にカバーされているデータ点を削除していくことで動作します。可視化のために、特徴量 x1 と x2 は連続量ですが、ほとんどの規則学習アルゴリズムはカテゴリカル特徴量を必要とします。 例として、家のサイズ、ロケーション、およびペットの可否から家の価値を予測するタスクおよびデータセットがあるとします。 初めに学習する規則は、もし、size=big かつ location=good ならば、value=high となります。 そして、データセットから全てのよいロケーションにある大きな家を削除します。 残ったデータで、我々は次の規則を学習すると、location=good ならば、value=medium となります。 注意すべき点としては、この規則は、ロケーションがよく大きな家を除いたデータで学習されており、ロケーションのいい家は medium か small しか残されていないということです。 多クラスの設定の場合は、アプローチを変える必要があります。 初めに、クラスは普及率を昇順に並べます。 sequential covering アルゴリズムは、最も一般的でないクラスから始まり、それのための規則を学習し、カバーされたインスタンスを全て削除し、次に一般的でないクラスに移動していきます。 現在のクラスは常にポジティブクラスとして扱われ、より高い普及率を持つ全てのクラスはネガティブクラスとしてまとめられます。 最後のクラスはデフォルト規則となります。 これは分類問題における one-versus-all 戦略とも呼ばれます。 どうやって1つのルールを学習するのでしょうか。 OneR アルゴリズムは、全ての特徴空間をカバーするので、役に立たないでしょう。 しかし、他にも多くのいろいろな可能性があります。 1つの可能性としては、ビームサーチを用いて決定木から単一の規則を学習することです。 決定木を（CARTや他の木学習アルゴリズムを用いて）学習します。 ルートノードから出発し、再帰的に最も不純度の低いノード（例：誤分類率が最も低いノード）を選択していきます。 規則の予測には、終端ノードにおける多数派のクラスが使用されます。つまり、そのノードに到達までのパスがルールの条件として使われます。 以下の図は、木をビームサーチした様子です。 FIGURE 4.20: 決定木のパスを探索することで規則を学習する。決定木は興味のある目的値を予測するために成長する。ルートノードから出発し、純度の高い(例: 正答率の高い)部分集合のパスへ貪欲的に遷移し、全ての分割の値を規則の条件に加える。最終的に、もし、location=good かつ size=big ならば value=highを得る。 単一の規則を学習することは、全ての可能な規則からなる空間が探索空間であるような探索問題です。 探索のゴールは、何らかの基準によって最適な規則を見つけることです。 いくつかの異なる探索の方策があります。 山登り法 (hill-climbing)、ビームサーチ (beam search)、全探索 (exhaustive search)、最良優先探索 (best-first search)、順序探索 (ordered search), 確率的探索 (stochastic search), トップダウン探索 (top-down search)、ボトムアップ探索 (bottom-up search)、など。 Cohen (1995)19 による RIPPER (Repeated Incremental Pruning to Produce Error Reduction) は Sequential Covering アルゴリズムの一種です。 RIPPER はより洗練されており、後処理 (rule pruning) を使って決定リスト (または、決定集合)を最適化します。 RIPPER は順序付き、順序なしモードで実行することができ、決定リストまたは決定集合のいずれかを生成できます。 例 例として、RIPPER を使用してみましょう。 RIPPER アルゴリズムは、子宮頸癌の分類問題において、規則を発見しません。 RIPPER を自転車レンタル台数の予測の回帰問題に適用したとき、いくつかの規則が見つかります。 RIPPER は分類問題に対して動作するため、自転車の数はカテゴリカルな出力に変換しなければいけません。 そのため、自転車の数は四分位数に変換しています。 例えば、(4548, 5956) は予測された自転車の数が 4548 台から 5956 台の間の区間を示しています。 次の表は、学習された規則の決定リストを示しています。 rules (days_since_2011 &gt;= 438) and (temp &gt;= 17) and (temp &lt;= 27) and (hum &lt;= 67) =&gt; cnt=(5956,8714] (days_since_2011 &gt;= 443) and (temp &gt;= 12) and (weathersit = GOOD) and (hum &gt;= 59) =&gt; cnt=(5956,8714] (days_since_2011 &gt;= 441) and (windspeed &lt;= 10) and (temp &gt;= 13) =&gt; cnt=(5956,8714] (temp &gt;= 12) and (hum &lt;= 68) and (days_since_2011 &gt;= 551) =&gt; cnt=(5956,8714] (days_since_2011 &gt;= 100) and (days_since_2011 &lt;= 434) and (hum &lt;= 72) and (workingday = WORKING DAY) =&gt; cnt=(3152,4548] (days_since_2011 &gt;= 106) and (days_since_2011 &lt;= 323) =&gt; cnt=(3152,4548] =&gt; cnt=[22,3152] 解釈は単純明快です。 もし、条件が適用されたら、右側の区間の自転車の台数であると予測します。 最後の規則はデフォルト規則で、インスタンスに対してどの規則も適用されなかったときに適用されます。 新しいインスタンスに対して予測するためには、リストの上から出発し、規則が適用されるかチェックします。 条件がマッチしたとき、規則の右側の値がこのインスタンスに対する予測となります。 デフォルト規則は、常に予測値が存在することを保証します。 4.5.3 Bayesian Rule Lists この章では、次の大まかな手順に従って、決定リストを学習する別のアプローチを紹介します。 決定規則の条件として使える頻出パターンをデータから事前にマイニングしておきます。 マイニングされた規則からいくつかを選択し、決定リストを学習します。 このようなアプローチを Bayesian Rule Lists (Lethan et. al, 2015)20または、略して BRL と呼びます。 BRL はベイズ統計を用いて、FP-tree アルゴリズム (Borgelt 2005)21 でマイニングされた頻出パターンから決定リストを学習しますが、まずは BRL の最初のステップからゆっくり始めましょう。 頻出パターンの事前マイニング 頻出パターンとは、特徴量の頻繁な共起のことを言います。 BRL アルゴリズムの前処理ステップとして、特徴量を使って頻出パターンを抽出します（この段階では目的値は不必要です）。 パターンには、size=medium のような単一の特徴量のものや、size=medium AND location=bad のような特徴量の組み合わせのものがあります。 パターンの頻度は次のように、データセット内のサポートで定量化されます。 \\[Support(x_j=A)=\\frac{1}n{}\\sum_{i=1}^nI(x^{(i)}_{j}=A)\\] ただし、A は特徴量の値、n はデータセット内のデータの数、I はデータ i の特徴 \\(x_j\\) のレベルが A の場合は 1、そうでない場合は 0 を返す指示関数です。 家の価値のデータセットで、もし家の20%にベランダがなく、80%で一個以上のベランダがあった場合、パターン balcony=0 に対するサポートは20%になります。 サポートは、balcony=0 AND pets=allowed のような、特徴量の組み合わせについても同様に測定できます。 AprioriやFP-Growth のような、頻出パターンを発見するためのアルゴリズムはたくさんあります。 結果のパターンは常に同じなので、計算速度だけが異なるため、どれを用いるかはそれほど重要ではありません。 Aprioriアルゴリズムがどのように頻繁なパターンを見つけるかについて大まかに説明します。 実は、Aprioriアルゴリズムは2つの部分で構成されており、まず最初に頻出パターンを見つけ、その次に、それらから相関規則を構築します。 BRL アルゴリズムにおいては、Aprioriアルゴリズムの最初の部分で生成される頻出パターンにのみ関心があります。 最初のステップでは、ユーザが定義した閾値より大きいサポートを持つすべての特徴量から始まります。 ユーザが最小のサポートを10%に設定しており、家の5%のみが size=big になっている場合、その特徴量の値は削除され、 size=medium と size=small のみがパターンとして保持されます。 これは、 size=big を持つ家がデータから削除されるということではなく、 size=big が頻出パターンとして返されなくなるという意味です。 Aprioriアルゴリズムは、単一の特徴量を持つ頻出パターンに基づいて、より高次の特徴量の組み合わせを繰り返し発見します。 パターンは、 feature=value ステートメントを論理 AND と組み合わせて構築されます。（例： size=medium AND location=bad ） 生成されたパターンのうち、閾値未満のサポートを持つものは削除されます。 最後には、すべての頻繁なパターンを持つことになります。 頻出パターンの部分集合もまた、頻出パターンになります。これはApriori propertyと呼ばれます。 これは、直感的にも成り立ちます。 パターンからある条件を外すと、削減された後のパターンは、より広い（または同じ）範囲のデータをカバーできるようになり、範囲が狭くなることはありえません。 例えば、家の20%が size=medium AND location=good ならば、 size=medium のみの家のサポートは20%以上になります。 このApriori propertyは、検査すべきパターンの数を減らすために使われます。 頻出パターンに対してのみ、高次のパターンをチェックする必要があります。 これで BRL アルゴリズムのための条件の事前マイニングが完了しました。 BRL の次のステップに進む前に、パターンの事前マイニングに基づく規則学習の別の方法を紹介します。 他のアプローチでは、関心のある出力結果を頻出パターンのマイニングプロセスに含め、Apriori アルゴリズムの2番目のステップである IF-THEN ルールを構築する部分でも使用することが提案されています。 教師なし学習アルゴリズムなので、THEN 部分に関心のない特徴量も含まれてしまいます。 ただし、THEN 部分に関心のある出力結果のみを持つ規則でフィルタリングできます。 これらの規則はすでに決定集合を形成していますが、規則の再配列、削除、再結合もできます。 しかしながら、BRL アルゴリズムでは、ベイズ統計を用いて頻出パターンから THEN 部分と決定リストに配置する方法を学習します。 Bayesian Rule Lists による学習 BRL アルゴリズムのゴールは、事前にマイニングされた条件から選択して、なるべく少ない規則、短い条件のリストとなることを優先させながら、正確な決定リストを学習することです。 BRL は、条件の長さ（短いルールで）と規則の数（短いリストで）に関する事前分布を用いて決定リストの分布を定義することにより、この目標を達成します。 リストの事後確率分布により、短さの仮定とどの程度データに適合しているかに基づいて、決定リストがどの程度尤もらしいかを言うことができます。 私たちの目標は、この事後確率を最大化するリストを見つけることです。 リストの分布から直接、最良のリストを見つけることはできないため、BRL は次のような手順に従います。 事前分布からランダムに最初の決定リストを生成します。 規則の追加、切り替え、または削除を繰り返し行い、結果のリストが、リストの事後分布に従うようにします。 事後分布に従ってサンプリングされたリストから最も確率の高い決定リストを選択します。 アルゴリズムをさらに詳しく見ていきましょう。 このアルゴリズムは、FP-Growth アルゴリズムを用いた特徴量のパターンを事前マイニングすることから始まります。 BRL は目的値の分布と、目的値の分布を定義するパラメータの分布について、いくつかの仮定をします。 （これがベイズ統計です。） ベイズ統計に慣れていない方は、以下の説明にとらわれすぎないようにしてください。 ベイズ統計のアプローチは、モデルをデータにフィットさせる一方で、既存の知識や必要条件（いわゆる事前分布）を組み合わせる方法であることを知っておくことが重要です。 決定リストの場合、決定リストの規則が短くなるように事前分布によって調整されるため、ベイズ統計のアプローチは理にかなっています。 ゴールは、事後分布から決定リスト d をサンプリングすることです。 \\[\\underbrace{p(d|x,y,A,\\alpha,\\lambda,\\eta)}_{posteriori}\\propto\\underbrace{p(y|x,d,\\alpha)}_{likelihood}\\cdot\\underbrace{p(d|A,\\lambda,\\eta)}_{priori}\\] ただし、d は決定リスト、x は特徴量、y は目的値、A は事前にマイニングされた条件の集合、\\(\\lambda\\) は事前に予想される決定リストの長さ、\\(\\eta\\) は事前に予想される規則の中の条件の数、\\(\\alpha\\) は正と負クラスに対する事前の擬似的なカウントで、 (1,1) に固定する方が良いです。 \\[p(d|x,y,A,\\alpha,\\lambda,\\eta)\\] この式は観測されたデータと事前の仮定に基づいて、決定リストの可能性を定量化します。 これは、決定リストとデータが与えられたときの出力 y の尤度と、与えられた事前情報と事前にマイニングされた条件に対するリストの確率をかけたものに比例します。 \\[p(y|x,d,\\alpha)\\] この式は、決定リストとデータが与えられたときに観測された y の尤度です。 BRL では y はディリクレ多項分布 (Dirichlet-Multinomial distribution) によって生成されることを仮定しています。 決定リスト d がデータをうまく説明できるほど、尤度は高くなります。 \\[p(d|A,\\lambda,\\eta)\\] この式は、決定リストの事前分布です。 これは、リスト内の規則の数に対するパラメータ \\(\\lambda\\) の truncated Poisson distribution と 規則の条件の特徴量の値の数に対するパラメータ \\(\\eta\\) の truncated Poisson distribution を掛け合わせます。 決定リストは、出力 y をうまく説明し、事前の仮定に従っている可能性が高いほど、事後確率が高くなります。 ベイズ統計の推定には少しトリッキーです。なぜなら、直接正解を計算できるとは限らず、通常は、候補を選んで評価し、マルコフ連鎖モンテカルロ法 (MCMC) を用いて事後推定を更新する必要があるからです。 決定リストの場合、決定リストの分布から引き出す必要があるため、さらに複雑になります。 BRL の著者は、まず最初の決定リストを作成し、次にそれを繰り返し変更して、リストの事後分布（決定リストのマルコフ連鎖）から決定リストのサンプルを生成することを提案しています。 これによって得られる結果は最初の決定リストに依存する、この手順を繰り返し実行し、多様なリストを確保することが望ましいです。ソフトウェアの実装の中では、基本的に10回繰り返します。 以下の手順は、最初の決定リストの作り方を示しています。 FP-Growthでパターンを事前にマイニング truncated Poisson distribution から、リストの長さのパラメータ m をサンプリング デフォルト規則の場合 (他に何も適用しない場合に用いられるルール)は以下を実行 目的値に関するディリクレ多項分布のパラメータ \\(\\theta_0\\) をサンプリング 決定リストの規則 j = 1,...,m に対して、以下を実行 規則 j に対して、規則の長さのパラメータ l (条件の数) をサンプリング 事前にマイニングした条件から、長さが \\(l_j\\) の条件をサンプリング THEN部分(規則によって与えられた出力結果の分布)に対して、ディリクレ多項分布のパラメータをサンプリング データセットのそれぞれの観測値に対して以下を実行 決定リストを上から下に探索し、最初に適用する規則を見つける 適合するルールによって提案された確率分布 (二項分布) から予測結果を引き出す 次のステップは、決定リストの事後分布から多くのサンプルを取得するために、この最初のサンプルからスタートし、たくさんの新しいリストを生成することです。 新しい決定リストは最初のリストから開始し、規則をリスト内の別の場所に移動するか、事前にマイニングされた条件から現在の決定リストに規則を追加するか、もしくは決定リストから規則を削除することによってサンプリングされます。 これらの規則の切り替え、追加、削除は無作為に選ばれて適用されます。 それぞれのステップにおいて、アルゴリズムは決定リストの（正答率と短さの組み合わさった）事後確率を評価します。 Metropolis Hastings アルゴリズムは、事後確率が高い決定リストをサンプリングすることを保証します。 この手順によって、決定リストの分布から多くのサンプルを得ることができます。 BRL アルゴリズムは最も高い事後確率を持つサンプルの決定リストを選択します。 例 理論はこれぐらいにして、BRL 法の動作を見てみましょう。 例では、Yang らによる BRL をより高速化した Scalable Bayesian Rule List (SBRL, 2017) 22を使用します。 SBRL アルゴリズムを子宮頸がんのリスクの予測に適用します。 まずはじめに、全ての入力特徴量を SBRL アルゴリズムで使用可能なように離散化する必要があります。 この目的のために、連続特徴量は分位数の頻度に基づいてビン化しています。 すると、以下のようなルールを得ることができます。 rules If {STDs=1} (rule[259]) then positive probability = 0.16049383 else if {Hormonal.Contraceptives..years.=[0,10)} (rule[82]) then positive probability = 0.04685408 else (default rule) then positive probability = 0.27777778 予測の THEN 部分がクラスの結果ではなく、がんの予測確率であるため、実用的なルールを得ることができていることに注意してください。 条件は、あらかじめ探索された FP-Growth アルゴリズムを使って得られたパターンから選択されました。 次の表は、SBRL アルゴリズムが決定リストを作成するために選択できる条件の候補を示しています。 ユーザが設定した、条件に含まれる最大の特徴量の数は 2 としています。 以下が 10 パターンの例です。 pre-mined conditions Num.of.pregnancies=[3.67,7.33) IUD=0,STDs=1 Number.of.sexual.partners=[1,10),STDs..Time.since.last.diagnosis=[1,8) First.sexual.intercourse=[10,17.3),STDs=0 Smokes=1,IUD..years.=[0,6.33) Hormonal.Contraceptives..years.=[10,20),STDs..Number.of.diagnosis=[0,1) Age=[13,36.7) Hormonal.Contraceptives=1,STDs..Number.of.diagnosis=[0,1) Number.of.sexual.partners=[1,10),STDs..number.=[0,1.33) STDs..number.=[1.33,2.67),STDs..Time.since.first.diagnosis=[1,8) 次に、自転車レンタル予測のタスクにも、SBRL アルゴリズムを適用してみましょう。 これは、自転車の数を予測する問題が、二値分類の問題に変換できたときのみ使用可能です。 そのため、ここでは恣意的に 1 日の自転車レンタル数が 4000 を超えるとき 1 , そうでないときは0とラベルを付与することで、分類問題に変換しています。 rules If {yr=2011,temp=[-5.22,7.35)} (rule[718]) then positive probability = 0.01041667 else if {yr=2012,temp=[7.35,19.9)} (rule[823]) then positive probability = 0.88125000 else if {yr=2012,temp=[19.9,32.5]} (rule[816]) then positive probability = 0.99253731 else if {season=SPRING} (rule[351]) then positive probability = 0.06410256 else if {temp=[7.35,19.9)} (rule[489]) then positive probability = 0.44444444 else (default rule) then positive probability = 0.79746835 気温が摂氏 17 度で、2012 年の 1日で自転車の数が 4000 を超える確率を予測してみましょう。 最初のルールは、2011 年の時のみ適用されるため、今回は適用されません。 2012 年で 17 度のときは、区間 [7.35,19.9) に入っているので、 2つ目のルールは適用されます。 予測の結果、4000 台を超える確率は 88% となりました。 4.5.4 長所 この章では一般的な IF-THEN ルールの長所について議論します。 IF-THEN ルールは解釈することが簡単です。 これはおそらく最も解釈しやすい解釈可能モデルと言えます。 ただし、このように言えるのは、ルールの数が少ないときに限られ、ある規則の条件が少なく(多くとも3が好ましい)、規則が決定リストか重複のない決定集合で管理される場合です。 決定規則は決定木のように表現力がありながら、よりコンパクトです。 決定木は複製された部分技に苦しむことが多く、これは、分岐点の左右の子ノードが同じ構造を持つときに起こります。 どのルールに決めるのかの少数のバイナリステートメントを確認するだけなので、IF-THEN ルールの予測は高速です。 決定規則は、入力特徴量の単調変換に対しては、条件に関する閾値が変わるだけなので、頑健です。 条件が適用されるかどうかの問題なので、外れ値に対しても頑健です。 IF-THEN ルールは通常、少数な特徴量だけを含むスパースなモデルを生成します。 モデルに関連する特徴量だけを選択するのです。 例えば、線形モデルは基本的にはすべての入力特徴量に重みを割り当てます。 無関係な特徴量は IF-THEN ルールでは、単に無視されるでしょう。 OneR のような単純な規則は、より複雑なアルゴリズムに対するベースラインとして使えるでしょう。 4.5.5 短所 この章では一般的な IF-THEN ルールの欠点について扱います。 IF-THEN ルールに関する研究や書物では分類に焦点を当ていて、完全に回帰を無視しています。 ほとんどの場合、連続値をある区間に分割することで分類問題に変形できますが、それによって必ず情報を失います。 一般的に、回帰と分類の両方に使える方法はより魅力的です。 また、特徴量はカテゴリカルでなければいけません。 つまり、量的特徴量を使いたいときは、カテゴリカル化しなければいけません。 連続値の特徴量をある区間に切る方法は沢山ありますが、これは自明なことではなく、明確な答えのない多くの疑問が付随します。 いくつの区間で特徴量を分けるべきか、分割の基準はなにか、固定長の区間か、分位点か、その他のなにかか。 連続値の特徴量をカテゴリカル化するのは重大な問題であるのに、無視されがちで、多くの人はここで例示したように、単に次の最も良い方法を使います。 多くの古いルール学習アルゴリズムは過学習する傾向があります。 ここで紹介したすべてのアルゴリズムは、過学習を防ぐために少なくともいくつかの安全策を講じています。 OneR は1つの特徴量しか使わないように制限されており (ただし、特徴量が多すぎるレベルを持っていたり、多重検定問題に相当するような特徴量が多すぎる場合には問題になります)、RIPPER ではプルーニングを行い、Bayesian Rule Lists では決定リストの事前分布として制約を課しています。 決定規則は、特徴量と出力との線形な関係を表現することには向いていません。 これは、決定木と共通する問題です。 決定木や決定規則はステップ状の予測関数しか生成できないため、常に予測の変化は離散的な階段状となり滑らかなカーブにはなりません。 これは、入力がカテゴリカルでなければいけないことに関連した問題です。 決定木では、分割によって暗黙的にカテゴリカル化が行われています。 4.5.6 ソフトウェアと代替手法 OneR は R パッケージ OneRに実装されており、この本の例でも使用されています。 OneR は機械学習ライブラリのWekaにも実装されており、Java や R 、そしてPythonで利用できます。 RIPPER も Weka で実装されています。例えば、私は RWekaパッケージ内の JRIP の R 実装を使いました。 SBRL も、この本の例で実行しているように、Rパッケージで利用できます。 他にも、Python や C言語 でも使えます。 決定集合や決定リストを学習する方法の全ての代替手法をリスト化することはしていませんが、ここでは、それらの要約を紹介します。 Fuernkranz らによる &quot;Foundations of Rule Learning&quot; (2012)23 の 本をおすすめします。 これは、決定規則に関して、より深い知識を身に付けたい人にとって役立つでしょう。 この本では、学習規則を考えるための全体的なフレームワークや、多くの規則を学習させるアルゴリズムを紹介しています。 また、こちらの資料(Weka rule learners) もおすすめで、RIPPER、 M5Rules、 OneR、 PART、その他諸々の実装があります。 IF-THEN ルールは、この本のRuleFit algorithmの章で述べられている通り、線形モデルで使用できます。 Holte, Robert C. &quot;Very simple classification rules perform well on most commonly used datasets.&quot; Machine learning 11.1 (1993): 63-90.↩ Cohen, William W. &quot;Fast effective rule induction.&quot; Machine Learning Proceedings (1995). 115-123.↩ Letham, Benjamin, et al. &quot;Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model.&quot; The Annals of Applied Statistics 9.3 (2015): 1350-1371.↩ Borgelt, C. &quot;An implementation of the FP-growth algorithm.&quot; Proceedings of the 1st International Workshop on Open Source Data Mining Frequent Pattern Mining Implementations - OSDM ’05, 1–5. http://doi.org/10.1145/1133905.1133907 (2005).↩ Yang, Hongyu, Cynthia Rudin, and Margo Seltzer. &quot;Scalable Bayesian rule lists.&quot; Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.↩ Fürnkranz, Johannes, Dragan Gamberger, and Nada Lavrač. &quot;Foundations of rule learning.&quot; Springer Science &amp; Business Media, (2012).↩ "],["rulefit.html", "4.6 RuleFit", " 4.6 RuleFit Friedman と Popescu (2008)24 による RuleFit アルゴリズムは、相互作用効果を決定規則の形で自動的に検出したスパース線形モデルの学習に使われます。 線形回帰モデルは特徴間の相互作用を考慮していません。 線形モデルのようにシンプルで解釈しやすいモデルでありながら、特徴間の相互作用を統合したモデルがあれば便利ではないでしょうか？ 実は、RuleFit はギャップを埋めることができます。 RuleFit は、元の特徴量と決定規則である多数の新しい特徴量を用いて、スパース線形モデルを学習します。 これらの新しい特徴量は、元の特徴量間の相互作用を説明します。 RuleFit は、決定木からこれらの特徴量を自動的に生成します。 分割された決定を結合し、規則にすることで、木を通る各パスを決定規則に変換できます。 ノードによる予測を破棄し、分割のみを決定規則に使用します。 FIGURE 4.21: 3つの終端ノードを持つ木から4つの規則を生成することができます。 これらの決定木はどこから来ているのでしょうか？ 決定木は興味のある結果を予測するために学習されます。 これは、予測問題に対して分割が意味を持つことを保証しています。 ランダムフォレストのように、多数の木を生成するアルゴリズムを RuleFit に使うことができます。 それぞれの木は、スパース線形回帰モデル (Lasso) で使用される追加の特徴量である決定規則に分解されます。 RuleFit が提案された論文では、ボストンの住宅データを使って説明しています。 目標は、ボストンの住宅の中央値を予測することです。 RuleFit で生成された規則の1つは、部屋の数 &gt; 6.64 かつ 一酸化炭素濃度 &lt; 0.67 であるならば 1、そうでないならば 0 としています。 RuleFit は、予測に重要な線形項や規則を特定するための重要度の指標としても機能します。 特徴量の重要度は回帰モデルの回帰係数から計算されます。 重要度の指標は元の特徴量（&quot;生&quot;の形で使われ、多くの決定規則で使われる可能性があります）を集約したものになります。 RuleFit は、特徴量を変更することで、予測値の平均的な変化を示す partial dependence plot を導入します。 partial dependence plot は、どんなモデルにも使うことのできるモデル診断の手法であり、partial dependence plotsの章で解説されています。 4.6.1 解釈と例 RuleFit は最終的には線形モデルを推定するので、解釈は&quot;普通の&quot;線形モデルと同じになります。 違いは、決定規則からなる新しい特徴量を有していることです。 値が 1 であることは全ての条件を満たしていることを示し、そうでない場合は値は 0 になります。 RuleFit における線形項の解釈は、線形回帰モデルと同様になります。ある特徴量が 1 増加すると、予測結果は特徴量の重みに応じて変化します。 この例では、RuleFit をある日付のレンタル自転車数を予測するために使用しています。 この表は、RuleFit によって生成された5つの規則と、それらの Lasso による重みと重要性を示しています。 計算については、この章の後半で説明します。 Description Weight Importance days_since_2011 &gt; 111 &amp; weathersit in (&quot;GOOD&quot;, &quot;MISTY&quot;) 793 303 37.25 &lt;= hum &lt;= 90 -20 272 temp &gt; 13 &amp; days_since_2011 &gt; 554 676 239 4 &lt;= windspeed &lt;= 24 -41 202 days_since_2011 &gt; 428 &amp; temp &gt; 5 366 179 最も重要な規則は &quot;days_since_2011 &gt; 111 &amp; weathersit in (&quot;GOOD&quot;, &quot;MISTY&quot;)&quot; であり、対応する重みは 793 です。 このような規則は、元の 8 つの特徴量から合計 278 個が作成されました。 かなりの数です! しかし Lasso のおかげで、278 のうち 58 だけが 0 ではない重みを持っていることがわかります。 大域的な特徴量の重要性を計算すると、気温と時間の傾向が最も重要な特徴量であることがわかります。 FIGURE 4.22: 自転車の数を予測する RuleFit モデルの特徴量重要度。予測のために最も重要な特徴量は気温と時間傾向でした。 特徴量重要度の尺度は、生の特徴量の重要度と、その特徴量が現れるすべての決定規則を含みます。 解釈のテンプレート 解釈は、線形モデルと類似しています。 他の特徴量が固定されている場合、特徴量 \\(x_j\\) が 1 変化すると、予測結果は \\(\\beta_j\\) だけ変化します。 決定規則に関する重みの解釈は特殊です。 決定規則 \\(r_k\\) のすべての条件を満たすならば、予測結果は \\(\\alpha_k\\) (線形モデルで学習された規則 \\(r_k\\) の重み) だけ変化します。 分類問題では（線形回帰ではなくロジスティック回帰を用いた場合）、 決定規則 \\(r_k\\) の全ての条件を満たすなら、その事象が発生するかしないかのオッズが \\(\\alpha_k\\) 倍変化します。 4.6.2 理論 RuleFit アルゴリズムの技術的な詳細について深く見ていくことにしましょう。 RuleFit は2つのコンポーネントで構成されています。 最初のコンポーネントは、決定木から&quot;規則&quot;を作成し、2番目のコンポーネントでは、元の特徴量と作成した規則を入力とする線形モデルを学習します（これが &quot;RuleFit&quot; という名前の由来です）。 Step 1: 規則の生成 規則とはどのようなものでしょうか？ アルゴリズムによって生成された規則は単純な形式になります。 例えば、IF x2 &lt; 3 AND x5 &lt; 7 THEN 1 ELSE 0 といったものです。 規則は、決定木を分解することで構築されます。 決定木上の任意のパスは、決定規則に変換できます。 規則のための木は、出力を予測するために利用されます。 したがって、分割や得られる規則は興味のある結果を得るために最適化されています。 特定のノードに至る二分決定を &quot;AND&quot; で連結させるだけで規則ができます。 多様かつ意味のある規則を多く生成することが望まれます。 勾配ブースティングでは、y を元の特徴量 X を使って回帰あるいは分類をすることで、決定木のアンサンブルを学習させます。 そして作成された各々の木は、複数の規則に変換されます。 ブースティングに限らず、任意の木のアンサンブルアルゴリズムに対して、RuleFit の木を生成できます。 木のアンサンブルは、次の一般的な式で記述できます。 \\[f(x)=a_0+\\sum_{m=1}^M{}a_m{}f_m(X)\\] M は木の数であり、\\(f_m(x)\\) は m 番目の木の予測関数です。 \\(a\\) は重みです。 Bagged ensembles、ランダムフォレスト、AdaBoost、そして MART は木のアンサンブルを生成し、RuleFit で使用されます。 アンサンブルの全ての木から規則を作成します。 各規則 \\(r_m\\) は次の形式で表されます。 \\[r_m(x)=\\prod_{j\\in\\text{T}_m}I(x_j\\in{}s_{jm})\\] ここで、\\(\\text{T}_{m}\\) は、m 番目の木で利用される特徴量の集合です。 I は、特徴量 \\(x_j\\) が j 番目の特徴量（木の分割で指定されたもの）に対する部分集合 s に含まれる場合に 1、それ以外の場合に 0 となる指示関数です。 量的特徴量の場合、\\(s_{jm}\\) は特徴量の値の区間となります。 区間は次の2つの場合のいずれかのようになります。 \\[x_{s_{jm},\\text{lower}}&lt;x_j\\] \\[x_j&lt;x_{s_{jm},upper}\\] 特徴量を更に分割すると、より複雑な区間になる可能性があります。 カテゴリカル特徴量の場合、部分集合は特徴量の特定のカテゴリが含まれることになります。 自転車レンタルのデータセットの例を見てみましょう。 \\[r_{17}(x)=I(x_{\\text{temp}}&lt;15)\\cdot{}I(x_{\\text{weather}}\\in\\{\\text{good},\\text{cloudy}\\})\\cdot{}I(10\\leq{}x_{\\text{windspeed}}&lt;20)\\] この規則は、3つの条件全てが満たされた場合に 1、それ以外は 0 を返します。 RuleFit は、葉だけではなく、木の全てのノードから規則を抽出します。 したがって、作成されるであろう規則は次のようになります。 \\[r_{18}(x)=I(x_{\\text{temp}}&lt;15)\\cdot{}I(x_{\\text{weather}}\\in\\{\\text{good},\\text{cloudy}\\}\\] 全体として、\\(t_m\\) 個の葉をもつ M 個の木のアンサンブルから作成される規則の数は次式で与えられます。 \\[K=\\sum_{m=1}^M2(t_m-1)\\] RuleFit の著者によって導入されたトリックは、ランダムな深さの木を学習することで、長さの異なる多種多様な規則を生成するというものです。 各ノードにおける予測値は破棄して、そのノードに至る条件のみを保持し、そこから規則を作るということに注意してください。 決定規則の重みづけは、RuleFit の第2ステップで行われます。 ステップ1はこのように見ることもできます。 RuleFit は、元の特徴量から新しい特徴量の集合を生成します。 これらの特徴量は、二値であり、元の特徴量の極めて複雑な相互作用を表現できます。 規則は予測タスクで最良の結果が得られるように選択されます。 規則は、共変量行列Xから自動的に生成されます。 規則は元の特徴量に基づく新たな特徴量としてみなすことができます。 Step 2: スパース線形モデル ステップ1で、多くの規則を得ることができます。 この最初のステップは、単なる特徴量の変換にすぎないため、モデルへの適合はまだ終わっていません。 また、規則の数を減らしたいとも思うでしょう。 これらの規則に加えて、元のデータセットの全ての&quot;生&quot;の特徴量も、スパース線形モデルで利用することになります。 全ての規則と元の特徴量が線形モデルの特徴量となり、重みが推定値されます。 元の生の特徴量を追加するのは、木は y と x の間の単純な線形関係を表現するのに失敗するためです。 スパース線形モデルを学習する前に、元の特徴量の外れ値をクリッピング (winsorizing) し、外れ値に対してより頑健になるようにします。 \\[l_j^*(x_j)=min(\\delta_j^+,max(\\delta_j^-,x_j))\\] ここで、\\(\\delta_j^-\\) と \\(\\delta_j^+\\) は、特徴量 \\(x_j\\) のデータ分布の \\(\\delta\\) 分位数です。 \\(\\delta\\) に0.05を選択すると、上位 5％ または下位 5％ の特徴量 \\(x_j\\) の値が、それぞれ 5％ または 95％ の分位数に設定されます。 経験則として、\\(\\delta\\) = 0.025 を選択できます。 さらに、線形項は、通常の決定規則と事前の重要性が同一となるように正規化する必要があります。 \\[l_j(x_j)=0.4\\cdot{}l^*_j(x_j)/std(l^*_j(x_j))\\] \\(0.4\\) は、\\(s_k\\sim{}U(0,1)\\) の一様なサポート分布を持つ規則の標準偏差の平均です。 両方のタイプの特徴量を組み合わせて、新たな特徴量行列を作成し、次の形式で Lasso を利用してスパース線形モデルを学習します。 \\[\\hat{f}(x)=\\hat{\\beta}_0+\\sum_{k=1}^K\\hat{\\alpha}_k{}r_k(x)+\\sum_{j=1}^p\\hat{\\beta}_j{}l_j(x_j)\\] ここで、\\(\\hat{\\alpha}\\) は、規則の特徴量に対して推定された重みベクトルであり、\\(\\hat{\\beta}\\) は、元の特徴量に対する重みベクトルです。 RuleFit は Lasso を利用するため、損失関数は、一部の重みを 0 にするための制約が必要になります。 \\[(\\{\\hat{\\alpha}\\}_1^K,\\{\\hat{\\beta}\\}_0^p)=argmin_{\\{\\hat{\\alpha}\\}_1^K,\\{\\hat{\\beta}\\}_0^p}\\sum_{i=1}^n{}L(y^{(i)},f(x^{(i)}))+\\lambda\\cdot\\left(\\sum_{k=1}^K|\\alpha_k|+\\sum_{j=1}^p|b_j|\\right)\\] この結果は、元の全ての特徴量と規則に対して線形な効果をもつ線形モデルです。 解釈は、線形モデルの場合と同様ですが、唯一の違いは、一部の特徴量が二値の規則となっている点です。 Step3（optional）: 特徴量重要度 元の特徴量の線形項については、標準化された予測器を利用して特徴量重要度を測定します。 \\[I_j=|\\hat{\\beta}_j|\\cdot{}std(l_j(x_j))\\] ここで、\\(\\beta_j\\) は、Lasso モデルから得られた重みであり、\\(std(l_j(x_j))\\) はデータ全体の線形項の標準偏差です。 決定規則の項の場合、重要度は次式で計算されます。 \\[I_k=|\\hat{\\alpha}_k|\\cdot\\sqrt{s_k(1-s_k)}\\] ここで、\\(\\hat{\\alpha}_k\\) は、決定規則の関連するLassoの重みであり、\\(s_k\\) は、データにおける特徴量のサポートであり、決定規則が適用されるデータの割合です（ここで、\\(r_k(x)=1\\) ）。 \\[s_k=\\frac{1}{n}\\sum_{i=1}^n{}r_k(x^{(i)})\\] 特徴量は線形項として現れるだけでなく、場合によっては多くの決定規則の内部にも現れます。 どのように特徴量の重要度を測るべきでしょうか？ 特徴量の重要度 \\(J_j(x)\\) は、個々の予測ごとに測定できます。 \\[J_j(x)=I_j(x)+\\sum_{x_j\\in{}r_k}I_k(x)/m_k\\] ここで、\\(I_l\\) は線形項の重要度、\\(I_k\\) は \\(x_j\\) が現れる決定規則の重要度、\\(m_k\\) は規則 \\(r_k\\) を構成する特徴量の数です。 全ての事例から特徴量の重要度を足し合わせることで、大域的な重要度を得ることができます。 \\[J_j(X)=\\sum_{i=1}^n{}J_j(x^{(i)})\\] 事例の部分集合を選択して、そのグループの特徴量重要度を計算できます。 4.6.3 長所 RuleFitは特徴量間の相互作用を線形モデルに自動で追加します。 したがって、相互作用項を手動で追加する必要のある線形モデルの問題を解決し、非線形関係をモデリングする問題にも少し役立ちます。 RuleFit は分類問題と回帰問題の両方を扱えます。 作成される決定規則は二値であるため、規則が観測データに適用されるかどうかを調べることで簡単に解釈できます。 優れた解釈可能性は、決定規則内の条件の数が多すぎない場合にのみ保証されます。 個人的には、1〜3 個の条件の決定規則が合理的だと思います。 つまり、アンサンブルの木の最大の深さは 3 が良いということです。 たとえモデルに多くの決定規則がある場合でも、それらがすべての観測データに適用されるわけではありません。 個々の観測データにはほんのひと握りの決定規則のみ（= 非ゼロの重みを持つ）が適用されます。 これにより、個々のデータに対する解釈可能性が向上します。 RuleFitは便利な診断ツールを多数提供しています。 これらのツールはモデルに依存しないため、この本のモデル非依存 (model-agnostic) のセクションで紹介されています：特徴量重要度、partial dependence plots、特徴量の相互作用。 4.6.4 短所 RuleFit は、Lasso モデルにおいて非ゼロな重みを得るたくさんの規則を作り出すことがあります。 解釈性は特徴量の数が増えるにつれ低下します。 有望な解決策としては特徴量の影響を単調にすることです。 つまり、特徴量が増加すると、予測結果も増加する必要があるということです。 論文では度々 RuleFit の性能が、ランダムフォレストの予測性能に匹敵するほど良いと主張しています。 しかしながら、私が個人的に試したいくつかの場合において、がっかりするような性能でした。 まず、適用してみてどのような性能が出るかを確認しましょう。 RuleFit の手順をふんで得られる最終生成物は、追加の特徴（決定規則）を持つ線形モデルです。 しかし、線形モデルであるからこそ、重みの解釈が直感的ではありません。 通常の線形回帰モデルと同様に、&quot;...他の全ての特徴量が固定されている場合に限る。&quot;という&quot;脚注&quot;がついています。 また、規則が重複していると少し厄介になります。 例えば、自転車の数を予測するための1つの決定規則（特徴量）として &quot;temp &gt; 10&quot; と &quot;temp &gt; 15 &amp; weather='GOOD'&quot; があるとします。 天気が良く、気温が15度以上であれば、自動的に気温が10度以上になります。 2つ目の規則が満たされているときに、1つ目の規則も満たされています。 2つ目の規則における推測された重みの解釈は&quot;他の特徴量が固定され、天気が良く、気温が15度以上のとき、予測される自転車の数は \\(\\beta_2\\) 増加する。&quot;となります。 しかしここで、&quot;他の特徴量が固定された場合&quot;というのが問題になってきます。 なぜなら、2つ目の規則が適合しているとき、1つ目の規則にも適合し、解釈が意味の無いものになってしまうからです。 4.6.5 ソフトウェアと代替手法 RuleFit アルゴリズムは R では Fokkema と Christoffersen (2017)25 によって実装されています。 Python 実装は Github 上にもあります。 非常によく似たフレームワークは skope-rules という Python のモジュールでアンサンブルから規則を抽出します。 これは最終的な規則を学習する方法が違います。 まず、skope-rules はパフォーマンスのよくない規則を、recall（再現性）とprecision（適合率）に基づいて除去します。 そして、重複あるいは似ている規則が、論理項（変数 + 大なり／小なり）の多様性や F1-score に基づいて除去します。 最後に Lasso を用いる代わりに、out-of-bag の F1-score や規則を構成する論理項を用います。 Friedman, Jerome H, and Bogdan E Popescu. &quot;Predictive learning via rule ensembles.&quot; The Annals of Applied Statistics. JSTOR, 916–54. (2008).↩ Fokkema, Marjolein, and Benjamin Christoffersen. &quot;Pre: Prediction rule ensembles&quot;. https://CRAN.R-project.org/package=pre (2017).↩ "],["other-interpretable.html", "4.7 その他の解釈可能なモデル", " 4.7 その他の解釈可能なモデル 解釈可能なモデルの種類は増加し続けており、どれぐらいの数があるかわかりません。 線形回帰や決定木、単純ベイズ分類器のようなシンプルなモデルもあれば、解釈性の低いモデルを組み合わせたり変更することでより解釈性を高めたような複雑なものもあります。 特に後者のタイプのモデルは現在、高頻度で開発・発表されており、それらについていくのは大変です。 この章では単純ベイズ分類器とk近傍法について軽く紹介します。 4.7.1 単純ベイズ分類器 (Naive Bayes Classifier) 単純ベイズ分類器は条件付き確率のベイズの定理を用います。 特徴量ごとに、クラスに属する確率を特徴量の値に基づいて計算します。 単純ベイズ分類器は各特徴量が互いに独立しているという強い(=単純な)仮定を置いていることになります。 単純ベイズは条件付き確率モデルであり、クラス \\(C_k\\) の予測確率を次のようにモデル化します。 \\[P(C_k|x)=\\frac{1}{Z}P(C_k)\\prod_{i=1}^n{}P(x_i|C_k)\\] Z はすべてのクラスの確率の合計が 1 になるようにするための規格化定数です(そうしなければ確率として扱えなくなります)。 クラスの条件付き確率は、クラスの確率とクラスが与えられたときのそれぞれの特徴量の確率の積を Z で正規化したものです。 この式はベイズの定理を用いて導出できます。 単純ベイズ分類器は特徴量同士の独立性を仮定しているため解釈可能なモデルであり、モジュールレベルで解釈が可能です。 条件付き確率を用いているため、各特徴量がクラスの分類にどれぐらい寄与しているかが非常に明確です。 4.7.2 k近傍法 k近傍法はデータ点の近傍を推論に使用する回帰や分類の手法です。 分類の場合、k近傍法はインスタンスの近傍の中で最も多くのものが属するクラスに割り当て、回帰では近傍の出力の平均をとります。 正しい k の値を見つけたり、近傍を定義するために使用される距離の算出方法を決定するには、工夫が必要です。 k近傍法は観測データに基づく学習アルゴリズムであるため、この本で紹介されている他の解釈可能なモデルとは異なるモデルと言えます。 k近傍法はどのようにすれば解釈できるのでしょうか。 まず、k近傍法には学習すべきパラメータが存在しないため、モジュールレベルでの解釈はできません。 さらに、k近傍法は局所的なモデルであり、明確に学習すべき大域的なパラメータや構造が存在しないため、大域的なモデルの解釈は困難です。 それでは、局所的には解釈が可能でしょうか。 推論について説明するには、使用された k 個の近傍を見つける必要があります。 モデルが解釈可能かどうかは、単純に、データセットの中の単一のインスタンスを解釈できるかどうかによります。 インスタンスが数百、数千の特徴量を持つ場合、それは解釈できないでしょう。しかし、少数の特徴量しかない、あるいはインスタンスの特徴量をいくつかの重要なもののみに削減できるのであれば、k近傍法は良い説明を与えることができます。 "],["agnostic.html", "Chapter 5 モデル非依存(Model-Agnostic)な手法", " Chapter 5 モデル非依存(Model-Agnostic)な手法 機械学習モデルから説明性を分離すること（=モデル非依存な解釈手法）には、いくつかの利点があります (Ribeiro, Singh, and Guestrin 201626)。 モデル固有の解釈手法と比べて、モデルに非依存な解釈手法の大きな利点は柔軟性があることです。 解釈手法がどのようなモデルにも適用できるならば、機械学習の開発者は好きな機械学習モデルに対して思いのまま使うことができます。 また、可視化の結果やユーザーインタフェースのような機械学習モデルの解釈を基に構築されるものは、根底にある機械学習モデルから独立したものになります。 通常、ある課題を解決するために機械学習モデルは1種類ではなく、多くの種類のモデルを評価します。 そして解釈性という観点でモデルを比較する際には、どのような種類のモデルに対しても同じ手法を用いることができるため、モデル非依存な手法を用いると簡単になります。 モデル非依存な解釈手法の代わりに解釈性のあるモデルのみを使用する方法がありますが、その場合は他の機械学習モデルと比較して予測性能が低い傾向があるという大きな欠点があり、使用するモデルが1種類に限定されることになります。 他の代替案は、モデル固有の解釈方法をつかうことです。 これの欠点は、やはり1種類のモデルに制限されてしまうことと、後から他のモデルに切り替えることが難しくなることです。 モデル非依存な解釈手法には、次のような性質が望まれます (Ribeiro, Singh, and Guestrin 2016)。 モデルの柔軟性 (Model flexibility) モデルの解釈手法がランダムフォレストやディープニューラルネットワークといったあらゆる機械学習モデルに対して使用できること。 説明の柔軟性 (Explanation flexibility) モデルの説明が特定の形式に制限されることがないこと。線形の関係を持つことが役に立つかもしれませんし、特徴量の重要度を可視化することが有用な場合もあるでしょう。 表現の柔軟性 (Representation flexibility) 説明のシステムは、説明対象のモデルとは異なる特徴量を使用できるべきです。抽象的な単語埋め込みベクトルを使用したテキスト分類に対しては、個々の単語を用いて説明することが好ましいかもしれません。 全体像 モデル非依存な解釈性を高い視点で見てみましょう。 私たちはデータを集めることで現実世界を捉え、（課題のために）機械学習モデルを用いてデータを予測するために学習し、さらに抽象化します。 解釈性は、人間の理解の助けとなる頂上のもう1つの層です。 FIGURE 5.1: 説明可能な機械学習の全体像。説明が人間に届く前に、現実世界はいくつかの層を通過します。 最も下に位置する層は現実世界です。 これは文字通り人体の生物学や薬に対する反応といった自然そのものを指すこともありますが、不動産市場といったより抽象的なことを指すこともあります。 現実世界の層には観察対象や興味の対象となりうるものすべてが含まれます。 最終的に私たちは現実世界に関する何かを学び、そして世界と対話することを目標としています。 2番目に位置する層はデータの層です。 コンピュータで情報を処理し保存するためには、現実世界はデジタル情報へと変換される必要があります。 データ層には画像やテキスト、表データなどが含まれています。 機械学習モデルをデータ層から学習させることで、ブラックボックスモデルの層が生まれます。 機械学習アルゴリズムは現実世界のデータから学習し、予測を出力したり何らかの構造を見つけ出したりします。 ブラックボックスモデルの層の上には解釈手法の層があり、機械学習モデルの不透明性に対処する助けとなります。 ある診断結果において最も重要な特徴量は何でしょうか。 なぜ金融取引が詐欺と分類されたのでしょうか。 最後の層は人間によって構成されています。 見てください！ここに描かれた人物はあなたに手を振っています！ あなたがこの本を読むことがブラックボックスモデルに対してより良い説明を与える手助けとなっているからです！ この多層から成る抽象化は統計学者と機械学習を実践している人のアプローチの違いを理解することにも役立ちます。 統計学者は臨床試験の計画や調査の設計などデータ層を扱っています。 彼らはブラックボックスモデルの層を飛ばし、直接解釈手法の層へと進みます。 機械学習のスペシャリストも同様にラベリングされた皮膚がんの画像を収集したり、ウィキペディアから情報を集めるなど、データ層を扱っています。 その後、彼らはブラックボックスな機械学習モデルを学習させます。 解釈手法の層は飛ばされ、人間が直接ブラックボックスモデルの予測と向き合います。 機械学習モデルを解釈可能とすることは統計学者と機械学習のスペシャリストの成果を融合させる素晴らしいことなのです。 もちろんこの図が全てを捉えているわけではありません。 データはシミュレーションから得られることもあります。 またブラックボックスモデルの出力する予測が他の機械へと与えられるのみで人間に届かないこともあります。 しかし全体として見るならば、この抽象化は解釈可能性が機械学習モデルの上に位置する新しい層となることを上手く表現しています。 Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Model-agnostic interpretability of machine learning.&quot; ICML Workshop on Human Interpretability in Machine Learning. (2016).↩ "],["pdp.html", "5.1 Partial Dependence Plot (PDP)", " 5.1 Partial Dependence Plot (PDP) Partial dependence plot (PDP, PD plot) は1つ、または2つの特徴量が機械学習モデルの予測結果に与える周辺効果 (marginal effect) を示します(J. H. Friedman 200127)。 Partial dependence plot は入力と出力の関係が線形か、単調か、より複雑かどうかを表現できます。 例えば、線形回帰モデルに適用した場合、partial dependence plot は常に線形の関係を示します。 回帰に対する Partial dependence 関数は以下のように定義されます。 \\[\\hat{f}_{x_S}(x_S)=E_{x_C}\\left[\\hat{f}(x_S,x_C)\\right]=\\int\\hat{f}(x_S,x_C)d\\mathbb{P}(x_C)\\] \\(x_S\\) は partial dependence 関数をプロットするべき特徴量で、\\(x_C\\) は機械学習モデル \\(\\hat{f}\\) のそのほかの特徴量を表します。 通常、集合 S の中には、1つか2つの特徴量が含まれます。 S の中の特徴量が、予測に与える効果を知りたい対象となります。 特徴ベクトル \\(x_S\\) と \\(x_C\\) を組み合わせて、特徴空間 x を構成します。 Partial dependence は、集合 C の中の特徴量の分布に対して機械学習モデルの出力を周辺化することで機能します。これによって、この関数は集合 S の中の関心のある特徴量と予測結果との関係を示すことができます。 他の特徴量に対して周辺化することによって、S の中の特徴量にのみ依存する関数を得ることができ、他の特徴量との相互作用も含まれます。 Partial function \\(\\hat{f}_{x_S}\\) は学習データの平均として計算されます。これはモンテカルロ法としても知られています。 \\[\\hat{f}_{x_S}(x_S)=\\frac{1}{n}\\sum_{i=1}^n\\hat{f}(x_S,x^{(i)}_{C})\\] Partial function は 特徴量 S の与えられた値に対して、予測に対する平均的な周辺効果 (average marginal effect) が何であるかを示しています。 この式中の、\\(x^{(i)}_{C}\\) は関心のない特徴量に対するデータセットからの実際の値であり、n はデータセットに含まれるインスタンスの数を表しています。 PDP の仮定は、C の中の特徴量は、S の中の特徴量と相関していないということです。もし、この仮定が成り立たなければ、PDP に対して計算された平均は、とても起こりそうにない、もしくは不可能なデータ点が含まれてしまいます。（短所を参照） 機械学習モデルが確率を出力する分類の場合、partial dependence plot は、S の特徴量に異なる値が与えられた特定のクラスの確率を表示します。 複数のクラスを扱うときの簡単な方法は、クラスごとに1本の線またはプロットを描くことです。 PDP はグローバルな方法です。 この方法は全てのインスタンスを考慮し、特徴量と予測結果のグローバルな関係についてのステートメントを提供します。 カテゴリカル特徴量 これまでは、数値の特徴量のみを想定していました。 カテゴリカル特徴量に対しては、 partial dependence はとても簡単に計算できます。 カテゴリのそれぞれに対して、全てのインスタンスを強制的に同じカテゴリとすることで、 PDP を計算できます。 例えば、自転車レンタルのデータセットに関して、季節に関する partial dependence plot に興味があるとすると、各季節に 1 つずつで、合計 4 つの数値が得られます。 “夏”の値を計算するためには、全てのデータの季節を”夏”に置き換えて、予測の平均を求めます。 5.1.1 例 実用上は、特徴量の集合 S は通常ただ1つ、もしくは多くても2つの特徴量のみを持たせます。なぜなら、1つの特徴量のとき2次元のプロットになり、2つの特徴量の場合は3次元のプロットになるからです。 2次元の紙やモニター上に3次元を描くことがすでに挑戦的なので、これ以上は、非常に扱いにくくなります。 ある日に借りられる自転車の数を予測する回帰の例に戻りましょう。 今回は、自転車の数を予測するためにランダムフォレストを用いて学習し、モデルが学習した関係性を可視化するために partial dependence plot を利用しました。 FIGURE 5.2: 自転車レンタル予測モデルの気温、湿度、風速に対するPDP。 気温で最も違いが見られ、暑くなればなるほど、自転車はレンタルされる。この傾向は20度まで上昇し、平坦になり、30度で少し減少する。x軸上のマークはデータの分布を示している。 暖かいが暑すぎない場合、モデルは平均して、多くの自転車がレンタルされると予測します。 湿度が 60% を超えると、自転車のレンタルは抑制されます。 それに加えて、風が吹けば吹くほど自転車に乗りたがる人は少なくなっていますが、これは理にかなっています。 興味深いことに、風速が 25km/h から 35km/h へ増加する間は、自転車の利用予測数は下降していません。しかし、これは十分な学習データが無いために、この範囲において機械学習モデルが意味のある予測を学習できなかったためかもしれません。 少なくとも直感的には、特に風速が非常に高い場合は、自転車の数が減ると考えられます。 カテゴリカル特徴量の partial dependence plot を例示するために、自転車レンタルにおける季節の特徴量の効果を調べます。 FIGURE 5.3: 季節に関する自転車レンタル予測モデルの PDP。 予想外にも全ての季節で同様の効果があることがわかった。ただし、春は自転車レンタル数のモデルの予測結果が小さかった。 Partial dependence を子宮頸がん分類についても計算してみます。 リスク要因に基づき女性が子宮頸がんにかかるか否かを予測するためにランダムフォレストを用いて学習しました。 ランダムフォレストでのがんにかかる確率と様々な特徴量との関係について、partial dependenceを計算し可視化します。 FIGURE 5.4: 年齢とホルモン避妊薬の使用年数に基づいたがんの確率のPDP。年齢に対して、40歳まで確率が低く、それ以降は確率が増加することをPDPは示している。ホルモン避妊薬の使用年数が増加すればするほど、特に、10年を境に、予測されたがんのリスクも高くなる。どちらの特徴量も大きな値の付近では十分な数のデータ点を使用することができなかったので、この付近における推定結果の信頼性は低いことに注意。 2つの特徴量について、一度に partial dependence を可視化できます。 FIGURE 5.5: 年齢と妊娠回数の相互作用とがんの確率のPDP。図は45のときにがんの確率が増加することを示している。25歳以下で妊娠回数が1または2回のときは、0回もしくは2回より多い場合と比較して予測されたがんのリスクは低かった。ただし、これは因果ではなく、単なる相関関係の可能性があるので、結論を出すときは注意。 5.1.2 長所 Partial dependence plot の計算は直感的です。 ある特徴量の値での partial dependence 関数は、全てのデータ点が特定の特徴量の値を持つと仮定した場合の予測の平均を表しています。 私の経験上、専門家ではない人たちも PDP のアイデアをすぐに理解できます。 もし、PDP を計算した特徴量が他の特徴量と相関していなかったのなら、PDP は完璧に、特徴量が平均的に予測にどのような影響を与えているかを表しています。 相関関係がない場合、説明は明快です。 Partial dependence plot は、j 番目の特徴量が変わった時に、あなたのデータセット内の予測値の平均がどう変化するのかを示します。 ただし、特徴量が相関している時、もっと複雑になります。詳しくは、短所の方をみてください。 Partial dependence plot は、実装が簡単です。 Partial dependence plot の計算には、因果関係の解釈があります。 特徴量に介入を行い、予測の変化を計算しています。 これは、特徴量と予測結果の因果関係を分析していることになります。28 出力を特徴量の関数として明示的にモデル化していることから、この関係性はモデルに関する因果関係であるが、必ずしも現実世界の因果関係ではないということに注意してください。 5.1.3 短所 Partial dependence 関数で確かめることができる現実的な最大特徴量の数 は2です。 これは PDP の問題ではなく、2次元の表現(紙やスクリーン)と我々が3次元以上をうまく想像できないことが原因です。 PDP の中には 特徴量の分布 を示さないものもあります。 分布を省くのは誤解を招くおそれがあります、なぜならほとんどデータがない部分を深読みしすぎてしまう可能性があるからです。 この問題は、ラグ（x軸上のデータ点を示す）またはヒストグラムを表示することで簡単に解決できます。 独立性の仮定が PDP の最大の問題です。 Partial dependence が計算される特徴量が他の特徴量と相関していないと仮定します。 例えば、体重と身長が与えられ、ある人が歩く速さを予測したいとしましょう。 その中の1つの特徴量 (身長) の partial dependence を調べるために、もう1つの特徴量(体重)が身長と相関がないと仮定しますが、それは明らかに間違った仮定です。 特定の身長 (例: 200cm) の PDP の計算のために、体重の周辺分布の平均を計算しますが、このとき 50kg 以下のデータも含まれます。これは2メートルの人にとっては現実的ではありません。 言い換えると、特徴量同士が相関していると、現実的にとても低い確率でしか起こらないような新しいデータ点をつくってしまうことになります (例えば、2メートルの身長で 50kg 以下である可能性は低いです)。 この問題に対する1つの解決策は Accumulated Local Effect plots や short ALE plots で、これらは周辺分布の代わりに条件付き分布を使用します。 PDP は平均的な周辺効果のみを示すので 不均一な影響が隠れてしまう可能性があります。 半分のデータ点が予測と正の相関 -- 特徴量が大くなるほど予測結果も大きくなる -- 、もう半分のデータ点が負の相関 -- 特徴量が小さくなるほど予測結果は大きくなる -- を持つ特徴量を考えてみましょう。 このとき、PDP の曲線は水平になるでしょう、なぜなら両方のデータセットが互いに影響を打ち消し合うからです。 そうすると、その特徴量は予測には影響を与えないと結論づけてしまうでしょう。 これに対する解決策として、集計された線の代わりに、individual conditional expectation curves をプロットすることで、不均一な影響も明らかにできます。 5.1.4 ソフトウェアと代替手法 PDP を実装した R のパッケージはたくさんあります。 例えば著者は iml パッケージを使っていますが、pdp や DALEX もあります。 Python では、partial dependence plots は scikit-learn に標準で実装されていますし PDPBox も使えます。 この本で紹介されている PDP の代替手法には ALE plots や ICE curves があります。 Friedman, Jerome H. &quot;Greedy function approximation: A gradient boosting machine.&quot; Annals of statistics (2001): 1189-1232.↩ Zhao, Qingyuan, and Trevor Hastie. &quot;Causal interpretations of black-box models.&quot; Journal of Business &amp; Economic Statistics, to appear. (2017).↩ "],["ice.html", "5.2 Individual Conditional Expectation (ICE)", " 5.2 Individual Conditional Expectation (ICE) Individual Conditional Expectation (ICE) plots は、ある特徴量が変化したときにそのインスタンスの予測がどのように変化するかを1本の線で可視化する手法です。 特徴量の平均的な効果に関する partial dependence plot は、特定のインスタンスではなく、全体的な平均に注目しているため、大域的な方法と言えます。 個々のインスタンスに対する PDP と等価な手法は、individual conditional expectation (ICE) plot (Goldstein et al. 201729) と呼ばれています。 ICE plot はインスタンスごとの、ある特徴量が予測に与える影響を別々に可視化します。 partial dependence plotでは、全体に対して1本の線で表現していましたが、ICE plot では、1つのインスタンスにつき1本の線で表現されます。 PDP は ICE plot の線を平均したものと一致します。 ある線(とそれに対応するインスタンス)における値は、他の全ての特徴量を一定に保ったまま、ある特徴量の値をグリッド上の 別の値に置き換えて、いくつかの新しいインスタンスを作成し、それらに対してブラックボックスモデルで予測をすることで計算されます。 その結果は、グリッド上の特徴量の値と、それぞれの予測値を持つインスタンスに対する点の集合です。 PDP の代わりに個々の予測を見るポイントは何でしょうか。 PDP は相互作用によって生まれる不均一な関係を見えなくしてしまうことがあります。 PDP は特徴量と予測が平均的にどんな関係にあるかを示していますが、これは、対象の特徴量と他の特徴量との相互作用が弱い場合にのみ有効です。 相互作用がある場合、ICE plot はより多くの洞察を与えてくれるでしょう。 より正式な定義は次のとおりです。 ICE plot では、\\(\\{(x_{S}^{(i)},x_{C}^{(i)})\\}_{i=1}^N\\) 内のそれぞれのインスタンスにおいて、曲線 \\(\\hat{f}_S^{(i)}\\) は \\(x^{(i)}_{S}\\) に対して、\\(x^{(i)}_{C}\\) が固定されたままプロットされます。 5.2.1 例 子宮頸がんのデータセット を使って、それぞれのインスタンスで「年齢」の特徴量がどれだけ影響を与えているかを見てみましょう. リスクのある要因が与えられたとき女性ががんになる確率を予測するランダムフォレストを分析してみます。 Partial dependence plot では、50歳周辺でがんの確率が増加しているのが見受けられますが、データセット内のすべての女性に対して当てはまるのでしょうか。 ICE plot は、ほとんどの女性にとって、年齢的な影響は50歳で確率が増加するという平均的なパターンに従っているが、例外もあるということを明らかにしています。 若いときから高確率を予測されている女性は、予測されるがん確率は年齢によってあまり変わりません。 FIGURE 5.6: 年齢ごとの子宮頸がんの確率のICEプロット。それぞれの線は一人の女性を表す。ほとんどの女性は、年齢の増加に伴って、がんと予測される確率が増加する。予測の確率が 0.4 を超える女性に対しては、年齢が高くなっても予測はあまり変化しない。 次の図は、自転車レンタル予測 に対する ICE プロットです。 ここでも使用されている予測モデルはランダムフォレストです。 FIGURE 5.7: 天候ごとの自転車レンタル予測の ICE プロット。PDP のときと同様の効果が見られる。 全ての曲線は同じコースを辿っているように見えるので、明らかな相互作用はないと言えます。 つまり、PDP は表示された特徴量と予測された自転車の数との関係の優れた要約となっていると言えます。 5.2.1.1 Centered ICE Plot ICE プロットには問題があります。 ICE 曲線は異なる予測から始まるため、個々の間で ICE 曲線が異なるかどうかを判断するのが難しい場合があります。 簡単な解決策は、特徴量の特定の点で曲線を中央に配置し、この点との予測の差のみを表示することです。 結果のプロットは、centered ICEプロット（c-ICE）と呼ばれています。 特徴量の下端にカーブを固定することをお勧めします。 新しい曲線は次のように定義されます。 \\[\\hat{f}_{cent}^{(i)}=\\hat{f}^{(i)}-\\mathbf{1}\\hat{f}(x^{a},x^{(i)}_{C})\\] ただし、\\(\\mathbf{1}\\) は適切な数(普通、1 か 2)だけ 1 が並んだベクトルであり、\\(\\hat{f}\\) は学習されたモデルで、xa はアンカーポイントです。 5.2.1.2 例 例えば、年齢に対して子宮頸がんの ICE プロットを作成し、観測された最も若い年齢を中心に線を引いてみましょう。 FIGURE 5.8: 年齢ごとに予測されたがんの確率に対する centered ICE プロット。線は年齢 14 が 0 に固定されている。年齢 14 に比べ、ほとんどの女性の予測は、予測確率が増加する45歳まで変化しない。 Centered ICE プロットでは、個々のインスタンスの曲線の比較を簡単にできます。 これは、予測値の絶対的な変化ではなく、特徴量の範囲の固定点と比較した予測の差を確認したい場合に役立ちます。 自転車レンタル数予測の例で、centered ICE プロットをみてみましょう。 FIGURE 5.9: 天候による予測された自転車レンタル数の centered ICE プロット。 線は、観測された特徴量の最小値での予測との差を示している。 5.2.1.3 Derivative ICE Plot 不均一性を簡単に視覚化するための別の方法は、特徴量に関して、予測関数の個々の微分を見ることです。 結果のプロットは derivative ICE plot (d-ICE)と呼ばれています。 関数の微分(または、曲線)は、変化が起きたのか、また、どの方向に起きたのかを教えてくれます。 Derivative ICE plot を用いると、（少なくとも一部の）インスタンスでブラックボックスの予測が変化する特徴値の範囲を簡単に見つけることができます。 もし、注目している特徴量 \\(x_S\\) と他の特徴量 \\(x_C\\) の間に相互作用がないのであれば、予測関数は以下のように表現できます。 \\[\\hat{f}(x)=\\hat{f}(x_S,x_C)=g(x_S)+h(x_C),\\quad\\text{with}\\quad\\frac{\\delta\\hat{f}(x)}{\\delta{}x_S}=g&#39;(x_S)\\] 相互作用がないとき、個々の偏微分は全てのインスタンスで同じである必要があります。もし、これらが異なる場合は相互作用が原因であり、d-ICE plot を用いて可視化できます。微分の標準偏差を示すことは、推定された微分に不均一性がある S の特徴量の領域を強調するのに役立ちます。 ただし、derivative ICE plot は計算に長い時間がかかるため、現実的ではないかもしれません。 5.2.2 長所 ICE 曲線は partial dependence plot よりも直感的に理解可能です。 1つの線は、1つのインスタンスに対して、対象の特徴量を変化させたときの予測を表します。 Partial dependence plot とは異なり、ICE 曲線は不均一な関係性を明らかにできます。 5.2.3 短所 ICE 曲線は1つの特徴量のみを意味のある形で表示できます。2つの特徴量を使うと、いくつかの重複した面を描画する必要があるため、このプロットをみても何も理解できないでしょう。 ICE 曲線は、PDP と同様の問題に直面します。 興味のある特徴量が、その他の特徴量と相関している場合、同時分布によって、線の中のいくつかの点は妥当でないデータ点となる可能性があるということです。 多くの ICE 曲線が描かれたとき、プロットは激しく重なり合い、何も発見できません。 解決方法は、線に透明度を追加するか、線のうちのいくつかのみを描画することです。 ICE 曲線の中で、平均をみることは簡単ではないかもしれません。 これに対する単純な解決方法は、ICE 曲線と PDP を組み合わせることです。 5.2.4 ソフトウェアと代替手法 ICE plots は、iml（これらの例で使用）、ICEbox [^ ICEbox]、および pdp の R パッケージで実装されています。 ICE にとても類似しているもう1つのRパッケージは condvisです。 Goldstein, Alex, et al. &quot;Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation.&quot; Journal of Computational and Graphical Statistics 24.1 (2015): 44-65.↩ "],["ale.html", "5.3 Accumulated Local Effects (ALE) Plot", " 5.3 Accumulated Local Effects (ALE) Plot Accumulated local effects30 describe how features influence the prediction of a machine learning model on average. ALE plots are a faster and unbiased alternative to partial dependence plots (PDPs). I recommend reading the chapter on partial dependence plots first, as they are easier to understand and both methods share the same goal: Both describe how a feature affects the prediction on average. In the following section, I want to convince you that partial dependence plots have a serious problem when the features are correlated. 5.3.1 Motivation and Intuition If features of a machine learning model are correlated, the partial dependence plot cannot be trusted. The computation of a partial dependence plot for a feature that is strongly correlated with other features involves averaging predictions of artificial data instances that are unlikely in reality. This can greatly bias the estimated feature effect. Imagine calculating partial dependence plots for a machine learning model that predicts the value of a house depending on the number of rooms and the size of the living area. We are interested in the effect of the living area on the predicted value. As a reminder, the recipe for partial dependence plots is: 1) Select feature. 2) Define grid. 3) Per grid value: a) Replace feature with grid value and b) average predictions. 4) Draw curve. For the calculation of the first grid value of the PDP -- say 30 m2 -- we replace the living area for all instances by 30 m2, even for houses with 10 rooms. Sounds to me like a very unusual house. The partial dependence plot includes these unrealistic houses in the feature effect estimation and pretends that everything is fine. The following figure illustrates two correlated features and how it comes that the partial dependence plot method averages predictions of unlikely instances. FIGURE 5.10: Strongly correlated features x1 and x2. To calculate the feature effect of x1 at 0.75, the PDP replaces x1 of all instances with 0.75, falsely assuming that the distribution of x2 at x1 = 0.75 is the same as the marginal distribution of x2 (vertical line). This results in unlikely combinations of x1 and x2 (e.g. x2=0.2 at x1=0.75), which the PDP uses for the calculation of the average effect. What can we do to get a feature effect estimate that respects the correlation of the features? We could average over the conditional distribution of the feature, meaning at a grid value of x1, we average the predictions of instances with a similar x1 value. The solution for calculating feature effects using the conditional distribution is called Marginal Plots, or M-Plots (confusing name, since they are based on the conditional, not the marginal distribution). Wait, did I not promise you to talk about ALE plots? M-Plots are not the solution we are looking for. Why do M-Plots not solve our problem? If we average the predictions of all houses of about 30 m2, we estimate the combined effect of living area and of number of rooms, because of their correlation. Suppose that the living area has no effect on the predicted value of a house, only the number of rooms has. The M-Plot would still show that the size of the living area increases the predicted value, since the number of rooms increases with the living area. The following plot shows for two correlated features how M-Plots work. FIGURE 5.11: Strongly correlated features x1 and x2. M-Plots average over the conditional distribution. Here the conditional distribution of x2 at x1 = 0.75. Averaging the local predictions leads to mixing the effects of both features. M-Plots avoid averaging predictions of unlikely data instances, but they mix the effect of a feature with the effects of all correlated features. ALE plots solve this problem by calculating -- also based on the conditional distribution of the features -- differences in predictions instead of averages. For the effect of living area at 30 m2, the ALE method uses all houses with about 30 m2, gets the model predictions pretending these houses were 31 m2 minus the prediction pretending they were 29 m2. This gives us the pure effect of the living area and is not mixing the effect with the effects of correlated features. The use of differences blocks the effect of other features. The following graphic provides intuition how ALE plots are calculated. FIGURE 5.12: Calculation of ALE for feature x1, which is correlated with x2. First, we divide the feature into intervals (vertical lines). For the data instances (points) in an interval, we calculate the difference in the prediction when we replace the feature with the upper and lower limit of the interval (horizontal lines). These differences are later accumulated and centered, resulting in the ALE curve. To summarize how each type of plot (PDP, M, ALE) calculates the effect of a feature at a certain grid value v: Partial Dependence Plots: &quot;Let me show you what the model predicts on average when each data instance has the value v for that feature. I ignore whether the value v makes sense for all data instances.&quot; M-Plots: &quot;Let me show you what the model predicts on average for data instances that have values close to v for that feature. The effect could be due to that feature, but also due to correlated features.&quot; ALE plots: &quot;Let me show you how the model predictions change in a small &quot;window&quot; of the feature around v for data instances in that window.&quot; 5.3.2 Theory How do PD, M and ALE plots differ mathematically? Common to all three methods is that they reduce the complex prediction function f to a function that depends on only one (or two) features. All three methods reduce the function by averaging the effects of the other features, but they differ in whether averages of predictions or of differences in predictions are calculated and whether averaging is done over the marginal or conditional distribution. Partial dependence plots average the predictions over the marginal distribution. \\[\\begin{align*}\\hat{f}_{x_S,PDP}(x_S)&amp;=E_{X_C}\\left[\\hat{f}(x_S,X_C)\\right]\\\\&amp;=\\int_{x_C}\\hat{f}(x_S,x_C)\\mathbb{P}(x_C)d{}x_C\\end{align*}\\] This is the value of the prediction function f, at feature value(s) \\(x_S\\), averaged over all features in \\(x_C\\). Averaging means calculating the marginal expectation E over the features in set C, which is the integral over the predictions weighted by the probability distribution. Sounds fancy, but to calculate the expected value over the marginal distribution, we simply take all our data instances, force them to have a certain grid value for the features in set S, and average the predictions for this manipulated dataset. This procedure ensures that we average over the marginal distribution of the features. M-plots average the predictions over the conditional distribution. \\[\\begin{align*}\\hat{f}_{x_S,M}(x_S)&amp;=E_{X_C|X_S}\\left[\\hat{f}(X_S,X_C)|X_S=x_s\\right]\\\\&amp;=\\int_{x_C}\\hat{f}(x_S,x_C)\\mathbb{P}(x_C|x_S)d{}x_C\\end{align*}\\] The only thing that changes compared to PDPs is that we average the predictions conditional on each grid value of the feature of interest, instead of assuming the marginal distribution at each grid value. In practice, this means that we have to define a neighborhood, for example for the calculation of the effect of 30 m2 on the predicted house value, we could average the predictions of all houses between 28 and 32 m2. ALE plots average the changes in the predictions and accumulate them over the grid (more on the calculation later). \\[\\begin{align*}\\hat{f}_{x_S,ALE}(x_S)=&amp;\\int_{z_{0,1}}^{x_S}E_{X_C|X_S}\\left[\\hat{f}^S(X_s,X_c)|X_S=z_S\\right]dz_S-\\text{constant}\\\\=&amp;\\int_{z_{0,1}}^{x_S}\\int_{x_C}\\hat{f}^S(z_s,x_c)\\mathbb{P}(x_C|z_S)d{}x_C{}dz_S-\\text{constant}\\end{align*}\\] The formula reveals three differences to M-Plots. First, we average the changes of predictions, not the predictions itself. The change is defined as the gradient (but later, for the actual computation, replaced by the differences in the predictions over an interval). \\[\\hat{f}^S(x_s,x_c)=\\frac{\\delta\\hat{f}(x_S,x_C)}{\\delta{}x_S}\\] The second difference is the additional integral over z. We accumulate the local gradients over the range of features in set S, which gives us the effect of the feature on the prediction. For the actual computation, the z's are replaced by a grid of intervals over which we compute the changes in the prediction. Instead of directly averaging the predictions, the ALE method calculates the prediction differences conditional on features S and integrates the derivative over features S to estimate the effect. Well, that sounds stupid. Derivation and integration usually cancel each other out, like first subtracting, then adding the same number. Why does it make sense here? The derivative (or interval difference) isolates the effect of the feature of interest and blocks the effect of correlated features. The third difference of ALE plots to M-plots is that we subtract a constant from the results. This step centers the ALE plot so that the average effect over the data is zero. One problem remains: Not all models come with a gradient, for example random forests have no gradient. But as you will see, the actual computation works without gradients and uses intervals. Let us delve a little deeper into the estimation of ALE plots. 5.3.3 Estimation First I will describe how ALE plots are estimated for a single numerical feature, later for two numerical features and for a single categorical feature. To estimate local effects, we divide the feature into many intervals and compute the differences in the predictions. This procedure approximates the gradients and also works for models without gradients. First we estimate the uncentered effect: \\[\\hat{\\tilde{f}}_{j,ALE}(x)=\\sum_{k=1}^{k_j(x)}\\frac{1}{n_j(k)}\\sum_{i:x_{j}^{(i)}\\in{}N_j(k)}\\left[f(z_{k,j},x^{(i)}_{\\setminus{}j})-f(z_{k-1,j},x^{(i)}_{\\setminus{}j})\\right]\\] Let us break this formula down, starting from the right side. The name Accumulated Local Effects nicely reflects all the individual components of this formula. At its core, the ALE method calculates the differences in predictions, whereby we replace the feature of interest with grid values z. The difference in prediction is the Effect the feature has for an individual instance in a certain interval. The sum on the right adds up the effects of all instances within an interval which appears in the formula as neighborhood \\(N_j(k)\\). We divide this sum by the number of instances in this interval to obtain the average difference of the predictions for this interval. This average in the interval is covered by the term Local in the name ALE. The left sum symbol means that we accumulate the average effects across all intervals. The (uncentered) ALE of a feature value that lies, for example, in the third interval is the sum of the effects of the first, second and third intervals. The word Accumulated in ALE reflects this. This effect is centered so that the mean effect is zero. \\[\\hat{f}_{j,ALE}(x)=\\hat{\\tilde{f}}_{j,ALE}(x)-\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\tilde{f}}_{j,ALE}(x^{(i)}_{j})\\] The value of the ALE can be interpreted as the main effect of the feature at a certain value compared to the average prediction of the data. For example, an ALE estimate of -2 at \\(x_j=3\\) means that when the j-th feature has value 3, then the prediction is lower by 2 compared to the average prediction. The quantiles of the distribution of the feature are used as the grid that defines the intervals. Using the quantiles ensures that there is the same number of data instances in each of the intervals. Quantiles have the disadvantage that the intervals can have very different lengths. This can lead to some weird ALE plots if the feature of interest is very skewed, for example many low values and only a few very high values. ALE plots for the interaction of two features ALE plots can also show the interaction effect of two features. The calculation principles are the same as for a single feature, but we work with rectangular cells instead of intervals, because we have to accumulate the effects in two dimensions. In addition to adjusting for the overall mean effect, we also adjust for the main effects of both features. This means that ALE for two features estimate the second-order effect, which does not include the main effects of the features. In other words, ALE for two features only shows the additional interaction effect of the two features. I spare you the formulas for 2D ALE plots because they are long and unpleasant to read. If you are interested in the calculation, I refer you to the paper, formulas (13) -- (16). I will rely on visualizations to develop intuition about the second-order ALE calculation. FIGURE 5.13: Calculation of 2D-ALE. We place a grid over the two features. In each grid cell we calculate the 2nd-order differences for all instance within. We first replace values of x1 and x2 with the values from the cell corners. If a, b, c and d represent the &quot;corner&quot;-predictions of a manipulated instance (as labeled in the graphic), then the 2nd-order difference is (d - c) - (b - a). The mean 2nd-order difference in each cell is accumulated over the grid and centered. In the previous figure, many cells are empty due to the correlation. In the ALE plot this can be visualized with a grayed out or darkened box. Alternatively, you can replace the missing ALE estimate of an empty cell with the ALE estimate of the nearest non-empty cell. Since the ALE estimates for two features only show the second-order effect of the features, the interpretation requires special attention. The second-order effect is the additional interaction effect of the features after we have accounted for the main effects of the features. Suppose two features do not interact, but each has a linear effect on the predicted outcome. In the 1D ALE plot for each feature, we would see a straight line as the estimated ALE curve. But when we plot the 2D ALE estimates, they should be close to zero, because the second-order effect is only the additional effect of the interaction. ALE plots and PD plots differ in this regard: PDPs always show the total effect, ALE plots show the first- or second-order effect. These are design decisions that do not depend on the underlying math. You can subtract the lower-order effects in a partial dependence plot to get the pure main or second-order effects or, you can get an estimate of the total ALE plots by refraining from subtracting the lower-order effects. The accumulated local effects could also be calculated for arbitrarily higher orders (interactions of three or more features), but as argued in the PDP chapter, only up to two features makes sense, because higher interactions cannot be visualized or even interpreted meaningfully. ALE for categorical features The accumulated local effects method needs -- by definition -- the feature values to have an order, because the method accumulates effects in a certain direction. Categorical features do not have any natural order. To compute an ALE plot for a categorical feature we have to somehow create or find an order. The order of the categories influences the calculation and interpretation of the accumulated local effects. One solution is to order the categories according to their similarity based on the other features. The distance between two categories is the sum over the distances of each feature. The feature-wise distance compares either the cumulative distribution in both categories, also called Kolmogorov-Smirnov distance (for numerical features) or the relative frequency tables (for categorical features). Once we have the distances between all categories, we use multi-dimensional scaling to reduce the distance matrix to a one-dimensional distance measure. This gives us a similarity-based order of the categories. To make this a little bit clearer, here is one example: Let us assume we have the two categorical features &quot;season&quot; and &quot;weather&quot; and a numerical feature &quot;temperature&quot;. For the first categorical feature (season) we want to calculate the ALEs. The feature has the categories &quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;, &quot;winter&quot;. We start to calculate the distance between categories &quot;spring&quot; and &quot;summer&quot;. The distance is the sum of distances over the features temperature and weather. For the temperature, we take all instances with season &quot;spring&quot;, calculate the empirical cumulative distribution function and do the same for instances with season &quot;summer&quot; and measure their distance with the Kolmogorov-Smirnov statistic. For the weather feature we calculate for all &quot;spring&quot; instances the probabilities for each weather type, do the same for the &quot;summer&quot; instances and sum up the absolute distances in the probability distribution. If &quot;spring&quot; and &quot;summer&quot; have very different temperatures and weather, the total category-distance is large. We repeat the procedure with the other seasonal pairs and reduce the resulting distance matrix to a single dimension by multi-dimensional scaling. 5.3.4 Examples Let us see ALE plots in action. I have constructed a scenario in which partial dependence plots fail. The scenario consists of a prediction model and two strongly correlated features. The prediction model is mostly a linear regression model, but does something weird at a combination of the two features for which we have never observed instances. FIGURE 5.14: Two features and the predicted outcome. The model predicts the sum of the two features (shaded background), with the exception that if x1 is greater than 0.7 and x2 less than 0.3, the model always predicts 2. This area is far from the distribution of data (point cloud) and does not affect the performance of the model and also should not affect its interpretation. Is this a realistic, relevant scenario at all? When you train a model, the learning algorithm minimizes the loss for the existing training data instances. Weird stuff can happen outside the distribution of training data, because the model is not penalized for doing weird stuff in these areas. Leaving the data distribution is called extrapolation, which can also be used to fool machine learning models, described in the chapter on adversarial examples. See in our little example how the partial dependence plots behave compared to ALE plots. FIGURE 5.15: Comparison of the feature effects computed with PDP (upper row) and ALE (lower row). The PDP estimates are influenced by the odd behavior of the model outside the data distribution (steep jumps in the plots). The ALE plots correctly identify that the machine learning model has a linear relationship between features and prediction, ignoring areas without data. But is it not interesting to see that our model behaves oddly at x1 &gt; 0.7 and x2 &lt; 0.3? Well, yes and no. Since these are data instances that might be physically impossible or at least extremely unlikely, it is usually irrelevant to look into these instances. But if you suspect that your test distribution might be slightly different and some instances are actually in that range, then it would be interesting to include this area in the calculation of feature effects. But it has to be a conscious decision to include areas where we have not observed data yet and it should not be a side-effect of the method of choice like PDP. If you suspect that the model will later be used with differently distributed data, I recommend to use ALE plots and simulate the distribution of data you are expecting. Turning to a real dataset, let us predict the number of rented bikes based on weather and day and check if the ALE plots really work as well as promised. We train a regression tree to predict the number of rented bicycles on a given day and use ALE plots to analyze how temperature, relative humidity and wind speed influence the predictions. Let us look at what the ALE plots say: FIGURE 5.16: ALE plots for the bike prediction model by temperature, humidity and wind speed. The temperature has a strong effect on the prediction. The average prediction rises with increasing temperature, but falls again above 25 degrees Celsius. Humidity has a negative effect: When above 60%, the higher the relative humidity, the lower the prediction. The wind speed does not affect the predictions much. Let us look at the correlation between temperature, humidity and wind speed and all other features. Since the data also contains categorical features, we cannot only use the Pearson correlation coefficient, which only works if both features are numerical. Instead, I train a linear model to predict, for example, temperature based on one of the other features as input. Then I measure how much variance the other feature in the linear model explains and take the square root. If the other feature was numerical, then the result is equal to the absolute value of the standard Pearson correlation coefficient. But this model-based approach of &quot;variance-explained&quot; (also called ANOVA, which stands for ANalysis Of VAriance) works even if the other feature is categorical. The &quot;variance-explained&quot; measure lies always between 0 (no association) and 1 (temperature can be perfectly predicted from the other feature). We calculate the explained variance of temperature, humidity and wind speed with all the other features. The higher the explained variance (correlation), the more (potential) problems with PD plots. The following figure visualizes how strongly the weather features are correlated with other features. FIGURE 5.17: The strength of the correlation between temperature, humidity and wind speed with all features, measured as the amount of variance explained, when we train a linear model with e.g. temperature to predict and season as feature. For temperature we observe -- not surprisingly -- a high correlation with season and month. Humidity correlates with weather situation. This correlation analysis reveals that we may encounter problems with partial dependence plots, especially for the temperature feature. Well, see for yourself: FIGURE 5.18: PDPs for temperature, humidity and wind speed. Compared to the ALE plots, the PDPs show a smaller decrease in predicted number of bikes for high temperature or high humidity. The PDP uses all data instances to calculate the effect of high temperatures, even if they are, for example, instances with the season &quot;winter&quot;. The ALE plots are more reliable. Next, let us see ALE plots in action for a categorical feature. The month is a categorical feature for which we want to analyze the effect on the predicted number of bikes. Arguably, the months already have a certain order (January to December), but let us try to see what happens if we first reorder the categories by similarity and then compute the effects. The months are ordered by the similarity of days of each month based on the other features, such as temperature or whether it is a holiday. FIGURE 5.19: ALE plot for the categorical feature month. The months are ordered by their similarity to each other, based on the distributions of the other features by month. We observe that January, March and April, but especially December and November, have a lower effect on the predicted number of rented bikes compared to the other months. Since many of the features are related to weather, the order of the months strongly reflects how similar the weather is between the months. All colder months are on the left side (February to April) and the warmer months on the right side (October to August). Keep in mind that non-weather features have also been included in the similarity calculation, for example relative frequency of holidays has the same weight as the temperature for calculating the similarity between the months. Next, we consider the second-order effect of humidity and temperature on the predicted number of bikes. Remember that the second-order effect is the additional interaction effect of the two features and does not include the main effects. This means that, for example, you will not see the main effect that high humidity leads to a lower number of predicted bikes on average in the second-order ALE plot. FIGURE 5.20: ALE plot for the 2nd-order effect of humidity and temperature on the predicted number of rented bikes. Lighter shade indicates an above average and darker shade a below average prediction when the main effects are already taken into account. The plot reveals an interaction between temperature and humidity: Hot and humid weather increases the prediction. In cold and humid weather an additional negative effect on the number of predicted bikes is shown. Keep in mind that both main effects of humidity and temperature say that the predicted number of bikes decreases in very hot and humid weather. In hot and humid weather, the combined effect of temperature and humidity is therefore not the sum of the main effects, but larger than the sum. To emphasize the difference between the pure second-order effect (the 2D ALE plot you just saw) and the total effect, let us look at the partial dependence plot. The PDP shows the total effect, which combines the mean prediction, the two main effects and the second-order effect (the interaction). FIGURE 5.21: PDP of the total effect of temperature and humidity on the predicted number of bikes. The plot combines the main effect of each of the features and their interaction effect, as opposed to the 2D-ALE plot which only shows the interaction. If you are only interested in the interaction, you should look at the second-order effects, because the total effect mixes the main effects into the plot. But if you want to know the combined effect of the features, you should look at the total effect (which the PDP shows). For example, if you want to know the expected number of bikes at 30 degrees Celsius and 80 percent humidity, you can read it directly from the 2D PDP. If you want to read the same from the ALE plots, you need to look at three plots: The ALE plot for temperature, for humidity and for temperature + humidity and you also need to know the overall mean prediction. In a scenario where two features have no interaction, the total effect plot of the two features could be misleading because it probably shows a complex landscape, suggesting some interaction, but it is simply the product of the two main effects. The second-order effect would immediately show that there is no interaction. Enough bicycles for now, let's turn to a classification task. We train a random forest to predict the probability of cervical cancer based on risk factors. We visualize the accumulated local effects for two of the features: FIGURE 5.22: ALE plots for the effect of age and years with hormonal contraceptives on the predicted probability of cervical cancer. For the age feature, the ALE plot shows that the predicted cancer probability is low on average up to age 40 and increases after that. The number of years with hormonal contraceptives is associated with a higher predicted cancer risk after 8 years. Next, we look at the interaction between number of pregnancies and age. FIGURE 5.23: ALE plot of the 2nd-order effect of number of pregnancies and age. The interpretation of the plot is a bit inconclusive, showing what seems like overfitting. For example, the plot shows an odd model behavior at age of 18-20 and more than 3 pregnancies (up to 5 percentage point increase in cancer probability). There are not many women in the data with this constellation of age and number of pregnancies (actual data are displayed as points), so the model is not severely penalized during the training for making mistakes for those women. 5.3.5 Advantages ALE plots are unbiased, which means they still work when features are correlated. Partial dependence plots fail in this scenario because they marginalize over unlikely or even physically impossible combinations of feature values. ALE plots are faster to compute than PDPs and scale with O(n), since the largest possible number of intervals is the number of instances with one interval per instance. The PDP requires n times the number of grid points estimations. For 20 grid points, PDPs require 20 times more predictions than the worst case ALE plot where as many intervals as instances are used. The interpretation of ALE plots is clear: Conditional on a given value, the relative effect of changing the feature on the prediction can be read from the ALE plot. ALE plots are centered at zero. This makes their interpretation nice, because the value at each point of the ALE curve is the difference to the mean prediction. The 2D ALE plot only shows the interaction: If two features do not interact, the plot shows nothing. All in all, in most situations I would prefer ALE plots over PDPs, because features are usually correlated to some extent. 5.3.6 Disadvantages ALE plots can become a bit shaky (many small ups and downs) with a high number of intervals. In this case, reducing the number of intervals makes the estimates more stable, but also smoothes out and hides some of the true complexity of the prediction model. There is no perfect solution for setting the number of intervals. If the number is too small, the ALE plots might not be very accurate. If the number is too high, the curve can become shaky. Unlike PDPs, ALE plots are not accompanied by ICE curves. For PDPs, ICE curves are great because they can reveal heterogeneity in the feature effect, which means that the effect of a feature looks different for subsets of the data. For ALE plots you can only check per interval whether the effect is different between the instances, but each interval has different instances so it is not the same as ICE curves. Second-order ALE estimates have a varying stability across the feature space, which is not visualized in any way. The reason for this is that each estimation of a local effect in a cell uses a different number of data instances. As a result, all estimates have a different accuracy (but they are still the best possible estimates). The problem exists in a less severe version for main effect ALE plots. The number of instances is the same in all intervals, thanks to the use of quantiles as grid, but in some areas there will be many short intervals and the ALE curve will consist of many more estimates. But for long intervals, which can make up a big part of the entire curve, there are comparatively fewer instances. This happened in the cervical cancer prediction ALE plot for high age for example. Second-order effect plots can be a bit annoying to interpret, as you always have to keep the main effects in mind. It is tempting to read the heat maps as the total effect of the two features, but it is only the additional effect of the interaction. The pure second-order effect is interesting for discovering and exploring interactions, but for interpreting what the effect looks like, I think it makes more sense to integrate the main effects into the plot. The implementation of ALE plots is much more complex and less intuitive compared to partial dependence plots. Even though ALE plots are not biased in case of correlated features, interpretation remains difficult when features are strongly correlated. Because if they have a very strong correlation, it only makes sense to analyze the effect of changing both features together and not in isolation. This disadvantage is not specific to ALE plots, but a general problem of strongly correlated features. If the features are uncorrelated and computation time is not a problem, PDPs are slightly preferable because they are easier to understand and can be plotted along with ICE curves. The list of disadvantages has become quite long, but do not be fooled by the number of words I use: As a rule of thumb: Use ALE instead of PDP. 5.3.7 Implementation and Alternatives Did I mention that partial dependence plots and individual conditional expectation curves are an alternative? =) To the best of my knowledge, ALE plots are currently only implemented in R, once in the ALEPlot R package by the inventor himself and once in the iml package. Apley, Daniel W. &quot;Visualizing the effects of predictor variables in black box supervised learning models.&quot; arXiv preprint arXiv:1612.08468 (2016).↩ "],["interaction.html", "5.4 Feature Interaction", " 5.4 Feature Interaction When features interact with each other in a prediction model, the prediction cannot be expressed as the sum of the feature effects, because the effect of one feature depends on the value of the other feature. Aristotle's predicate &quot;The whole is greater than the sum of its parts&quot; applies in the presence of interactions. 5.4.1 Feature Interaction? If a machine learning model makes a prediction based on two features, we can decompose the prediction into four terms: a constant term, a term for the first feature, a term for the second feature and a term for the interaction between the two features. The interaction between two features is the change in the prediction that occurs by varying the features after considering the individual feature effects. For example, a model predicts the value of a house, using house size (big or small) and location (good or bad) as features, which yields four possible predictions: Location Size Prediction good big 300,000 good small 200,000 bad big 250,000 bad small 150,000 We decompose the model prediction into the following parts: A constant term (150,000), an effect for the size feature (+100,000 if big; +0 if small) and an effect for the location (+50,000 if good; +0 if bad). This decomposition fully explains the model predictions. There is no interaction effect, because the model prediction is a sum of the single feature effects for size and location. When you make a small house big, the prediction always increases by 100,000, regardless of location. Also, the difference in prediction between a good and a bad location is 50,000, regardless of size. Let's now look at an example with interaction: Location Size Prediction good big 400,000 good small 200,000 bad big 250,000 bad small 150,000 We decompose the prediction table into the following parts: A constant term (150,000), an effect for the size feature (+100,000 if big, +0 if small) and an effect for the location (+50,000 if good, +0 if bad). For this table we need an additional term for the interaction: +100,000 if the house is big and in a good location. This is an interaction between size and location, because in this case the difference in prediction between a big and a small house depends on the location. One way to estimate the interaction strength is to measure how much of the variation of the prediction depends on the interaction of the features. This measurement is called H-statistic, introduced by Friedman and Popescu (2008)31. 5.4.2 Theory: Friedman's H-statistic We are going to deal with two cases: First, a two-way interaction measure that tells us whether and to what extent two features in the model interact with each other; second, a total interaction measure that tells us whether and to what extent a feature interacts in the model with all the other features. In theory, arbitrary interactions between any number of features can be measured, but these two are the most interesting cases. If two features do not interact, we can decompose the partial dependence function as follows (assuming the partial dependence functions are centered at zero): \\[PD_{jk}(x_j,x_k)=PD_j(x_j)+PD_k(x_k)\\] where \\(PD_{jk}(x_j,x_k)\\) is the 2-way partial dependence function of both features and \\(PD_j(x_j)\\) and \\(PD_k(x_k)\\) the partial dependence functions of the single features. Likewise, if a feature has no interaction with any of the other features, we can express the prediction function \\(\\hat{f}(x)\\) as a sum of partial dependence functions, where the first summand depends only on j and the second on all other features except j: \\[\\hat{f}(x)=PD_j(x_j)+PD_{-j}(x_{-j})\\] where \\(PD_{-j}(x_{-j})\\) is the partial dependence function that depends on all features except the j-th feature. This decomposition expresses the partial dependence (or full prediction) function without interactions (between features j and k, or respectively j and all other features). In a next step, we measure the difference between the observed partial dependence function and the decomposed one without interactions. We calculate the variance of the output of the partial dependence (to measure the interaction between two features) or of the entire function (to measure the interaction between a feature and all other features). The amount of the variance explained by the interaction (difference between observed and no-interaction PD) is used as interaction strength statistic. The statistic is 0 if there is no interaction at all and 1 if all of the variance of the \\(PD_{jk}\\) or \\(\\hat{f}\\) is explained by the sum of the partial dependence functions. An interaction statistic of 1 between two features means that each single PD function is constant and the effect on the prediction only comes through the interaction. The H-statistic can also be larger than 1, which is more difficult to interpret. This can happen when the variance of the 2-way interaction is larger than the variance of the 2-dimensional partial dependence plot. Mathematically, the H-statistic proposed by Friedman and Popescu for the interaction between feature j and k is: \\[H^2_{jk}=\\sum_{i=1}^n\\left[PD_{jk}(x_{j}^{(i)},x_k^{(i)})-PD_j(x_j^{(i)})-PD_k(x_{k}^{(i)})\\right]^2/\\sum_{i=1}^n{PD}^2_{jk}(x_j^{(i)},x_k^{(i)})\\] The same applies to measuring whether a feature j interacts with any other feature: \\[H^2_{j}=\\sum_{i=1}^n\\left[\\hat{f}(x^{(i)})-PD_j(x_j^{(i)})-PD_{-j}(x_{-j}^{(i)})\\right]^2/\\sum_{i=1}^n\\hat{f}^2(x^{(i)})\\] The H-statistic is expensive to evaluate, because it iterates over all data points and at each point the partial dependence has to be evaluated which in turn is done with all n data points. In the worst case, we need 2n2 calls to the machine learning models predict function to compute the two-way H-statistic (j vs. k) and 3n2 for the total H-statistic (j vs. all). To speed up the computation, we can sample from the n data points. This has the disadvantage of increasing the variance of the partial dependence estimates, which makes the H-statistic unstable. So if you are using sampling to reduce the computational burden, make sure to sample enough data points. Friedman and Popescu also propose a test statistic to evaluate whether the H-statistic differs significantly from zero. The null hypothesis is the absence of interaction. To generate the interaction statistic under the null hypothesis, you must be able to adjust the model so that it has no interaction between feature j and k or all others. This is not possible for all types of models. Therefore this test is model-specific, not model-agnostic, and as such not covered here. The interaction strength statistic can also be applied in a classification setting if the prediction is a probability. 5.4.3 Examples Let us see what feature interactions look like in practice! We measure the interaction strength of features in a support vector machine that predicts the number of rented bikes based on weather and calendrical features. The following plot shows the feature interaction H-statistic: FIGURE 5.24: The interaction strength (H-statistic) for each feature with all other features for a support vector machine predicting bicycle rentals. Overall, the interaction effects between the features are very weak (below 10% of variance explained per feature). In the next example, we calculate the interaction statistic for a classification problem. We analyze the interactions between features in a random forest trained to predict cervical cancer, given some risk factors. FIGURE 5.25: The interaction strength (H-statistic) for each feature with all other features for a random forest predicting the probability of cervical cancer. The years on hormonal contraceptives has the highest relative interaction effect with all other features, followed by the number of pregnancies. After looking at the feature interactions of each feature with all other features, we can select one of the features and dive deeper into all the 2-way interactions between the selected feature and the other features. FIGURE 5.26: The 2-way interaction strengths (H-statistic) between number of pregnancies and each other feature. There is a strong interaction between the number of pregnancies and the age. 5.4.4 Advantages The interaction H-statistic has an underlying theory through the partial dependence decomposition. The H-statistic has a meaningful interpretation: The interaction is defined as the share of variance that is explained by the interaction. Since the statistic is dimensionless, it is comparable across features and even across models. The statistic detects all kinds of interactions, regardless of their particular form. With the H-statistic it is also possible to analyze arbitrary higher interactions such as the interaction strength between 3 or more features. 5.4.5 Disadvantages The first thing you will notice: The interaction H-statistic takes a long time to compute, because it is computationally expensive. The computation involves estimating marginal distributions. These estimates also have a certain variance if we do not use all data points. This means that as we sample points, the estimates also vary from run to run and the results can be unstable. I recommend repeating the H-statistic computation a few times to see if you have enough data to get a stable result. It is unclear whether an interaction is significantly greater than 0. We would need to conduct a statistical test, but this test is not (yet) available in a model-agnostic version. Concerning the test problem, it is difficult to say when the H-statistic is large enough for us to consider an interaction &quot;strong&quot;. Also, the H-statistics can be larger than 1, which makes the interpretation difficult. The H-statistic tells us the strength of interactions, but it does not tell us how the interactions look like. That is what partial dependence plots are for. A meaningful workflow is to measure the interaction strengths and then create 2D-partial dependence plots for the interactions you are interested in. The H-statistic cannot be used meaningfully if the inputs are pixels. So the technique is not useful for image classifier. The interaction statistic works under the assumption that we can shuffle features independently. If the features correlate strongly, the assumption is violated and we integrate over feature combinations that are very unlikely in reality. That is the same problem that partial dependence plots have. You cannot say in general if it leads to overestimation or underestimation. Sometimes the results are strange and for small simulations do not yield the expected results. But this is more of an anecdotal observation. 5.4.6 Implementations For the examples in this book, I used the R package iml, which is available on CRAN and the development version on Github. There are other implementations, which focus on specific models: The R package pre implements RuleFit and H-statistic. The R package gbm implements gradient boosted models and H-statistic. 5.4.7 Alternatives The H-statistic is not the only way to measure interactions: Variable Interaction Networks (VIN) by Hooker (2004)32 is an approach that decomposes the prediction function into main effects and feature interactions. The interactions between features are then visualized as a network. Unfortunately no software is available yet. Partial dependence based feature interaction by Greenwell et. al (2018)33 measures the interaction between two features. This approach measures the feature importance (defined as the variance of the partial dependence function) of one feature conditional on different, fixed points of the other feature. If the variance is high, then the features interact with each other, if it is zero, they do not interact. The corresponding R package vip is available on Github. The package also covers partial dependence plots and feature importance. Friedman, Jerome H, and Bogdan E Popescu. &quot;Predictive learning via rule ensembles.&quot; The Annals of Applied Statistics. JSTOR, 916–54. (2008).↩ Hooker, Giles. &quot;Discovering additive structure in black box functions.&quot; Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004).↩ Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. &quot;A simple and effective model-based variable importance measure.&quot; arXiv preprint arXiv:1805.04755 (2018).↩ "],["feature-importance.html", "5.5 Permutation Feature Importance", " 5.5 Permutation Feature Importance Permutation feature importance measures the increase in the prediction error of the model after we permuted the feature's values, which breaks the relationship between the feature and the true outcome. 5.5.1 Theory The concept is really straightforward: We measure the importance of a feature by calculating the increase in the model's prediction error after permuting the feature. A feature is &quot;important&quot; if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction. A feature is &quot;unimportant&quot; if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction. The permutation feature importance measurement was introduced by Breiman (2001)34 for random forests. Based on this idea, Fisher, Rudin, and Dominici (2018)35 proposed a model-agnostic version of the feature importance and called it model reliance. They also introduced more advanced ideas about feature importance, for example a (model-specific) version that takes into account that many prediction models may predict the data well. Their paper is worth reading. The permutation feature importance algorithm based on Fisher, Rudin, and Dominici (2018): Input: Trained model f, feature matrix X, target vector y, error measure L(y,f). Estimate the original model error eorig = L(y, f(X)) (e.g. mean squared error) For each feature j = 1,...,p do: Generate feature matrix Xperm by permuting feature j in the data X. This breaks the association between feature j and true outcome y. Estimate error eperm = L(Y,f(Xperm)) based on the predictions of the permuted data. Calculate permutation feature importance FIj= eperm/eorig. Alternatively, the difference can be used: FIj = eperm - eorig Sort features by descending FI. Fisher, Rudin, and Dominici (2018) suggest in their paper to split the dataset in half and swap the values of feature j of the two halves instead of permuting feature j. This is exactly the same as permuting feature j, if you think about it. If you want a more accurate estimate, you can estimate the error of permuting feature j by pairing each instance with the value of feature j of each other instance (except with itself). This gives you a dataset of size n(n-1) to estimate the permutation error, and it takes a large amount of computation time. I can only recommend using the n(n-1) -method if you are serious about getting extremely accurate estimates. 5.5.2 Should I Compute Importance on Training or Test Data? tl;dr: I do not have a definite answer. Answering the question about training or test data touches the fundamental question of what feature importance is. The best way to understand the difference between feature importance based on training vs. based on test data is an &quot;extreme&quot; example. I trained a support vector machine to predict a continuous, random target outcome given 50 random features (200 instances). By &quot;random&quot; I mean that the target outcome is independent of the 50 features. This is like predicting tomorrow's temperature given the latest lottery numbers. If the model &quot;learns&quot; any relationships, then it overfits. And in fact, the SVM did overfit on the training data. The mean absolute error (short: mae) for the training data is 0.29 and for the test data 0.82, which is also the error of the best possible model that always predicts the mean outcome of 0 (mae of 0.78). In other words, the SVM model is garbage. What values for the feature importance would you expect for the 50 features of this overfitted SVM? Zero because none of the features contribute to improved performance on unseen test data? Or should the importances reflect how much the model depends on each of the features, regardless whether the learned relationships generalize to unseen data? Let us take a look at how the distributions of feature importances for training and test data differ. FIGURE 5.27: Distributions of feature importance values by data type. An SVM was trained on a regression dataset with 50 random features and 200 instances. The SVM overfits the data: Feature importance based on the training data shows many important features. Computed on unseen test data, the feature importances are close to a ratio of one (=unimportant). It is unclear to me which of the two results is more desirable. So I will try to make a case for both versions and let you decide for yourself. The case for test data This is a simple case: Model error estimates based on training data are garbage -&gt; feature importance relies on model error estimates -&gt; feature importance based on training data is garbage. Really, it is one of the first things you learn in machine learning: If you measure the model error (or performance) on the same data on which the model was trained, the measurement is usually too optimistic, which means that the model seems to work much better than it does in reality. And since the permutation feature importance relies on measurements of the model error, we should use unseen test data. The feature importance based on training data makes us mistakenly believe that features are important for the predictions, when in reality the model was just overfitting and the features were not important at all. The case for training data The arguments for using training data are somewhat more difficult to formulate, but are IMHO just as compelling as the arguments for using test data. We take another look at our garbage SVM. Based on the training data, the most important feature was X42. Let us look at a partial dependence plot of feature X42. The partial dependence plot shows how the model output changes based on changes of the feature and does not rely on the generalization error. It does not matter whether the PDP is computed with training or test data. FIGURE 5.28: PDP of feature X42, which is the most important feature according to the feature importance based on the training data. The plot shows how the SVM depends on this feature to make predictions The plot clearly shows that the SVM has learned to rely on feature X42 for its predictions, but according to the feature importance based on the test data (1), it is not important. Based on the training data, the importance is 1.19, reflecting that the model has learned to use this feature. Feature importance based on the training data tells us which features are important for the model in the sense that it depends on them for making predictions. As part of the case for using training data, I would like to introduce an argument against test data. In practice, you want to use all your data to train your model to get the best possible model in the end. This means no unused test data is left to compute the feature importance. You have the same problem when you want to estimate the generalization error of your model. If you would use (nested) cross-validation for the feature importance estimation, you would have the problem that the feature importance is not calculated on the final model with all the data, but on models with subsets of the data that might behave differently. In the end, you need to decide whether you want to know how much the model relies on each feature for making predictions (-&gt; training data) or how much the feature contributes to the performance of the model on unseen data (-&gt; test data). To the best of my knowledge, there is no research addressing the question of training vs. test data. It will require more thorough examination than my &quot;garbage-SVM&quot; example. We need more research and more experience with these tools to gain a better understanding. Next, we will look at some examples. I based the importance computation on the training data, because I had to choose one and using the training data needed a few lines less code. 5.5.3 Example and Interpretation I show examples for classification and regression. Cervical cancer (classification) We fit a random forest model to predict cervical cancer. We measure the error increase by 1-AUC (1 minus the area under the ROC curve). Features associated with a model error increase by a factor of 1 (= no change) were not important for predicting cervical cancer. FIGURE 5.29: The importance of each of the features for predicting cervical cancer with a random forest. The most important feature was Hormonal.Contraceptives..years.. Permuting Hormonal.Contraceptives..years. resulted in an increase in 1-AUC by a factor of 6.13 The feature with the highest importance was Hormonal.Contraceptives..years. associated with an error increase of 6.13 after permutation. Bike sharing (regression) We fit a support vector machine model to predict the number of rented bikes, given weather conditions and calendar information. As error measurement we use the mean absolute error. FIGURE 5.30: The importance for each of the features in predicting bike counts with a support vector machine. The most important feature was temp, the least important was holiday. 5.5.4 Advantages Nice interpretation: Feature importance is the increase in model error when the feature's information is destroyed. Feature importance provides a highly compressed, global insight into the model's behavior. A positive aspect of using the error ratio instead of the error difference is that the feature importance measurements are comparable across different problems. The importance measure automatically takes into account all interactions with other features. By permuting the feature you also destroy the interaction effects with other features. This means that the permutation feature importance takes into account both the main feature effect and the interaction effects on model performance. This is also a disadvantage because the importance of the interaction between two features is included in the importance measurements of both features. This means that the feature importances do not add up to the total drop in performance, but the sum is larger. Only if there is no interaction between the features, as in a linear model, the importances add up approximately. Permutation feature importance does not require retraining the model. Some other methods suggest deleting a feature, retraining the model and then comparing the model error. Since the retraining of a machine learning model can take a long time, &quot;only&quot; permuting a feature can save a lot of time. Importance methods that retrain the model with a subset of features appear intuitive at first glance, but the model with the reduced data is meaningless for the feature importance. We are interested in the feature importance of a fixed model. Retraining with a reduced dataset creates a different model than the one we are interested in. Suppose you train a sparse linear model (with Lasso) with a fixed number of features with a non-zero weight. The dataset has 100 features, you set the number of non-zero weights to 5. You analyze the importance of one of the features that have a non-zero weight. You remove the feature and retrain the model. The model performance remains the same because another equally good feature gets a non-zero weight and your conclusion would be that the feature was not important. Another example: The model is a decision tree and we analyze the importance of the feature that was chosen as the first split. You remove the feature and retrain the model. Since another feature is chosen as the first split, the whole tree can be very different, which means that we compare the error rates of (potentially) completely different trees to decide how important that feature is for one of the trees. 5.5.5 Disadvantages It is very unclear whether you should use training or test data to compute the feature importance. Permutation feature importance is linked to the error of the model. This is not inherently bad, but in some cases not what you need. In some cases, you might prefer to know how much the model's output varies for a feature without considering what it means for performance. For example, you want to find out how robust your model's output is when someone manipulates the features. In this case, you would not be interested in how much the model performance decreases when a feature is permuted, but how much of the model's output variance is explained by each feature. Model variance (explained by the features) and feature importance correlate strongly when the model generalizes well (i.e. it does not overfit). You need access to the true outcome. If someone only provides you with the model and unlabeled data -- but not the true outcome -- you cannot compute the permutation feature importance. The permutation feature importance depends on shuffling the feature, which adds randomness to the measurement. When the permutation is repeated, the results might vary greatly. Repeating the permutation and averaging the importance measures over repetitions stabilizes the measure, but increases the time of computation. If features are correlated, the permutation feature importance can be biased by unrealistic data instances. The problem is the same as with partial dependence plots: The permutation of features produces unlikely data instances when two or more features are correlated. When they are positively correlated (like height and weight of a person) and I shuffle one of the features, I create new instances that are unlikely or even physically impossible (2 meter person weighing 30 kg for example), yet I use these new instances to measure the importance. In other words, for the permutation feature importance of a correlated feature, we consider how much the model performance decreases when we exchange the feature with values we would never observe in reality. Check if the features are strongly correlated and be careful about the interpretation of the feature importance if they are. Another tricky thing: Adding a correlated feature can decrease the importance of the associated feature by splitting the importance between both features. Let me give you an example of what I mean by &quot;splitting&quot; feature importance: We want to predict the probability of rain and use the temperature at 8:00 AM of the day before as a feature along with other uncorrelated features. I train a random forest and it turns out that the temperature is the most important feature and all is well and I sleep well the next night. Now imagine another scenario in which I additionally include the temperature at 9:00 AM as a feature that is strongly correlated with the temperature at 8:00 AM. The temperature at 9:00 AM does not give me much additional information if I already know the temperature at 8:00 AM. But having more features is always good, right? I train a random forest with the two temperature features and the uncorrelated features. Some of the trees in the random forest pick up the 8:00 AM temperature, others the 9:00 AM temperature, again others both and again others none. The two temperature features together have a bit more importance than the single temperature feature before, but instead of being at the top of the list of important features, each temperature is now somewhere in the middle. By introducing a correlated feature, I kicked the most important feature from the top of the importance ladder to mediocrity. On one hand this is fine, because it simply reflects the behavior of the underlying machine learning model, here the random forest. The 8:00 AM temperature has simply become less important because the model can now rely on the 9:00 AM measurement as well. On the other hand, it makes the interpretation of the feature importance considerably more difficult. Imagine you want to check the features for measurement errors. The check is expensive and you decide to check only the top 3 of the most important features. In the first case you would check the temperature, in the second case you would not include any temperature feature just because they now share the importance. Even though the importance values might make sense at the level of model behavior, it is confusing if you have correlated features. 5.5.6 Software and Alternatives The iml R package was used for the examples. The R packages DALEX and vip, as well as the Python library alibi, also implement model-agnostic permutation feature importance. An algorithm called PIMP adapts the feature importance algorithm to provide p-values for the importances. Breiman, Leo.“Random Forests.” Machine Learning 45 (1). Springer: 5-32 (2001).↩ Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. “Model Class Reliance: Variable importance measures for any machine learning model class, from the ‘Rashomon’ perspective.” http://arxiv.org/abs/1801.01489 (2018).↩ "],["global.html", "5.6 Global Surrogate", " 5.6 Global Surrogate A global surrogate model is an interpretable model that is trained to approximate the predictions of a black box model. We can draw conclusions about the black box model by interpreting the surrogate model. Solving machine learning interpretability by using more machine learning! 5.6.1 Theory Surrogate models are also used in engineering: If an outcome of interest is expensive, time-consuming or otherwise difficult to measure (e.g. because it comes from a complex computer simulation), a cheap and fast surrogate model of the outcome can be used instead. The difference between the surrogate models used in engineering and in interpretable machine learning is that the underlying model is a machine learning model (not a simulation) and that the surrogate model must be interpretable. The purpose of (interpretable) surrogate models is to approximate the predictions of the underlying model as accurately as possible and to be interpretable at the same time. The idea of surrogate models can be found under different names: Approximation model, metamodel, response surface model, emulator, ... About the theory: There is actually not much theory needed to understand surrogate models. We want to approximate our black box prediction function f as closely as possible with the surrogate model prediction function g, under the constraint that g is interpretable. For the function g any interpretable model -- for example from the interpretable models chapter -- can be used. For example a linear model: \\[g(x)=\\beta_0+\\beta_1{}x_1{}+\\ldots+\\beta_p{}x_p\\] Or a decision tree: \\[g(x)=\\sum_{m=1}^Mc_m{}I\\{x\\in{}R_m\\}\\] Training a surrogate model is a model-agnostic method, since it does not require any information about the inner workings of the black box model, only access to data and the prediction function is necessary. If the underlying machine learning model was replaced with another, you could still use the surrogate method. The choice of the black box model type and of the surrogate model type is decoupled. Perform the following steps to obtain a surrogate model: Select a dataset X. This can be the same dataset that was used for training the black box model or a new dataset from the same distribution. You could even select a subset of the data or a grid of points, depending on your application. For the selected dataset X, get the predictions of the black box model. Select an interpretable model type (linear model, decision tree, ...). Train the interpretable model on the dataset X and its predictions. Congratulations! You now have a surrogate model. Measure how well the surrogate model replicates the predictions of the black box model. Interpret the surrogate model. You may find approaches for surrogate models that have some extra steps or differ a little, but the general idea is usually as described here. One way to measure how well the surrogate replicates the black box model is the R-squared measure: \\[R^2=1-\\frac{SSE}{SST}=1-\\frac{\\sum_{i=1}^n(\\hat{y}_*^{(i)}-\\hat{y}^{(i)})^2}{\\sum_{i=1}^n(\\hat{y}^{(i)}-\\bar{\\hat{y}})^2}\\] where \\(\\hat{y}_*^{(i)}\\) is the prediction for the i-th instance of the surrogate model, \\(\\hat{y}^{(i)}\\) the prediction of the black box model and \\(\\bar{\\hat{y}}\\) the mean of the black box model predictions. SSE stands for sum of squares error and SST for sum of squares total. The R-squared measure can be interpreted as the percentage of variance that is captured by the surrogate model. If R-squared is close to 1 (= low SSE), then the interpretable model approximates the behavior of the black box model very well. If the interpretable model is very close, you might want to replace the complex model with the interpretable model. If the R-squared is close to 0 (= high SSE), then the interpretable model fails to explain the black box model. Note that we have not talked about the model performance of the underlying black box model, i.e. how good or bad it performs in predicting the actual outcome. The performance of the black box model does not play a role in training the surrogate model. The interpretation of the surrogate model is still valid because it makes statements about the model and not about the real world. But of course the interpretation of the surrogate model becomes irrelevant if the black box model is bad, because then the black box model itself is irrelevant. We could also build a surrogate model based on a subset of the original data or reweight the instances. In this way, we change the distribution of the surrogate model's input, which changes the focus of the interpretation (then it is no longer really global). If we weight the data locally by a specific instance of the data (the closer the instances to the selected instance, the higher their weight), we get a local surrogate model that can explain the individual prediction of the instance. Read more about local models in the following chapter. 5.6.2 Example To demonstrate the surrogate models, we consider a regression and a classification example. First, we train a support vector machine to predict the daily number of rented bikes given weather and calendar information. The support vector machine is not very interpretable, so we train a surrogate with a CART decision tree as interpretable model to approximate the behavior of the support vector machine. FIGURE 5.31: The terminal nodes of a surrogate tree that approximates the predictions of a support vector machine trained on the bike rental dataset. The distributions in the nodes show that the surrogate tree predicts a higher number of rented bikes when temperature is above 13 degrees Celsius and when the day was later in the 2 year period (cut point at 435 days). The surrogate model has a R-squared (variance explained) of 0.77 which means it approximates the underlying black box behavior quite well, but not perfectly. If the fit were perfect, we could throw away the support vector machine and use the tree instead. In our second example, we predict the probability of cervical cancer with a random forest. Again we train a decision tree with the original dataset, but with the prediction of the random forest as outcome, instead of the real classes (healthy vs. cancer) from the data. FIGURE 5.32: The terminal nodes of a surrogate tree that approximates the predictions of a random forest trained on the cervical cancer dataset. The counts in the nodes show the frequency of the black box models classifications in the nodes. The surrogate model has an R-squared (variance explained) of 0.19, which means it does not approximate the random forest well and we should not overinterpret the tree when drawing conclusions about the complex model. 5.6.3 Advantages The surrogate model method is flexible: Any model from the interpretable models chapter can be used. This also means that you can exchange not only the interpretable model, but also the underlying black box model. Suppose you create some complex model and explain it to different teams in your company. One team is familiar with linear models, the other team can understand decision trees. You can train two surrogate models (linear model and decision tree) for the original black box model and offer two kinds of explanations. If you find a better performing black box model, you do not have to change your method of interpretation, because you can use the same class of surrogate models. I would argue that the approach is very intuitive and straightforward. This means it is easy to implement, but also easy to explain to people not familiar with data science or machine learning. With the R-squared measure, we can easily measure how good our surrogate models are in approximating the black box predictions. 5.6.4 Disadvantages You have to be aware that you draw conclusions about the model and not about the data, since the surrogate model never sees the real outcome. It is not clear what the best cut-off for R-squared is in order to be confident that the surrogate model is close enough to the black box model. 80% of variance explained? 50%? 99%? We can measure how close the surrogate model is to the black box model. Let us assume we are not very close, but close enough. It could happen that the interpretable model is very close for one subset of the dataset, but widely divergent for another subset. In this case the interpretation for the simple model would not be equally good for all data points. The interpretable model you choose as a surrogate comes with all its advantages and disadvantages. Some people argue that there are, in general, no intrinsically interpretable models (including even linear models and decision trees) and that it would even be dangerous to have an illusion of interpretability. If you share this opinion, then of course this method is not for you. 5.6.5 Software I used the iml R package for the examples. If you can train a machine learning model, then you should be able to implement surrogate models yourself. Simply train an interpretable model to predict the predictions of the black box model. "],["lime.html", "5.7 Local Surrogate (LIME)", " 5.7 Local Surrogate (LIME) Local surrogate models are interpretable models that are used to explain individual predictions of black box machine learning models. Local interpretable model-agnostic explanations (LIME)36 is a paper in which the authors propose a concrete implementation of local surrogate models. Surrogate models are trained to approximate the predictions of the underlying black box model. Instead of training a global surrogate model, LIME focuses on training local surrogate models to explain individual predictions. The idea is quite intuitive. First, forget about the training data and imagine you only have the black box model where you can input data points and get the predictions of the model. You can probe the box as often as you want. Your goal is to understand why the machine learning model made a certain prediction. LIME tests what happens to the predictions when you give variations of your data into the machine learning model. LIME generates a new dataset consisting of permuted samples and the corresponding predictions of the black box model. On this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest. The interpretable model can be anything from the interpretable models chapter, for example Lasso or a decision tree. The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation. This kind of accuracy is also called local fidelity. Mathematically, local surrogate models with interpretability constraint can be expressed as follows: \\[\\text{explanation}(x)=\\arg\\min_{g\\in{}G}L(f,g,\\pi_x)+\\Omega(g)\\] The explanation model for instance x is the model g (e.g. linear regression model) that minimizes loss L (e.g. mean squared error), which measures how close the explanation is to the prediction of the original model f (e.g. an xgboost model), while the model complexity \\(\\Omega(g)\\) is kept low (e.g. prefer fewer features). G is the family of possible explanations, for example all possible linear regression models. The proximity measure \\(\\pi_x\\) defines how large the neighborhood around instance x is that we consider for the explanation. In practice, LIME only optimizes the loss part. The user has to determine the complexity, e.g. by selecting the maximum number of features that the linear regression model may use. The recipe for training local surrogate models: Select your instance of interest for which you want to have an explanation of its black box prediction. Perturb your dataset and get the black box predictions for these new points. Weight the new samples according to their proximity to the instance of interest. Train a weighted, interpretable model on the dataset with the variations. Explain the prediction by interpreting the local model. In the current implementations in R and Python, for example, linear regression can be chosen as interpretable surrogate model. In advance, you have to select K, the number of features you want to have in your interpretable model. The lower K, the easier it is to interpret the model. A higher K potentially produces models with higher fidelity. There are several methods for training models with exactly K features. A good choice is Lasso. A Lasso model with a high regularization parameter \\(\\lambda\\) yields a model without any feature. By retraining the Lasso models with slowly decreasing \\(\\lambda\\), one after the other, the features get weight estimates that differ from zero. If there are K features in the model, you have reached the desired number of features. Other strategies are forward or backward selection of features. This means you either start with the full model (= containing all features) or with a model with only the intercept and then test which feature would bring the biggest improvement when added or removed, until a model with K features is reached. How do you get the variations of the data? This depends on the type of data, which can be either text, image or tabular data. For text and images, the solution is to turn single words or super-pixels on or off. In the case of tabular data, LIME creates new samples by perturbing each feature individually, drawing from a normal distribution with mean and standard deviation taken from the feature. 5.7.1 LIME for Tabular Data Tabular data is data that comes in tables, with each row representing an instance and each column a feature. LIME samples are not taken around the instance of interest, but from the training data's mass center, which is problematic. But it increases the probability that the result for some of the sample points predictions differ from the data point of interest and that LIME can learn at least some explanation. It is best to visually explain how sampling and local model training works: FIGURE 5.33: LIME algorithm for tabular data. A) Random forest predictions given features x1 and x2. Predicted classes: 1 (dark) or 0 (light). B) Instance of interest (big dot) and data sampled from a normal distribution (small dots). C) Assign higher weight to points near the instance of interest. D) Signs of the grid show the classifications of the locally learned model from the weighted samples. The white line marks the decision boundary (P(class=1) = 0.5). As always, the devil is in the detail. Defining a meaningful neighborhood around a point is difficult. LIME currently uses an exponential smoothing kernel to define the neighborhood. A smoothing kernel is a function that takes two data instances and returns a proximity measure. The kernel width determines how large the neighborhood is: A small kernel width means that an instance must be very close to influence the local model, a larger kernel width means that instances that are farther away also influence the model. If you look at LIME's Python implementation (file lime/lime_tabular.py) you will see that it uses an exponential smoothing kernel (on the normalized data) and the kernel width is 0.75 times the square root of the number of columns of the training data. It looks like an innocent line of code, but it is like an elephant sitting in your living room next to the good porcelain you got from your grandparents. The big problem is that we do not have a good way to find the best kernel or width. And where does the 0.75 even come from? In certain scenarios, you can easily turn your explanation around by changing the kernel width, as shown in the following figure: FIGURE 5.34: Explanation of the prediction of instance x = 1.6. The predictions of the black box model depending on a single feature is shown as a thick line and the distribution of the data is shown with rugs. Three local surrogate models with different kernel widths are computed. The resulting linear regression model depends on the kernel width: Does the feature have a negative, positive or no effect for x = 1.6? The example shows only one feature. It gets worse in high-dimensional feature spaces. It is also very unclear whether the distance measure should treat all features equally. Is a distance unit for feature x1 identical to one unit for feature x2? Distance measures are quite arbitrary and distances in different dimensions (aka features) might not be comparable at all. 5.7.1.1 Example Let us look at a concrete example. We go back to the bike rental data and turn the prediction problem into a classification: After taking into account the trend that the bicycle rental has become more popular over time, we want to know on a certain day whether the number of bicycles rented will be above or below the trend line. You can also interpret &quot;above&quot; as being above the average number of bicycles, but adjusted for the trend. First we train a random forest with 100 trees on the classification task. On what day will the number of rental bikes be above the trend-free average, based on weather and calendar information? The explanations are created with 2 features. The results of the sparse local linear models trained for two instances with different predicted classes: FIGURE 5.35: LIME explanations for two instances of the bike rental dataset. Warmer temperature and good weather situation have a positive effect on the prediction. The x-axis shows the feature effect: The weight times the actual feature value. From the figure it becomes clear that it is easier to interpret categorical features than numerical features. One solution is to categorize the numerical features into bins. 5.7.2 LIME for Text LIME for text differs from LIME for tabular data. Variations of the data are generated differently: Starting from the original text, new texts are created by randomly removing words from the original text. The dataset is represented with binary features for each word. A feature is 1 if the corresponding word is included and 0 if it has been removed. 5.7.2.1 Example In this example we classify YouTube comments as spam or normal. The black box model is a deep decision tree trained on the document word matrix. Each comment is one document (= one row) and each column is the number of occurrences of a given word. Short decision trees are easy to understand, but in this case the tree is very deep. Also in place of this tree there could have been a recurrent neural network or a support vector machine trained on word embeddings (abstract vectors). Let us look at the two comments of this dataset and the corresponding classes (1 for spam, 0 for normal comment): CONTENT CLASS 267 PSY is a good guy 0 173 For Christmas Song visit my channel! ;) 1 The next step is to create some variations of the datasets used in a local model. For example, some variations of one of the comments: For Christmas Song visit my channel! ;) prob weight 2 1 0 1 1 0 0 1 0.17 0.57 3 0 1 1 1 1 0 1 0.17 0.71 4 1 0 0 1 1 1 1 0.99 0.71 5 1 0 1 1 1 1 1 0.99 0.86 6 0 1 1 1 0 0 1 0.17 0.57 Each column corresponds to one word in the sentence. Each row is a variation, 1 means that the word is part of this variation and 0 means that the word has been removed. The corresponding sentence for one of the variations is &quot;Christmas Song visit my ;)&quot;. The &quot;prob&quot; column shows the predicted probability of spam for each of the sentence variations. The &quot;weight&quot; column shows the proximity of the variation to the original sentence, calculated as 1 minus the proportion of words that were removed, for example if 1 out of 7 words was removed, the proximity is 1 - 1/7 = 0.86. Here are the two sentences (one spam, one no spam) with their estimated local weights found by the LIME algorithm: case label_prob feature feature_weight 1 0.1701170 is 0.000000 1 0.1701170 good 0.000000 1 0.1701170 a 0.000000 2 0.9939024 channel! 6.180747 2 0.9939024 Christmas 0.000000 2 0.9939024 Song 0.000000 The word &quot;channel&quot; indicates a high probability of spam. For the non-spam comment no non-zero weight was estimated, because no matter which word is removed, the predicted class remains the same. 5.7.3 LIME for Images This section was written by Verena Haunschmid. LIME for images works differently than LIME for tabular data and text. Intuitively, it would not make much sense to perturb individual pixels, since many more than one pixel contribute to one class. Randomly changing individual pixels would probably not change the predictions by much. Therefore, variations of the images are created by segmenting the image into &quot;superpixels&quot; and turning superpixels off or on. Superpixels are interconnected pixels with similar colors and can be turned off by replacing each pixel with a user-defined color such as gray. The user can also specify a probability for turning off a superpixel in each permutation. 5.7.3.1 Example In this example we look at a classification made by the Inception V3 neural network. The image used shows some bread I baked which are in a bowl. Since we can have several predicted labels per image (sorted by probability), we can explain the top labels. The top prediction is &quot;Bagel&quot; with a probability of 77%, followed by &quot;Strawberry&quot; with a probability of 4%. The following images show for &quot;Bagel&quot; and &quot;Strawberry&quot; the LIME explanations. The explanations can be displayed directly on the image samples. Green means that this part of the image increases the probability for the label and red means a decrease. FIGURE 5.36: Left: Image of a bowl of bread. Middle and right: LIME explanations for the top 2 classes (bagel, strawberry) for image classification made by Google's Inception V3 neural network. The prediction and explanation for &quot;Bagel&quot; are very reasonable, even if the prediction is wrong -- these are clearly no bagels since the hole in the middle is missing. 5.7.4 Advantages Even if you replace the underlying machine learning model, you can still use the same local, interpretable model for explanation. Suppose the people looking at the explanations understand decision trees best. Because you use local surrogate models, you use decision trees as explanations without actually having to use a decision tree to make the predictions. For example, you can use a SVM. And if it turns out that an xgboost model works better, you can replace the SVM and still use as decision tree to explain the predictions. Local surrogate models benefit from the literature and experience of training and interpreting interpretable models. When using Lasso or short trees, the resulting explanations are short (= selective) and possibly contrastive. Therefore, they make human-friendly explanations. This is why I see LIME more in applications where the recipient of the explanation is a lay person or someone with very little time. It is not sufficient for complete attributions, so I do not see LIME in compliance scenarios where you might be legally required to fully explain a prediction. Also for debugging machine learning models, it is useful to have all the reasons instead of a few. LIME is one of the few methods that works for tabular data, text and images. The fidelity measure (how well the interpretable model approximates the black box predictions) gives us a good idea of how reliable the interpretable model is in explaining the black box predictions in the neighborhood of the data instance of interest. LIME is implemented in Python (lime library) and R (lime package and iml package) and is very easy to use. The explanations created with local surrogate models can use other (interpretable) features than the original model was trained on.. Of course, these interpretable features must be derived from the data instances. A text classifier can rely on abstract word embeddings as features, but the explanation can be based on the presence or absence of words in a sentence. A regression model can rely on a non-interpretable transformation of some attributes, but the explanations can be created with the original attributes. For example, the regression model could be trained on components of a principal component analysis (PCA) of answers to a survey, but LIME might be trained on the original survey questions. Using interpretable features for LIME can be a big advantage over other methods, especially when the model was trained with non-interpretable features. 5.7.5 Disadvantages The correct definition of the neighborhood is a very big, unsolved problem when using LIME with tabular data. In my opinion it is the biggest problem with LIME and the reason why I would recommend to use LIME only with great care. For each application you have to try different kernel settings and see for yourself if the explanations make sense. Unfortunately, this is the best advice I can give to find good kernel widths. Sampling could be improved in the current implementation of LIME. Data points are sampled from a Gaussian distribution, ignoring the correlation between features. This can lead to unlikely data points which can then be used to learn local explanation models. The complexity of the explanation model has to be defined in advance. This is just a small complaint, because in the end the user always has to define the compromise between fidelity and sparsity. Another really big problem is the instability of the explanations. In an article 37 the authors showed that the explanations of two very close points varied greatly in a simulated setting. Also, in my experience, if you repeat the sampling process, then the explantions that come out can be different. Instability means that it is difficult to trust the explanations, and you should be very critical. Conclusion: Local surrogate models, with LIME as a concrete implementation, are very promising. But the method is still in development phase and many problems need to be solved before it can be safely applied. Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Why should I trust you?: Explaining the predictions of any classifier.&quot; Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016).↩ Alvarez-Melis, David, and Tommi S. Jaakkola. &quot;On the robustness of interpretability methods.&quot; arXiv preprint arXiv:1806.08049 (2018).↩ "],["anchors.html", "5.8 Scoped Rules (Anchors)", " 5.8 Scoped Rules (Anchors) Authors: Tobias Goerke &amp; Magdalena Lang This chapter is currently only available in this web version. ebook and print will follow. Anchors explains individual predictions of any black-box classification model by finding a decision rule that &quot;anchors&quot; the prediction sufficiently. A rule anchors a prediction if changes in other feature values do not affect the prediction. Anchors utilizes reinforcement learning techniques in combination with a graph search algorithm to reduce the number of model calls (and hence the required runtime) to a minimum while still being able to recover from local optima. Ribeiro, Singh, and Guestrin proposed the algorithm in 201838 -- the same researchers that introduced the LIME algorithm. Like its predecessor, the anchors approach deploys a perturbation-based strategy to generate local explanations for predictions of black-box machine learning models. However, instead of surrogate models used by LIME, the resulting explanations are expressed as easy-to-understand IF-THEN rules, called anchors. These rules are reusable since they are scoped: anchors include the notion of coverage, stating precisely to which other, possibly unseen, instances they apply. Finding anchors involves an exploration or multi-armed bandit problem, which originates in the discipline of reinforcement learning. To this end, neighbors, or perturbations, are created and evaluated for every instance that is being explained. Doing so allows the approach to disregard the black-box’s structure and its internal parameters so that these can remain both unobserved and unaltered. Thus, the algorithm is model-agnostic, meaning it can be applied to any class of model. In their paper, the authors compare both of their algorithms and visualize how differently these consult an instance's neighborhood to derive results. For this, the following figure depicts both LIME and anchors locally explaining a complex binary classifier (predicts either - or +) using two exemplary instances. LIME’s results do not indicate how faithful they are as LIME solely learns a linear decision boundary that best approximates the model given a perturbation space \\(D\\). Given the same perturbation space, the anchors approach constructs explanations whose coverage is adapted to the model’s behavior and clearly express their boundaries. Thus, they are faithful by design and state exactly for which instances they are valid. This property makes anchors particularly intuitive and easy to comprehend. FIGURE 5.37: LIME vs. Anchors -- A Toy Visualization. Figure from Ribeiro, Singh, and Guestrin (2018). As mentioned before, the algorithm’s results or explanations come in the form of rules, called anchors. The following simple example illustrates such an anchor. For instance, suppose we are given a bivariate black-box model that predicts whether or not a passenger survives the Titanic disaster. Now we would like to know why the model predicts for one specific individual that it survives. The anchors algorithm provides a result explanation like the one shown below. one exemplary individual and the model's prediction Feature Value Age 20 Sex female Class first TicketPrice 300$ More attributes ... Survived true And the corresponding anchors explanation is: IF SEX = female AND Class = first THEN PREDICT Survived = true WITH PRECISION 97% AND COVERAGE 15% The example shows how anchors can provide essential insights into a model's prediction and its underlying reasoning. The result shows which attributes were taken into account by the model, which in this case, is the female sex and first class. Humans, being paramount for correctness, can use this rule to validate the model's behavior. The anchor additionally tells us that it applies to 15% of perturbation space's instances. In those cases the explanation is 97% accurate, meaning the displayed predicates are almost exclusively responsible for the predicted outcome. An anchor \\(A\\) is formally defined as follows: \\[\\mathbb{E}_{\\mathcal{D}_x(z|A)}[1_{f(x)=f(z)}]\\geq\\tau,A(x)=1\\] Wherein: \\(x\\) represents the instance being explained (e.g., one row in a tabular data set). \\(A\\) is a set of predicates, i.e., the resulting rule or anchor, such that \\(A(x)=1\\) when all feature predicates defined by \\(A\\) correspond to \\(x\\)’s feature values. \\(f\\) denotes the classification model to be explained (e.g., an artificial neural network model). It can be queried to predict a label for \\(x\\) and its perturbations. \\(D_x (\\cdot|A)\\) indicates the distribution of neighbors of \\(x\\), matching \\(A\\). \\(0 \\leq \\tau \\leq 1\\) specifies a precision threshold. Only rules that achieve a local fidelity of at least \\(\\tau\\) are considered a valid result. The formal description may be intimidating and can be put in words: Given an instance \\(x\\) to be explained, a rule or an anchor \\(A\\) is to be found, such that it applies to \\(x\\), while the same class as for \\(x\\) gets predicted for a fraction of at least \\(\\tau\\) of \\(x\\)’s neighbors where the same \\(A\\) is applicable. A rule’s precision results from evaluating neighbors or perturbations (following \\(D_x (z|A)\\)) using the provided machine learning model (denoted by the indicator function \\(1_{f(x) = f(z)}\\)). 5.8.1 Finding Anchors Although anchors’ mathematical description may seem clear and straightforward, constructing particular rules is infeasible. It would require evaluating \\(1_{f(x) = f(z)}\\) for all \\(z \\in \\mathcal{D}_x(\\cdot|A)\\) which is not possible in continuous or large input spaces. Therefore, the authors propose to introduce the parameter \\(0 \\leq \\delta \\leq 1\\) to create a probabilistic definition. This way, samples are drawn until there is statistical confidence concerning their precision. The probabilistic definition reads as follows: \\[P(prec(A)\\geq\\tau)\\geq{}1-\\delta\\quad\\textrm{with}\\quad{}prec(A)=\\mathbb{E}_{\\mathcal{D}_x(z|A)}[1_{f(x)=f(z)}]\\] The previous two definitions are combined and extended by the notion of coverage. Its rationale consists of finding rules that apply to a preferably large part of the model’s input space. Coverage is formally defined as an anchors' probability of applying to its neighbors, i.e., its perturbation space: \\[cov(A)=\\mathbb{E}_{\\mathcal{D}_{(z)}[A(z)]}\\] Including this element leads to anchors' final definition taking into account the maximization of coverage: \\[\\underset{A\\:\\textrm{s.t.}\\;P(prec(A)\\geq\\tau)\\geq{}1-\\delta}{\\textrm{max}}cov(A)\\] Thus, the proceeding strives for a rule that has the highest coverage among all eligible rules (all those that satisfy the precision threshold given the probabilistic definition). These rules are thought to be more important, as they describe a larger part of the model. Note that rules with more predicates tend to have higher precision than rules with fewer predicates. In particular, a rule that fixes every feature of \\(x\\) reduces the evaluated neighborhood to identical instances. Thus, the model classifies all neighbors equally, and the rule’s precision is \\(1\\). At the same time, a rule that fixes many features is overly specific and only applicable to a few instances. Hence, there is a trade-off between precision and coverage. The anchors approach uses four main components to find explanations, as is shown in the figure below. Candidate Generation: Generates new explanation candidates. In the first round, one candidate per feature of \\(x\\) gets created and fixes the respective value of possible perturbations. In every other round, the best candidates of the previous round are extended by one feature predicate that is not yet contained therein. Best Candidate Identification: Candidate rules are to be compared in regard to which rule explains \\(x\\) the best. To this end, perturbations that match the currently observed rule are created evaluated by calling the model. However, these calls need to be minimized as to limit computational overhead. This is why, at the core of this component, there is a pure-exploration Multi-Armed-Bandit (MAB; KL-LUCB39, to be precise). MABs are used to efficiently explore and exploit different strategies (called arms in an analogy to slot machines) using sequential selection. In the given setting, each candidate rule is to be seen as an arm that can be pulled. Each time it is pulled, respective neighbors get evaluated, and we thereby obtain more information about the candidate rule's payoff (precision in anchors' case). The precision thus states how well the rule describes the instance to be explained. Candidate Precision Validation: Takes more samples in case there is no statistical confidence yet that the candidate exceeds the \\(\\tau\\) threshold. Modified Beam Search: All of the above components are assembled in a beam search, which is a graph search algorithm and a variant of the breadth-first algorithm. It carries the \\(B\\) best candidates of each round over to the next one (where \\(B\\) is called the Beam Width). These \\(B\\) best rules are then used to create new rules. The beam search conducts at most \\(featureCount(x)\\) rounds, as each feature can only be included in a rule at most once. Thus, at every round \\(i\\), it generates candidates with exactly \\(i\\) predicates and selects the B best thereof. Therefore, by setting \\(B\\) high, the algorithm more likely avoids local optima. In turn, this requires a high number of model calls and thereby increases the computational load. FIGURE 5.38: The anchors algorithm’s components and their interrelations (simplified) The approach is a seemingly perfect recipe for efficiently deriving statistically sound information about why any system classified an instance the way it did. It systematically experiments with the model’s input and concludes by observing respective outputs. It relies on well established and researched Machine Learning methods (MABs) to reduce the number of calls made to the model. This, in turn, drastically reduces the algorithm’s runtime. 5.8.2 Complexity and Runtime Knowing the anchors approach’s asymptotic runtime behavior helps to evaluate how well it is expected to perform on specific problems. Let \\(B\\) denote the beam width and \\(p\\) the number of all features. Then the anchors algorithm is subject to: \\[\\mathcal{O}(B\\cdot{}p^2+p^2\\cdot\\mathcal{O}_{\\textrm{MAB}\\lbrack{}B\\cdot{}p,B\\rbrack})\\] This boundary abstracts from problem-independent hyperparameters, such as the statistical confidence \\(\\delta\\). Ignoring hyperparameters helps reduce the boundary's complexity (see original paper for more info). Since the MAB extracts the \\(B\\) best out of \\(B \\cdot p\\) candidates in each round, most MABs and their runtimes multiply the \\(p^2\\) factor more than any other parameter. It thus becomes apparent: the algorithm’s efficiency decreases with feature abundant problems. 5.8.3 Tabular Data Example Tabular data is structured data represented by tables, wherein columns embody features and rows instances. For instance, we use the bike rental data to demonstrate the anchors approach's potential to explain ML predictions for selected instances. For this, we turn the regression into a classification problem and train a random forest as our black-box model. It is to classify whether the number of rented bicycles lies above or below the trend line. Before creating anchor explanations, one needs to define a perturbation function. An easy way to do so is to use an intuitive default perturbation space for tabular explanation cases which can be built by sampling from, e.g., the training data. When perturbing an instance, this default approach maintains the features' values that are subject to the anchors' predicates, while replacing the non-fixed features with values taken from another randomly sampled instance with a specified probability. This process yields new instances that are similar to the explained one but have adopted some values from other random instances. Thus, they resemble neighbors. FIGURE 5.39: Anchors explaining six instances of the bike rental dataset. Each row represents one explanation or anchor, and each bar depicts the feature predicates contained by it. The x-axis displays a rule's precision, and a bar's thickness corresponds to its coverage. The 'base' rule contains no predicates. These anchors show that the model mainly considers the temperature for predictions. The results are instinctively interpretable and show for each explained instance, which features are most important for the model's prediction. As the anchors only have a few predicates, they additionally have high coverage and hence apply to other cases. The rules shown above were generated with \\(\\tau = 0.9\\). Thus, we ask for anchors whose evaluated perturbations faithfully support the label with an accuracy of at least \\(90\\%\\). Also, discretization was used to increase the expressiveness and applicability of numerical features. All of the previous rules were generated for instances where the model decides confidently based on a few features. However, other instances are not as distinctly classified by the model as more features are of importance. In such cases, anchors get more specific, comprise more features, and apply to fewer instances. FIGURE 5.40: Explaining instances near decision boundaries leads to specific rules comprising a higher number of feature predicates and lower coverage. Also, the empty rule, i.e., the base feature, gets less important. This can be interpreted as a signal for a decision boundary, as the instance is located in a volatile neighborhood. While choosing the default perturbation space is a comfortable choice to make, it may have a great impact on the algorithm and can thus lead to biased results. For example, if the train set is unbalanced (there is an unequal number of instances of each class), the perturbation space is as well. This condition further affects the rule-finding and the result's precision. The cervical cancer data set is an excellent example of this situation Applying the anchors algorithm leads to one of the following situations: Explaining instances labeled healthy yields empty rules as all generated neighbors evaluate to healthy. Explanations for instances labeled cancer are overly specific, i.e., comprise many feature predicates, since the perturbation space mostly covers values from healthy instances. FIGURE 5.41: Constructing anchors within unbalanced perturbation spaces leads to unexpressive results. This outcome may be unwanted and can be approached in multiple ways. For example, a custom perturbation space can be defined that samples differently, e.g., from an unbalanced data set or a normal distribution. This, however, comes with a side-effect: the sampled neighbors are not representative and change the coverage's scope. Alternatively, we could modify the MAB's confidence \\(\\delta\\) and error parameter values \\(\\epsilon\\). This would cause the MAB to draw more samples, ultimately leading to the minority being sampled more often in absolute terms. For this example, we use a subset of the cervical cancer set in which the majority of cases are labeled cancer. We then have the framework to create a corresponding perturbation space from it. Perturbations are now more likely to lead to varying predictions, and the anchors algorithm can identify important features. However, one needs to take the coverage's definition into account: it is only defined within the perturbation space. In the previous examples, we used the train set as the perturbation space's basis. Since we only use a subset here, a high coverage does not necessarily indicate globally high rule importance. FIGURE 5.42: Balancing the data set before constructing anchors shows the model's reasoning for decisions in minority cases. 5.8.4 Advantages The anchors approach offers multiple advantages over LIME. First, the algorithm’s output is easier to understand, as rules are easy to interpret (even for laypersons). Furthermore, anchors are subsettable and even state a measure of importance by including the notion of coverage. Second, the anchors approach works when model predictions are non-linear or complex in an instance’s neighborhood. As the approach deploys reinforcement learning techniques instead of fitting surrogate models, it is less likely to underfit the model. Apart from that, the algorithm is model-agnostic and thus applicable to any model. Furthermore, it is highly efficient as it can be parallelized by making use of MABs that support batch sampling (e.g., BatchSAR). 5.8.5 Disadvantages The algorithm suffers from a highly configurable and impactful setup, just like most perturbation-based explainers. Not only do hyperparameters such as the beam width or precision threshold need to be tuned to yield meaningful results but also does the perturbation function need to be explicitly designed for one domain/use-case. Think of how tabular data gets perturbed and think of how to apply the same concepts to image data (Hint: these cannot be applied). Luckily, default approaches may be used in some domains (e.g., tabular), facilitating an initial explanation setup. Also, many scenarios require discretization as otherwise results are too specific, have low coverage, and do not contribute to understanding the model. While discretization can help, it may also blur decision boundaries if used carelessly and thus have the exact opposite effect. Since there is no best discretization technique, users need to be aware of the data before deciding on how to discretize data not to obtain poor results. Constructing anchors requires many calls to the ML model, just like all perturbation-based explainers. While the algorithm deploys MABs to minimize the number of calls, its runtime still very much depends on the model’s performance and is therefore highly variable. Lastly, the notion of coverage is undefined in some domains. For example, there is no obvious or universal definition of how superpixels in one image compare to such in other images. 5.8.6 Software and Alternatives There currently are two implementations available: anchor, a Python package (also integrated by Alibi) and a Java implementation. The former is the anchors algorithm's authors' reference and the latter a high-performance implementation which comes with an R interface, called anchors, which was used for the examples in this chapter. As of now, anchors supports tabular data only. However, anchors may theoretically be constructed for any domain or type of data. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. &quot;Anchors: High-Precision Model-Agnostic Explanations.&quot; AAAI Conference on Artificial Intelligence (AAAI), 2018↩ Emilie Kaufmann and Shivaram Kalyanakrishnan. “Information Complexity in Bandit Subset Selection”. Proceedings of Machine Learning Research (2013).↩ "],["shapley.html", "5.9 Shapley Values", " 5.9 Shapley Values A prediction can be explained by assuming that each feature value of the instance is a &quot;player&quot; in a game where the prediction is the payout. Shapley values -- a method from coalitional game theory -- tells us how to fairly distribute the &quot;payout&quot; among the features. 5.9.1 General Idea Assume the following scenario: You have trained a machine learning model to predict apartment prices. For a certain apartment it predicts €300,000 and you need to explain this prediction. The apartment has a size of 50 m2, is located on the 2nd floor, has a park nearby and cats are banned: FIGURE 5.43: The predicted price for a 50 m2 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain how each of these feature values contributed to the prediction. The average prediction for all apartments is €310,000. How much has each feature value contributed to the prediction compared to the average prediction? The answer is simple for linear regression models. The effect of each feature is the weight of the feature times the feature value. This only works because of the linearity of the model. For more complex models, we need a different solution. For example, LIME suggests local models to estimate effects. Another solution comes from cooperative game theory: The Shapley value, coined by Shapley (1953)40, is a method for assigning payouts to players depending on their contribution to the total payout. Players cooperate in a coalition and receive a certain profit from this cooperation. Players? Game? Payout? What is the connection to machine learning predictions and interpretability? The &quot;game&quot; is the prediction task for a single instance of the dataset. The &quot;gain&quot; is the actual prediction for this instance minus the average prediction for all instances. The &quot;players&quot; are the feature values of the instance that collaborate to receive the gain (= predict a certain value). In our apartment example, the feature values park-nearby, cat-banned, area-50 and floor-2nd worked together to achieve the prediction of €300,000. Our goal is to explain the difference between the actual prediction (€300,000) and the average prediction (€310,000): a difference of -€10,000. The answer could be: The park-nearby contributed €30,000; size-50 contributed €10,000; floor-2nd contributed €0; cat-banned contributed -€50,000. The contributions add up to -€10,000, the final prediction minus the average predicted apartment price. How do we calculate the Shapley value for one feature? The Shapley value is the average marginal contribution of a feature value across all possible coalitions. All clear now? In the following figure we evaluate the contribution of the cat-banned feature value when it is added to a coalition of park-nearby and size-50. We simulate that only park-nearby, cat-banned and size-50 are in a coalition by randomly drawing another apartment from the data and using its value for the floor feature. The value floor-2nd was replaced by the randomly drawn floor-1st. Then we predict the price of the apartment with this combination (€310,000). In a second step, we remove cat-banned from the coalition by replacing it with a random value of the cat allowed/banned feature from the randomly drawn apartment. In the example it was cat-allowed, but it could have been cat-banned again. We predict the apartment price for the coalition of park-nearby and size-50 (€320,000). The contribution of cat-banned was €310,000 - €320,000 = -€10.000. This estimate depends on the values of the randomly drawn apartment that served as a &quot;donor&quot; for the cat and floor feature values. We will get better estimates if we repeat this sampling step and average the contributions. FIGURE 5.44: One sample repetition to estimate the contribution of cat-banned to the prediction when added to the coalition of park-nearby and area-50. We repeat this computation for all possible coalitions. The Shapley value is the average of all the marginal contributions to all possible coalitions. The computation time increases exponentially with the number of features. One solution to keep the computation time manageable is to compute contributions for only a few samples of the possible coalitions. The following figure shows all coalitions of feature values that are needed to determine the Shapley value for cat-banned. The first row shows the coalition without any feature values. The second, third and fourth rows show different coalitions with increasing coalition size, separated by &quot;|&quot;. All in all, the following coalitions are possible: No feature values park-nearby size-50 floor-2nd park-nearby+size-50 park-nearby+floor-2nd size-50+floor-2nd park-nearby+size-50+floor-2nd. For each of these coalitions we compute the predicted apartment price with and without the feature value cat-banned and take the difference to get the marginal contribution. The Shapley value is the (weighted) average of marginal contributions. We replace the feature values of features that are not in a coalition with random feature values from the apartment dataset to get a prediction from the machine learning model. FIGURE 5.45: All 8 coalitions needed for computing the exact Shapley value of the cat-banned feature value. If we estimate the Shapley values for all feature values, we get the complete distribution of the prediction (minus the average) among the feature values. 5.9.2 Examples and Interpretation The interpretation of the Shapley value for feature value j is: The value of the j-th feature contributed \\(\\phi_j\\) to the prediction of this particular instance compared to the average prediction for the dataset. The Shapley value works for both classification (if we are dealing with probabilities) and regression. We use the Shapley value to analyze the predictions of a random forest model predicting cervical cancer: FIGURE 5.46: Shapley values for a woman in the cervical cancer dataset. With a prediction of 0.57, this woman's cancer probability is 0.54 above the average prediction of 0.03. The number of diagnosed STDs increased the probability the most. The sum of contributions yields the difference between actual and average prediction (0.54). For the bike rental dataset, we also train a random forest to predict the number of rented bikes for a day, given weather and calendar information. The explanations created for the random forest prediction of a particular day: FIGURE 5.47: Shapley values for day 285. With a predicted 2409 rental bikes, this day is -2108 below the average prediction of 4518. The weather situation and humidity had the largest negative contributions. The temperature on this day had a positive contribution. The sum of Shapley values yields the difference of actual and average prediction (-2108). Be careful to interpret the Shapley value correctly: The Shapley value is the average contribution of a feature value to the prediction in different coalitions. The Shapley value is NOT the difference in prediction when we would remove the feature from the model. 5.9.3 The Shapley Value in Detail This section goes deeper into the definition and computation of the Shapley value for the curious reader. Skip this section and go directly to &quot;Advantages and Disadvantages&quot; if you are not interested in the technical details. We are interested in how each feature affects the prediction of a data point. In a linear model it is easy to calculate the individual effects. Here is what a linear model prediction looks like for one data instance: \\[\\hat{f}(x)=\\beta_0+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\] where x is the instance for which we want to compute the contributions. Each \\(x_j\\) is a feature value, with j = 1,...,p. The \\(\\beta_j\\) is the weight corresponding to feature j. The contribution \\(\\phi_j\\) of the j-th feature on the prediction \\(\\hat{f}(x)\\) is: \\[\\phi_j(\\hat{f})=\\beta_{j}x_j-E(\\beta_{j}X_{j})=\\beta_{j}x_j-\\beta_{j}E(X_{j})\\] where \\(E(\\beta_jX_{j})\\) is the mean effect estimate for feature j. The contribution is the difference between the feature effect minus the average effect. Nice! Now we know how much each feature contributed to the prediction. If we sum all the feature contributions for one instance, the result is the following: \\[\\begin{align*}\\sum_{j=1}^{p}\\phi_j(\\hat{f})=&amp;\\sum_{j=1}^p(\\beta_{j}x_j-E(\\beta_{j}X_{j}))\\\\=&amp;(\\beta_0+\\sum_{j=1}^p\\beta_{j}x_j)-(\\beta_0+\\sum_{j=1}^{p}E(\\beta_{j}X_{j}))\\\\=&amp;\\hat{f}(x)-E(\\hat{f}(X))\\end{align*}\\] This is the predicted value for the data point x minus the average predicted value. Feature contributions can be negative. Can we do the same for any type of model? It would be great to have this as a model-agnostic tool. Since we usually do not have similar weights in other model types, we need a different solution. Help comes from unexpected places: cooperative game theory. The Shapley value is a solution for computing feature contributions for single predictions for any machine learning model. 5.9.3.1 The Shapley Value The Shapley value is defined via a value function val of players in S. The Shapley value of a feature value is its contribution to the payout, weighted and summed over all possible feature value combinations: \\[\\phi_j(val)=\\sum_{S\\subseteq\\{x_{1},\\ldots,x_{p}\\}\\setminus\\{x_j\\}}\\frac{|S|!\\left(p-|S|-1\\right)!}{p!}\\left(val\\left(S\\cup\\{x_j\\}\\right)-val(S)\\right)\\] where S is a subset of the features used in the model, x is the vector of feature values of the instance to be explained and p the number of features. \\(val_x(S)\\) is the prediction for feature values in set S that are marginalized over features that are not included in set S: \\[val_{x}(S)=\\int\\hat{f}(x_{1},\\ldots,x_{p})d\\mathbb{P}_{x\\notin{}S}-E_X(\\hat{f}(X))\\] You actually perform multiple integrations for each feature that is not contained S. A concrete example: The machine learning model works with 4 features x1, x2, x3 and x4 and we evaluate the prediction for the coalition S consisting of feature values x1 and x3: \\[val_{x}(S)=val_{x}(\\{x_{1},x_{3}\\})=\\int_{\\mathbb{R}}\\int_{\\mathbb{R}}\\hat{f}(x_{1},X_{2},x_{3},X_{4})d\\mathbb{P}_{X_2X_4}-E_X(\\hat{f}(X))\\] This looks similar to the feature contributions in the linear model! Do not get confused by the many uses of the word &quot;value&quot;: The feature value is the numerical or categorical value of a feature and instance; the Shapley value is the feature contribution to the prediction; the value function is the payout function for coalitions of players (feature values). The Shapley value is the only attribution method that satisfies the properties Efficiency, Symmetry, Dummy and Additivity, which together can be considered a definition of a fair payout. Efficiency The feature contributions must add up to the difference of prediction for x and the average. \\[\\sum\\nolimits_{j=1}^p\\phi_j=\\hat{f}(x)-E_X(\\hat{f}(X))\\] Symmetry The contributions of two feature values j and k should be the same if they contribute equally to all possible coalitions. If \\[val(S\\cup\\{x_j\\})=val(S\\cup\\{x_k\\})\\] for all \\[S\\subseteq\\{x_{1},\\ldots,x_{p}\\}\\setminus\\{x_j,x_k\\}\\] then \\[\\phi_j=\\phi_{k}\\] Dummy A feature j that does not change the predicted value -- regardless of which coalition of feature values it is added to -- should have a Shapley value of 0. If \\[val(S\\cup\\{x_j\\})=val(S)\\] for all \\[S\\subseteq\\{x_{1},\\ldots,x_{p}\\}\\] then \\[\\phi_j=0\\] Additivity For a game with combined payouts val+val+ the respective Shapley values are as follows: \\[\\phi_j+\\phi_j^{+}\\] Suppose you trained a random forest, which means that the prediction is an average of many decision trees. The Additivity property guarantees that for a feature value, you can calculate the Shapley value for each tree individually, average them, and get the Shapley value for the feature value for the random forest. 5.9.3.2 Intuition An intuitive way to understand the Shapley value is the following illustration: The feature values enter a room in random order. All feature values in the room participate in the game (= contribute to the prediction). The Shapley value of a feature value is the average change in the prediction that the coalition already in the room receives when the feature value joins them. 5.9.3.3 Estimating the Shapley Value All possible coalitions (sets) of feature values have to be evaluated with and without the j-th feature to calculate the exact Shapley value. For more than a few features, the exact solution to this problem becomes problematic as the number of possible coalitions exponentially increases as more features are added. Strumbelj et al. (2014)41 propose an approximation with Monte-Carlo sampling: \\[\\hat{\\phi}_{j}=\\frac{1}{M}\\sum_{m=1}^M\\left(\\hat{f}(x^{m}_{+j})-\\hat{f}(x^{m}_{-j})\\right)\\] where \\(\\hat{f}(x^{m}_{+j})\\) is the prediction for x, but with a random number of feature values replaced by feature values from a random data point z, except for the respective value of feature j. The x-vector \\(x^{m}_{-j}\\) is almost identical to \\(x^{m}_{+j}\\), but the value \\(x_j^{m}\\) is also taken from the sampled z. Each of these M new instances is a kind of &quot;Frankenstein Monster&quot; assembled from two instances. Approximate Shapley estimation for single feature value: Output: Shapley value for the value of the j-th feature Required: Number of iterations M, instance of interest x, feature index j, data matrix X, and machine learning model f For all m = 1,...,M: Draw random instance z from the data matrix X Choose a random permutation o of the feature values Order instance x: \\(x_o=(x_{(1)},\\ldots,x_{(j)},\\ldots,x_{(p)})\\) Order instance z: \\(z_o=(z_{(1)},\\ldots,z_{(j)},\\ldots,z_{(p)})\\) Construct two new instances With feature j: \\(x_{+j}=(x_{(1)},\\ldots,x_{(j-1)},x_{(j)},z_{(j+1)},\\ldots,z_{(p)})\\) Without feature j: \\(x_{-j}=(x_{(1)},\\ldots,x_{(j-1)},z_{(j)},z_{(j+1)},\\ldots,z_{(p)})\\) Compute marginal contribution: \\(\\phi_j^{m}=\\hat{f}(x_{+j})-\\hat{f}(x_{-j})\\) Compute Shapley value as the average: \\(\\phi_j(x)=\\frac{1}{M}\\sum_{m=1}^M\\phi_j^{m}\\) First, select an instance of interest x, a feature j and the number of iterations M. For each iteration, a random instance z is selected from the data and a random order of the features is generated. Two new instances are created by combining values from the instance of interest x and the sample z. The instance \\(x_{+j}\\) is the instance of interest, but all values in the order before feature j are replaced by feature values from the sample z. The instance \\(x_{-j}\\) is the same as \\(x_{+j}\\), but in addition has feature j replaced by the value for feature j from the sample z. The difference in the prediction from the black box is computed: \\[\\phi_j^{m}=\\hat{f}(x^m_{+j})-\\hat{f}(x^m_{-j})\\] All these differences are averaged and result in: \\[\\phi_j(x)=\\frac{1}{M}\\sum_{m=1}^M\\phi_j^{m}\\] Averaging implicitly weighs samples by the probability distribution of X. The procedure has to be repeated for each of the features to get all Shapley values. 5.9.4 Advantages The difference between the prediction and the average prediction is fairly distributed among the feature values of the instance -- the Efficiency property of Shapley values. This property distinguishes the Shapley value from other methods such as LIME. LIME does not guarantee that the prediction is fairly distributed among the features. The Shapley value might be the only method to deliver a full explanation. In situations where the law requires explainability -- like EU's &quot;right to explanations&quot; -- the Shapley value might be the only legally compliant method, because it is based on a solid theory and distributes the effects fairly. I am not a lawyer, so this reflects only my intuition about the requirements. The Shapley value allows contrastive explanations. Instead of comparing a prediction to the average prediction of the entire dataset, you could compare it to a subset or even to a single data point. This contrastiveness is also something that local models like LIME do not have. The Shapley value is the only explanation method with a solid theory. The axioms -- efficiency, symmetry, dummy, additivity -- give the explanation a reasonable foundation. Methods like LIME assume linear behavior of the machine learning model locally, but there is no theory as to why this should work. It is mind-blowing to explain a prediction as a game played by the feature values. 5.9.5 Disadvantages The Shapley value requires a lot of computing time. In 99.9% of real-world problems, only the approximate solution is feasible. An exact computation of the Shapley value is computationally expensive because there are 2k possible coalitions of the feature values and the &quot;absence&quot; of a feature has to be simulated by drawing random instances, which increases the variance for the estimate of the Shapley values estimation. The exponential number of the coalitions is dealt with by sampling coalitions and limiting the number of iterations M. Decreasing M reduces computation time, but increases the variance of the Shapley value. There is no good rule of thumb for the number of iterations M. M should be large enough to accurately estimate the Shapley values, but small enough to complete the computation in a reasonable time. It should be possible to choose M based on Chernoff bounds, but I have not seen any paper on doing this for Shapley values for machine learning predictions. The Shapley value can be misinterpreted. The Shapley value of a feature value is not the difference of the predicted value after removing the feature from the model training. The interpretation of the Shapley value is: Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value. The Shapley value is the wrong explanation method if you seek sparse explanations (explanations that contain few features). Explanations created with the Shapley value method always use all the features. Humans prefer selective explanations, such as those produced by LIME. LIME might be the better choice for explanations lay-persons have to deal with. Another solution is SHAP introduced by Lundberg and Lee (2016)42, which is based on the Shapley value, but can also provide explanations with few features. The Shapley value returns a simple value per feature, but no prediction model like LIME. This means it cannot be used to make statements about changes in prediction for changes in the input, such as: &quot;If I were to earn €300 more a year, my credit score would increase by 5 points.&quot; Another disadvantage is that you need access to the data if you want to calculate the Shapley value for a new data instance. It is not sufficient to access the prediction function because you need the data to replace parts of the instance of interest with values from randomly drawn instances of the data. This can only be avoided if you can create data instances that look like real data instances but are not actual instances from the training data. Like many other permutation-based interpretation methods, the Shapley value method suffers from inclusion of unrealistic data instances when features are correlated. To simulate that a feature value is missing from a coalition, we marginalize the feature. This is achieved by sampling values from the feature's marginal distribution. This is fine as long as the features are independent. When features are dependent, then we might sample feature values that do not make sense for this instance. But we would use those to compute the feature's Shapley value. One solution might be to permute correlated features together and get one mutual Shapley value for them. Another adaptation is conditional sampling: Features are sampled conditional on the features that are already in the team. While conditional sampling fixes the issue of unrealistic data points, a new issue is introduced: The resulting values are no longer the Shapley values to our game, since they violate the symmetry axiom, as found out by Sundararajan et. al (2019)43 and further discussed by Janzing et. al (2020)44. 5.9.6 Software and Alternatives Shapley values are implemented in both the iml and fastshap packages for R. SHAP, an alternative estimation method for Shapley values, is presented in the next chapter. Another approach is called breakDown, which is implemented in the breakDown R package45. BreakDown also shows the contributions of each feature to the prediction, but computes them step by step. Let us reuse the game analogy: We start with an empty team, add the feature value that would contribute the most to the prediction and iterate until all feature values are added. How much each feature value contributes depends on the respective feature values that are already in the &quot;team&quot;, which is the big drawback of the breakDown method. It is faster than the Shapley value method, and for models without interactions, the results are the same. Shapley, Lloyd S. &quot;A value for n-person games.&quot; Contributions to the Theory of Games 2.28 (1953): 307-317.↩ Štrumbelj, Erik, and Igor Kononenko. &quot;Explaining prediction models and individual predictions with feature contributions.&quot; Knowledge and information systems 41.3 (2014): 647-665.↩ Lundberg, Scott M., and Su-In Lee. &quot;A unified approach to interpreting model predictions.&quot; Advances in Neural Information Processing Systems. 2017.↩ Sundararajan, Mukund, and Amir Najmi. &quot;The many Shapley values for model explanation.&quot; arXiv preprint arXiv:1908.08474 (2019).↩ Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. &quot;Feature relevance quantification in explainable AI: A causal problem.&quot; International Conference on Artificial Intelligence and Statistics. PMLR, 2020.↩ Staniak, Mateusz, and Przemyslaw Biecek. &quot;Explanations of model predictions with live and breakDown packages.&quot; arXiv preprint arXiv:1804.01955 (2018).↩ "],["shap.html", "5.10 SHAP (SHapley Additive exPlanations)", " 5.10 SHAP (SHapley Additive exPlanations) This chapter is currently only available in this web version. ebook and print will follow. SHAP (SHapley Additive exPlanations) by Lundberg and Lee (2016)46 is a method to explain individual predictions. SHAP is based on the game theoretically optimal Shapley Values. There are two reasons why SHAP got its own chapter and is not a subchapter of Shapley values. First, the SHAP authors proposed KernelSHAP, an alternative, kernel-based estimation approach for Shapley values inspired by local surrogate models. And they proposed TreeSHAP, an efficient estimation approach for tree-based models. Second, SHAP comes with many global interpretation methods based on aggregations of Shapley values. This chapter explains both the new estimation approaches and the global interpretation methods. I recommend reading the chapters on Shapley values and local models (LIME) first. 5.10.1 Definition The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction. The SHAP explanation method computes Shapley values from coalitional game theory. The feature values of a data instance act as players in a coalition. Shapley values tell us how to fairly distribute the &quot;payout&quot; (= the prediction) among the features. A player can be an individual feature value, e.g. for tabular data. A player can also be a group of feature values. For example to explain an image, pixels can be grouped to super pixels and the prediction distributed among them. One innovation that SHAP brings to the table is that the Shapley value explanation is represented as an additive feature attribution method, a linear model. That view connects LIME and Shapley Values. SHAP specifies the explanation as: \\[g(z&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_jz_j&#39;\\] where g is the explanation model, \\(z&#39;\\in\\{0,1\\}^M\\) is the coalition vector, M is the maximum coalition size and \\(\\phi_j\\in\\mathbb{R}\\) is the feature attribution for a feature j, the Shapley values. What I call &quot;coalition vector&quot; is called &quot;simplified features&quot; in the SHAP paper. I think this name was chosen, because for e.g. image data, the images are not represented on the pixel level, but aggregated to super pixels. I believe it is helpful to think about the z's as describing coalitions: In the coalition vector, an entry of 1 means that the corresponding feature value is &quot;present&quot; and 0 that it is &quot;absent&quot;. This should sound familiar to you if you know about Shapley values. To compute Shapley values, we simulate that only some features values are playing (&quot;present&quot;) and some are not (&quot;absent&quot;). The representation as a linear model of coalitions is a trick for the computation of the \\(\\phi\\)'s. For x, the instance of interest, the coalition vector x' is a vector of all 1's, i.e., all feature values are &quot;present&quot;. The formula simplifies to: \\[g(x&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_j\\] You can find this formula in similar notation in the Shapley value chapter. More about the actual estimation comes later. Let us first talk about the properties of the \\(\\phi\\)'s before we go into the details of their estimation. Shapley values are the only solution that satisfies properties of Efficiency, Symmetry, Dummy and Additivity. SHAP also satisfies these, since it computes Shapley values. In the SHAP paper, you will find discrepancies between SHAP properties and Shapley properties. SHAP describes the following three desirable properties: 1) Local accuracy \\[f(x)=g(x&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_jx_j&#39;\\] If you define \\(\\phi_0=E_X(\\hat{f}(x))\\) and set all \\(x_j&#39;\\) to 1, this is the Shapley efficiency property. Only with a different name and using the coalition vector. \\[f(x)=\\phi_0+\\sum_{j=1}^M\\phi_jx_j&#39;=E_X(\\hat{f}(X))+\\sum_{j=1}^M\\phi_j\\] 2) Missingness \\[x_j&#39;=0\\Rightarrow\\phi_j=0\\] Missingness says that a missing feature gets an attribution of zero. Note that \\(x_j&#39;\\) refers to the coalitions, where a value of 0 represents the absence of a feature value. In coalition notation, all feature values \\(x_j&#39;\\) of the instance to be explained should be '1'. The presence of a 0 would mean that the feature value is missing for the instance of interest. This property is not among the properties of the &quot;normal&quot; Shapley values. So why do we need it for SHAP? Lundberg calls it a &quot;minor book-keeping property&quot;. A missing feature could -- in theory -- have an arbitrary Shapley value without hurting the local accuracy property, since it is multiplied with \\(x_j&#39;=0\\). The Missingness property enforces that missing features get a Shapley value of 0. In practice this is only relevant for features that are constant. 3) Consistency Let \\(f_x(z&#39;)=f(h_x(z&#39;))\\) and \\(z_{\\setminus{}j&#39;}\\) indicate that \\(z_j&#39;=0\\). For any two models f and f' that satisfy: \\[f_x&#39;(z&#39;)-f_x&#39;(z_{\\setminus{}j}&#39;)\\geq{}f_x(z&#39;)-f_x(z_{\\setminus{}j}&#39;)\\] for all inputs \\(z&#39;\\in\\{0,1\\}^M\\), then: \\[\\phi_j(f&#39;,x)\\geq\\phi_j(f,x)\\] The consistency property says that if a model changes so that the marginal contribution of a feature value increases or stays the same (regardless of other features), the Shapley value also increases or stays the same. From Consistency the Shapley properties Linearity, Dummy and Symmetry follow, as described in the Appendix of Lundberg and Lee. 5.10.2 KernelSHAP KernelSHAP estimates for an instance x the contributions of each feature value to the prediction. KernelSHAP consists of 5 steps: Sample coalitions \\(z_k&#39;\\in\\{0,1\\}^M,\\quad{}k\\in\\{1,\\ldots,K\\}\\) (1 = feature present in coalition, 0 = feature absent). Get prediction for each \\(z_k&#39;\\) by first converting \\(z_k&#39;\\) to the original feature space and then applying model f: \\(f(h_x(z_k&#39;))\\) Compute the weight for each \\(z_k&#39;\\) with the SHAP kernel. Fit weighted linear model. Return Shapley values \\(\\phi_k\\), the coefficients from the linear model. We can create a random coalition by repeated coin flips until we have a chain of 0's and 1's. For example, the vector of (1,0,1,0) means that we have a coalition of the first and third features. The K sampled coalitions become the dataset for the regression model. The target for the regression model is the prediction for a coalition. (&quot;Hold on!,&quot; you say, &quot;The model has not been trained on these binary coalition data and can't make predictions for them.&quot;) To get from coalitions of feature values to valid data instances, we need a function \\(h_x(z&#39;)=z\\) where \\(h_x:\\{0,1\\}^M\\rightarrow\\mathbb{R}^p\\). The function \\(h_x\\) maps 1's to the corresponding value from the instance x that we want to explain. For tabular data, it maps 0's to the values of another instance that we sample from the data. This means that we equate &quot;feature value is absent&quot; with &quot;feature value is replaced by random feature value from data&quot;. For tabular data, the following figure visualizes the mapping from coalitions to feature values: FIGURE 5.48: Function \\(h_x\\) maps a coalition to a valid instance. For present features (1), \\(h_x\\) maps to the feature values of x. For absent features (0), \\(h_x\\) maps to the values of a randomly sampled data instance. \\(h_x\\) for tabular data treats \\(X_C\\) and \\(X_S\\) as independent and integrates over the marginal distribution: \\[f(h_x(z&#39;))=E_{X_C}[f(x)]\\] Sampling from the marginal distribution means ignoring the dependence structure between present and absent features. KernelSHAP therefore suffers from the same problem as all permutation-based interpretation methods. The estimation puts too much weight on unlikely instances. Results can become unreliable. But it is necessary to sample from the marginal distribution. If the absent feature values would be sampled from the conditional distribution, then the resulting values are no longer Shapley values. The resulting values would violate the Shapley axiom of Dummy, which says that a feature that does not contribute to the outcome should have a Shapley value of zero. For images, the following figure describes a possible mapping function: FIGURE 5.49: Function \\(h_x\\) maps coalitions of super pixels (sp) to images. Super-pixels are groups of pixels. For present features (1), \\(h_x\\) returns the corresponding part of the original image. For absent features (0), \\(h_x\\) greys out the corresponding area. Assigning the average color of surrounding pixels or similar would also be an option. The big difference to LIME is the weighting of the instances in the regression model. LIME weights the instances according to how close they are to the original instance. The more 0's in the coalition vector, the smaller the weight in LIME. SHAP weights the sampled instances according to the weight the coalition would get in the Shapley value estimation. Small coalitions (few 1's) and large coalitions (i.e. many 1's) get the largest weights. The intuition behind it is: We learn most about individual features if we can study their effects in isolation. If a coalition consists of a single feature, we can learn about the features' isolated main effect on the prediction. If a coalition consists of all but one feature, we can learn about this features' total effect (main effect plus feature interactions). If a coalition consists of half the features, we learn little about an individual features contribution, as there are many possible coalitions with half of the features. To achieve Shapley compliant weighting, Lundberg et. al propose the SHAP kernel: \\[\\pi_{x}(z&#39;)=\\frac{(M-1)}{\\binom{M}{|z&#39;|}|z&#39;|(M-|z&#39;|)}\\] Here, M is the maximum coalition size and \\(|z&#39;|\\) the number of present features in instance z'. Lundberg and Lee show that linear regression with this kernel weight yields Shapley values. If you would use the SHAP kernel with LIME on the coalition data, LIME would also estimate Shapley values! We can be a bit smarter about the sampling of coalitions: The smallest and largest coalitions take up most of the weight. We get better Shapley value estimates by using some of the sampling budget K to include these high-weight coalitions instead of sampling blindly. We start with all possible coalitions with 1 and M-1 features, which makes 2 times M coalitions in total. When we have enough budget left (current budget is K - 2M), we can include coalitions with two features and with M-2 features and so on. From the remaining coalition sizes, we sample with readjusted weights. We have the data, the target and the weights. Everything to build our weighted linear regression model: \\[g(z&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_jz_j&#39;\\] We train the linear model g by optimizing the following loss function L: \\[L(f,g,\\pi_{x})=\\sum_{z&#39;\\in{}Z}[f(h_x(z&#39;))-g(z&#39;)]^2\\pi_{x}(z&#39;)\\] where Z is the training data. This is the good old boring sum of squared errors that we usually optimize for linear models. The estimated coefficients of the model, the \\(\\phi_j\\)'s are the Shapley values. Since we are in a linear regression setting, we can also make use of the standard tools for regression. For example, we can add regularization terms to make the model sparse. If we add an L1 penalty to the loss L, we can create sparse explanations. (I am not so sure whether the resulting coefficients would still be valid Shapley values though) 5.10.3 TreeSHAP Lundberg et. al (2018)47 proposed TreeSHAP, a variant of SHAP for tree-based machine learning models such as decision trees, random forests and gradient boosted trees. TreeSHAP was introduced as a fast, model-specific alternative to KernelSHAP, but it turned out that it can produce unintuitive feature attributions. TreeSHAP defines the value function using the conditional expectation \\(E_{X_S|X_C}(f(x)|x_S)\\) instead of the marginal expectation. The problem with the conditional expectation is that features that have no influence on the prediction function f can get a TreeSHAP estimate different from zero.4849 The non-zero estimate can happen when the feature is correlated with another feature that actually has an influence on the prediction. How much faster is TreeSHAP? Compared to exact KernelSHAP, it reduces the computational complexity from \\(O(TL2^M)\\) to \\(O(TLD^2)\\), where T is the number of trees, L is the maximum number of leaves in any tree and D the maximal depth of any tree. TreeSHAP uses the conditional expectation \\(E_{X_S|X_C}(f(x)|x_S)\\) to estimate effects. I will give you some intuition on how we can compute the expected prediction for a single tree, an instance x and feature subset S. If we conditioned on all features -- if S was the set of all features -- then the prediction from the node in which the instance x falls would be the expected prediction. If we did no condition on any feature -- if S was empty -- we would use the weighted average of predictions of all terminal nodes. If S contains some, but not all, features, we ignore predictions of unreachable nodes. Unreachable means that the decision path that leads to this node contradicts values in \\(x_S\\). From the remaining terminal nodes, we average the predictions weighted by node sizes (i.e. number of training samples in that node). The mean of the remaining terminal nodes, weighted by the number of instances per node, is the expected prediction for x given S. The problem is that we have to apply this procedure for each possible subset S of the feature values. TreeSHAP computes in polynomial time instead of exponential. The basic idea is to push all possible subsets S down the tree at the same time. For each decision node we have to keep track of the number of subsets. This depends on the subsets in the parent node and the split feature. For example, when the first split in a tree is on feature x3, then all the subsets that contain feature x3 will go to one node (the one where x goes). Subsets that do not contain feature x3 go to both nodes with reduced weight. Unfortunately, subsets of different sizes have different weights. The algorithm has to keep track of the overall weight of the subsets in each node. This complicates the algorithm. I refer to the original paper for details of TreeSHAP. The computation can be expanded to more trees: Thanks to the Additivity property of Shapley values, the Shapley values of a tree ensemble is the (weighted) average of the Shapley values of the individual trees. Next, we will look at SHAP explanations in action. 5.10.4 Examples I trained a random forest classifier with 100 trees to predict the risk for cervical cancer. We will use SHAP to explain individual predictions. We can use the fast TreeSHAP estimation method instead of the slower KernelSHAP method, since a random forest is an ensemble of trees. But instead of relying on the conditional distribution, this example uses the marginal distribution. This is described in the package, but not in the original paper. The Python TreeSHAP function is slower with the marginal distribution, but still faster than KernelSHAP, since it scales linearly with the rows in the data. Because we use the marginal distribution here, the interpretation is the same as in the Shapley value chapter. But with the Python shap package comes a different visualization: You can visualize feature attributions such as Shapley values as &quot;forces&quot;. Each feature value is a force that either increases or decreases the prediction. The prediction starts from the baseline. The baseline for Shapley values is the average of all predictions. In the plot, each Shapley value is an arrow that pushes to increase (positive value) or decrease (negative value) the prediction. These forces balance each other out at the actual prediction of the data instance. The following figure shows SHAP explanation force plots for two women from the cervical cancer dataset: FIGURE 5.50: SHAP values to explain the predicted cancer probabilities of two individuals. The baseline -- the average predicted probability -- is 0.066. The first woman has a low predicted risk of 0.06. Risk increasing effects such as STDs are offset by decreasing effects such as age. The second woman has a high predicted risk of 0.71. Age of 51 and 34 years of smoking increase her predicted cancer risk. These were explanations for individual predictions. Shapley values can be combined into global explanations. If we run SHAP for every instance, we get a matrix of Shapley values. This matrix has one row per data instance and one column per feature. We can interpret the entire model by analyzing the Shapley values in this matrix. We start with SHAP feature importance. 5.10.5 SHAP Feature Importance The idea behind SHAP feature importance is simple: Features with large absolute Shapley values are important. Since we want the global importance, we average the absolute Shapley values per feature across the data: \\[I_j=\\sum_{i=1}^n{}|\\phi_j^{(i)}|\\] Next, we sort the features by decreasing importance and plot them. The following figure shows the SHAP feature importance for the random forest trained before for predicting cervical cancer. FIGURE 5.51: SHAP feature importance measured as the mean absolute Shapley values. The number of years with hormonal contraceptives was the most important feature, changing the predicted absolute cancer probability on average by 2.4 percentage points (0.024 on x-axis). SHAP feature importance is an alternative to permutation feature importance. There is a big difference between both importance measures: Permutation feature importance is based on the decrease in model performance. SHAP is based on magnitude of feature attributions. The feature importance plot is useful, but contains no information beyond the importances. For a more informative plot, we will next look at the summary plot. 5.10.6 SHAP Summary Plot The summary plot combines feature importance with feature effects. Each point on the summary plot is a Shapley value for a feature and an instance. The position on the y-axis is determined by the feature and on the x-axis by the Shapley value. The color represents the value of the feature from low to high. Overlapping points are jittered in y-axis direction, so we get a sense of the distribution of the Shapley values per feature. The features are ordered according to their importance. FIGURE 5.52: SHAP summary plot. Low number of years on hormonal contraceptives reduce the predicted cancer risk, a large number of years increases the risk. Your regular reminder: All effects describe the behavior of the model and are not necessarily causal in the real world. In the summary plot, we see first indications of the relationship between the value of a feature and the impact on the prediction. But to see the exact form of the relationship, we have to look at SHAP dependence plots. 5.10.7 SHAP Dependence Plot SHAP feature dependence might be the simplest global interpretation plot: 1) Pick a feature. 2) For each data instance, plot a point with the feature value on the x-axis and the corresponding Shapley value on the y-axis. 3) Done. Mathematically, the plot contains the following points: \\(\\{(x_j^{(i)},\\phi_j^{(i)})\\}_{i=1}^n\\) The following figure shows the SHAP feature dependence for years on hormonal contraceptives: FIGURE 5.53: SHAP dependence plot for years on hormonal contraceptives. Compared to 0 years, a few years lower the predicted probability and a high number of years increases the predicted cancer probability. SHAP dependence plots are an alternative to partial dependence plots and accumulated local effects. While PDP and ALE plot show average effects, SHAP dependence also shows the variance on the y-axis. Especially in case of interactions, the SHAP dependence plot will be much more dispersed in the y-axis. The dependence plot can be improved by highlighting these feature interactions. 5.10.8 SHAP Interaction Values The interaction effect is the additional combined feature effect after accounting for the individual feature effects. The Shapley interaction index from game theory is defined as: \\[\\phi_{i,j}=\\sum_{S\\subseteq\\setminus\\{i,j\\}}\\frac{|S|!(M-|S|-2)!}{2(M-1)!}\\delta_{ij}(S)\\] when \\(i\\neq{}j\\) and: \\[\\delta_{ij}(S)=f_x(S\\cup\\{i,j\\})-f_x(S\\cup\\{i\\})-f_x(S\\cup\\{j\\})+f_x(S)\\] This formula subtracts the main effect of the features so that we get the pure interaction effect after accounting for the individual effects. We average the values over all possible feature coalitions S, as in the Shapley value computation. When we compute SHAP interaction values for all features, we get one matrix per instance with dimensions M x M, where M is the number of features. How can we use the interaction index? For example, to automatically color the SHAP feature dependence plot with the strongest interaction: FIGURE 5.54: SHAP feature dependence plot with interaction visualization. Years on hormonal contraceptives interacts with STDs. In cases close to 0 years, the occurence of an STD increases the predicted cancer risk. For more years on contraceptives, the occurence of an STD reduces the predicted risk. Again, this is not a causal model. Effects might be due to confounding (e.g. STDs and lower cancer risk could be correlated with more doctor visits). 5.10.9 Clustering SHAP values You can cluster your data with the help of Shapley values. The goal of clustering is to find groups of similar instances. Normally, clustering is based on features. Features are often on different scales. For example, height might be measured in meters, color intensity from 0 to 100 and some sensor output between -1 and 1. The difficulty is to compute distances between instances with such different, non-comparable features. SHAP clustering works by clustering on Shapley values of each instance. This means that you cluster instances by explanation similarity. All SHAP values have the same unit -- the unit of the prediction space. You can use any clustering method. The following example uses hierarchical agglomerative clustering to order the instances. The plot consists of many force plots, each of which explains the prediction of an instance. We rotate the force plots vertically and place them side by side according to their clustering similarity. FIGURE 5.55: Stacked SHAP explanations clustered by explanation similarity. Each position on the x-axis is an instance of the data. Red SHAP values increase the prediction, blue values decrease it. A cluster stands out: On the right is a group with a high predicted cancer risk. 5.10.10 Advantages Since SHAP computes Shapley values, all the advantages of Shapley values apply: SHAP has a solid theoretical foundation in game theory. The prediction is fairly distributed among the feature values. We get contrastive explanations that compare the prediction with the average prediction. SHAP connects LIME and Shapley values. This is very useful to better understand both methods. It also helps to unify the field of interpretable machine learning. SHAP has a fast implementation for tree-based models. I believe this was key to the popularity of SHAP, because the biggest barrier for adoption of Shapley values is the slow computation. The fast computation makes it possible to compute the many Shapley values needed for the global model interpretations. The global interpretation methods include feature importance, feature dependence, interactions, clustering and summary plots. With SHAP, global interpretations are consistent with the local explanations, since the Shapley values are the &quot;atomic unit&quot; of the global interpretations. If you use LIME for local explanations and partial dependence plots plus permutation feature importance for global explanations, you lack a common foundation. 5.10.11 Disadvantages KernelSHAP is slow. This makes KernelSHAP impractical to use when you want to compute Shapley values for many instances. Also all global SHAP methods such as SHAP feature importance require computing Shapley values for a lot of instances. KernelSHAP ignores feature dependence. Most other permutation based interpretation methods have this problem. By replacing feature values with values from random instances, it is usually easier to randomly sample from the marginal distribution. However, if features are dependent, e.g. correlated, this leads to putting too much weight on unlikely data points. TreeSHAP solves this problem by explicitly modeling the conditional expected prediction. TreeSHAP can produce unintuitive feature attributions. While TreeSHAP solves the problem of extrapolating to unlikely data points, it introduces a new problem. TreeSHAP changes the value function by relying on the conditional expected prediction. With the change in the value function, features that have no influence on the prediction can get a TreeSHAP value different from zero. The disadvantages of Shapley values also apply to SHAP: Shapley values can be misinterpreted and access to data is needed to compute them for new data (except for TreeSHAP). 5.10.12 Software The authors implemented SHAP in the shap Python package. This implementation works for tree-based models in the scikit-learn machine learning library for Python. The shap package was also used for the examples in this chapter. SHAP is integrated into the tree boosting frameworks xgboost and LightGBM. In R, there is the shapper and fastshap packages. SHAP is also included in the R xgboost package. Lundberg, Scott M., and Su-In Lee. &quot;A unified approach to interpreting model predictions.&quot; Advances in Neural Information Processing Systems. 2017.↩ Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. &quot;Consistent individualized feature attribution for tree ensembles.&quot; arXiv preprint arXiv:1802.03888 (2018).↩ Sundararajan, Mukund, and Amir Najmi. &quot;The many Shapley values for model explanation.&quot; arXiv preprint arXiv:1908.08474 (2019).↩ Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. &quot;Feature relevance quantification in explainable AI: A causality problem.&quot; arXiv preprint arXiv:1910.13413 (2019).↩ "],["example-based.html", "Chapter 6 Example-Based Explanations", " Chapter 6 Example-Based Explanations Example-based explanation methods select particular instances of the dataset to explain the behavior of machine learning models or to explain the underlying data distribution. Example-based explanations are mostly model-agnostic, because they make any machine learning model more interpretable. The difference to model-agnostic methods is that the example-based methods explain a model by selecting instances of the dataset and not by creating summaries of features (such as feature importance or partial dependence). Example-based explanations only make sense if we can represent an instance of the data in a humanly understandable way. This works well for images, because we can view them directly. In general, example-based methods work well if the feature values of an instance carry more context, meaning the data has a structure, like images or texts do. It is more challenging to represent tabular data in a meaningful way, because an instance can consist of hundreds or thousands of (less structured) features. Listing all feature values to describe an instance is usually not useful. It works well if there are only a handful of features or if we have a way to summarize an instance. Example-based explanations help humans construct mental models of the machine learning model and the data the machine learning model has been trained on. It especially helps to understand complex data distributions. But what do I mean by example-based explanations? We often use them in our jobs and daily lives. Let us start with some examples50. A physician sees a patient with an unusual cough and a mild fever. The patient's symptoms remind her of another patient she had years ago with similar symptoms. She suspects that her current patient could have the same disease and she takes a blood sample to test for this specific disease. A data scientist works on a new project for one of his clients: Analysis of the risk factors that lead to the failure of production machines for keyboards. The data scientist remembers a similar project he worked on and reuses parts of the code from the old project because he thinks the client wants the same analysis. A kitten sits on the window ledge of a burning and uninhabited house. The fire department has already arrived and one of the firefighters ponders for a second whether he can risk going into the building to save the kitten. He remembers similar cases in his life as a firefighter: Old wooden houses that have been burning slowly for some time were often unstable and eventually collapsed. Because of the similarity of this case, he decides not to enter, because the risk of the house collapsing is too great. Fortunately, the kitty jumps out of the window, lands safely and nobody is harmed in the fire. Happy end. These stories illustrate how we humans think in examples or analogies. The blueprint of example-based explanations is: Thing B is similar to thing A and A caused Y, so I predict that B will cause Y as well. Implicitly, some machine learning approaches work example-based. Decision trees partition the data into nodes based on the similarities of the data points in the features that are important for predicting the target. A decision tree gets the prediction for a new data instance by finding the instances that are similar (= in the same terminal node) and returning the average of the outcomes of those instances as the prediction. The k-nearest neighbors (knn) method works explicitly with example-based predictions. For a new instance, a knn model locates the k-nearest neighbors (e.g. the k=3 closest instances) and returns the average of the outcomes of those neighbors as a prediction. The prediction of a knn can be explained by returning the k neighbors, which -- again -- is only meaningful if we have a good way to represent a single instance. The chapters in this part cover the following example-based interpretation methods: Counterfactual explanations tell us how an instance has to change to significantly change its prediction. By creating counterfactual instances, we learn about how the model makes its predictions and can explain individual predictions. Adversarial examples are counterfactuals used to fool machine learning models. The emphasis is on flipping the prediction and not explaining it. Prototypes are a selection of representative instances from the data and criticisms are instances that are not well represented by those prototypes. 51 Influential instances are the training data points that were the most influential for the parameters of a prediction model or the predictions themselves. Identifying and analysing influential instances helps to find problems with the data, debug the model and understand the model's behavior better. k-nearest neighbors model: An (interpretable) machine learning model based on examples. Aamodt, Agnar, and Enric Plaza. &quot;Case-based reasoning: Foundational issues, methodological variations, and system approaches.&quot; AI communications 7.1 (1994): 39-59.↩ Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. &quot;Examples are not enough, learn to criticize! Criticism for interpretability.&quot; Advances in Neural Information Processing Systems (2016).↩ "],["counterfactual.html", "6.1 Counterfactual Explanations", " 6.1 Counterfactual Explanations A counterfactual explanation describes a causal situation in the form: &quot;If X had not occurred, Y would not have occurred&quot;. For example: &quot;If I hadn't taken a sip of this hot coffee, I wouldn't have burned my tongue&quot;. Event Y is that I burned my tongue; cause X is that I had a hot coffee. Thinking in counterfactuals requires imagining a hypothetical reality that contradicts the observed facts (e.g. a world in which I have not drunk the hot coffee), hence the name &quot;counterfactual&quot;. The ability to think in counterfactuals makes us humans so smart compared to other animals. In interpretable machine learning, counterfactual explanations can be used to explain predictions of individual instances. The &quot;event&quot; is the predicted outcome of an instance, the &quot;causes&quot; are the particular feature values of this instance that were input to the model and &quot;caused&quot; a certain prediction. Displayed as a graph, the relationship between the inputs and the prediction is very simple: The feature values cause the prediction. FIGURE 6.1: The causal relationships between inputs of a machine learning model and the predictions, when the model is merely seen as a black box. The inputs cause the prediction (not necessarily reflecting the real causal relation of the data). Even if in reality the relationship between the inputs and the outcome to be predicted might not be causal, we can see the inputs of a model as the cause of the prediction. Given this simple graph, it is easy to see how we can simulate counterfactuals for predictions of machine learning models: We simply change the feature values of an instance before making the predictions and we analyze how the prediction changes. We are interested in scenarios in which the prediction changes in a relevant way, like a flip in predicted class (e.g. credit application accepted or rejected) or in which the prediction reaches a certain threshold (e.g. the probability for cancer reaches 10%). A counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output. The counterfactual explanation method is model-agnostic, since it only works with the model inputs and output. This method would also feel at home in the model-agnostic chapter, since the interpretation can be expressed as a summary of the differences in feature values (&quot;change features A and B to change the prediction&quot;). But a counterfactual explanation is itself a new instance, so it lives in this chapter (&quot;starting from instance X, change A and B to get a counterfactual instance&quot;). Unlike prototypes, counterfactuals do not have to be actual instances from the training data, but can be a new combination of feature values. Before discussing how to create counterfactuals, I would like to discuss some use cases for counterfactuals and how a good counterfactual explanation looks like. In this first example, Peter applies for a loan and gets rejected by the (machine learning powered) banking software. He wonders why his application was rejected and how he might improve his chances to get a loan. The question of &quot;why&quot; can be formulated as a counterfactual: What is the smallest change to the features (income, number of credit cards, age, ...) that would change the prediction from rejected to approved? One possible answer could be: If Peter would earn 10,000 Euro more per year, he would get the loan. Or if Peter had fewer credit cards and had not defaulted on a loan 5 years ago, he would get the loan. Peter will never know the reasons for the rejection, as the bank has no interest in transparency, but that is another story. In our second example we want to explain a model that predicts a continuous outcome with counterfactual explanations. Anna wants to rent out her apartment, but she is not sure how much to charge for it, so she decides to train a machine learning model to predict the rent. Of course, since Anna is a data scientist, that is how she solves her problems. After entering all the details about size, location, whether pets are allowed and so on, the model tells her that she can charge 900 Euro. She expected 1000 Euro or more, but she trusts her model and decides to play with the feature values of the apartment to see how she can improve the value of the apartment. She finds out that the apartment could be rented out for over 1000 Euro, if it were 15 m2 larger. Interesting, but non-actionable knowledge, because she cannot enlarge her apartment. Finally, by tweaking only the feature values under her control (built-in kitchen yes/no, pets allowed yes/no, type of floor, etc.), she finds out that if she allows pets and installs windows with better insulation, she can charge 1000 Euro. Anna had intuitively worked with counterfactuals to change the outcome. Counterfactuals are human-friendly explanations, because they are contrastive to the current instance and because they are selective, meaning they usually focus on a small number of feature changes. But counterfactuals suffer from the 'Rashomon effect'. Rashomon is a Japanese movie in which the murder of a Samurai is told by different people. Each of the stories explains the outcome equally well, but the stories contradict each other. The same can also happen with counterfactuals, since there are usually multiple different counterfactual explanations. Each counterfactual tells a different &quot;story&quot; of how a certain outcome was reached. One counterfactual might say to change feature A, the other counterfactual might say to leave A the same but change feature B, which is a contradiction. This issue of multiple truths can be addressed either by reporting all counterfactual explanations or by having a criterion to evaluate counterfactuals and select the best one. Speaking of criteria, how do we define a good counterfactual explanation? First, the user of a counterfactual explanation defines a relevant change in the prediction of an instance (= the alternative reality), so an obvious first requirement is that a counterfactual instance produces the predefined prediction as closely as possible. It is not always possible to match the predefined output exactly. In a classification setting with two classes, a rare class and a frequent class, the model could always classify an instance as the frequent class. Changing the feature values so that the predicted label would flip from the common class to the rare class might be impossible. We therefore want to relax the requirement that the predicted output of the counterfactual must correspond exactly to the defined outcome. In the classification example, we could look for a counterfactual where the predicted probability of the rare class is increased to 10% instead of the current 2%. The question then is, what are the minimum changes to the features so that the predicted probability changes from 2% to 10% (or close to 10%)? Another quality criterion is that a counterfactual should be as similar as possible to the instance regarding feature values. This requires a distance measure between two instances. The counterfactual should not only be close to the original instance, but should also change as few features as possible. This can be achieved by selecting an appropriate distance measure like the Manhattan distance. The last requirement is that a counterfactual instance should have feature values that are likely. It would not make sense to generate a counterfactual explanation for the rent example where the size of an apartment is negative or the number of rooms is set to 200. It is even better when the counterfactual is likely according to the joint distribution of the data, e.g. an apartment with 10 rooms and 20 m2 should not be regarded as counterfactual explanation. 6.1.1 Generating Counterfactual Explanations A simple and naive approach to generating counterfactual explanations is searching by trial and error. This approach involves randomly changing feature values of the instance of interest and stopping when the desired output is predicted. Like the example where Anna tried to find a version of her apartment for which she could charge more rent. But there are better approaches than trial and error. First, we define a loss function that takes as input the instance of interest, a counterfactual and the desired (counterfactual) outcome. The loss measures how far the predicted outcome of the counterfactual is from the predefined outcome and how far the counterfactual is from the instance of interest. We can either optimize the loss directly with an optimization algorithm or by searching around the instance, as suggested in the &quot;Growing Spheres&quot; method (see Software and Alternatives). In this section, I will present the approach suggested by Wachter et. al (2017)52. They suggest minimizing the following loss. \\[L(x,x^\\prime,y^\\prime,\\lambda)=\\lambda\\cdot(\\hat{f}(x^\\prime)-y^\\prime)^2+d(x,x^\\prime)\\] The first term is the quadratic distance between the model prediction for the counterfactual x' and the desired outcome y', which the user must define in advance. The second term is the distance d between the instance x to be explained and the counterfactual x', but more about this later. The parameter \\(\\lambda\\) balances the distance in prediction (first term) against the distance in feature values (second term). The loss is solved for a given \\(\\lambda\\) and returns a counterfactual x'. A higher value of \\(\\lambda\\) means that we prefer counterfactuals that come close to the desired outcome y', a lower value means that we prefer counterfactuals x' that are very similar to x in the feature values. If \\(\\lambda\\) is very large, the instance with the prediction that comes closest to y' will be selected, regardless how far it is away from x. Ultimately, the user must decide how to balance the requirement that the prediction for the counterfactual matches the desired outcome with the requirement that the counterfactual is similar to x. The authors of the method suggest instead of selecting a value for \\(\\lambda\\) to select a tolerance \\(\\epsilon\\) for how far away the prediction of the counterfactual instance is allowed to be from y'. This constraint can be written as: \\[|\\hat{f}(x^\\prime)-y^\\prime|\\leq\\epsilon\\] To minimize this loss function, any suitable optimization algorithm can be used, e.g. Nelder-Mead. If you have access to the gradients of the machine learning model, you can use gradient-based methods like ADAM. The instance x to be explained, the desired output y' and the tolerance parameter \\(\\epsilon\\) must be set in advance. The loss function is minimized for x' and the (locally) optimal counterfactual x' returned while increasing \\(\\lambda\\) until a sufficiently close solution is found (= within the tolerance parameter). \\[\\arg\\min_{x^\\prime}\\max_{\\lambda}L(x,x^\\prime,y^\\prime,\\lambda)\\] The function d for measuring the distance between instance x and counterfactual x' is the Manhattan distance weighted feature-wise with the inverse median absolute deviation (MAD). \\[d(x,x^\\prime)=\\sum_{j=1}^p\\frac{|x_j-x^\\prime_j|}{MAD_j}\\] The total distance is the sum of all p feature-wise distances, that is, the absolute differences of feature values between instance x and counterfactual x'. The feature-wise distances are scaled by the inverse of the median absolute deviation of feature j over the dataset defined as: \\[MAD_j=\\text{median}_{i\\in{}\\{1,\\ldots,n\\}}(|x_{i,j}-\\text{median}_{l\\in{}\\{1,\\ldots,n\\}}(x_{l,j})|)\\] The median of a vector is the value at which half of the vector values are greater and the other half smaller. The MAD is the equivalent of the variance of a feature, but instead of using the mean as the center and summing over the square distances, we use the median as the center and sum over the absolute distances. The proposed distance function has the advantage over the Euclidean distance that it introduces sparsity. This means that two points are closer to each other when fewer features are different. And it is more robust to outliers. Scaling with the MAD is necessary to bring all the features to the same scale -- it should not matter whether you measure the size of an apartment in square meters or square feet. The recipe for producing the counterfactuals is simple: Select an instance x to be explained, the desired outcome y', a tolerance \\(\\epsilon\\) and a (low) initial value for \\(\\lambda\\). Sample a random instance as initial counterfactual. Optimize the loss with the initially sampled counterfactual as starting point. While \\(|\\hat{f}(x^\\prime)-y^\\prime|&gt;\\epsilon\\): Increase \\(\\lambda\\). Optimize the loss with the current counterfactual as starting point. Return the counterfactual that minimizes the loss. Repeat steps 2-4 and return the list of counterfactuals or the one that minimizes the loss. 6.1.2 Examples Both examples are from the work of Wachter et. al (2017). In the first example, the authors train a three-layer fully-connected neural network to predict a student's average grade of the first year at law school, based on grade point average (GPA) prior to law school, race and law school entrance exam scores. The goal is to find counterfactual explanations for each student that answer the following question: How would the input features need to be changed, to get a predicted score of 0? Since the scores have been normalized before, a student with a score of 0 is as good as the average of the students. A negative score means a below-average result, a positive score an above-average result. The following table shows the learned counterfactuals: Score GPA LSAT Race GPA x' LSAT x' Race x' 0.17 3.1 39.0 0 3.1 34.0 0 0.54 3.7 48.0 0 3.7 32.4 0 -0.77 3.3 28.0 1 3.3 33.5 0 -0.83 2.4 28.5 1 2.4 35.8 0 -0.57 2.7 18.3 0 2.7 34.9 0 The first column contains the predicted score, the next 3 columns the original feature values and the last 3 columns the counterfactual feature values that result in a score close to 0. The first two rows are students with above-average predictions, the other three rows below-average. The counterfactuals for the first two rows describe how the student features would have to change to decrease the predicted score and for the other three cases how they would have to change to increase the score to the average. The counterfactuals for increasing the score always change the race from black (coded with 1) to white (coded with 0) which shows a racial bias of the model. The GPA is not changed in the counterfactuals, but LSAT is. The second example shows counterfactual explanations for predicted risk of diabetes. A three-layer fully-connected neural network is trained to predict the risk for diabetes depending on age, BMI, number of pregnancies and so on for women of Pima heritage. The counterfactuals answer the question: Which feature values must be changed to increase or decrease the risk score of diabetes to 0.5? The following counterfactuals were found: Person 1: If your 2-hour serum insulin level was 154.3, you would have a score of 0.51 Person 2: If your 2-hour serum insulin level was 169.5, you would have a score of 0.51 Person 3: If your Plasma glucose concentration was 158.3 and your 2-hour serum insulin level was 160.5, you would have a score of 0.51 6.1.3 Advantages The interpretation of counterfactual explanations is very clear. If the feature values of an instance are changed according to the counterfactual, the prediction changes to the predefined prediction. There are no additional assumptions and no magic in the background. This also means it is not as dangerous as methods like LIME, where it is unclear how far we can extrapolate the local model for the interpretation. The counterfactual method creates a new instance, but we can also summarize a counterfactual by reporting which feature values have changed. This gives us two options for reporting our results. You can either report the counterfactual instance or highlight which features have been changed between the instance of interest and the counterfactual instance. The counterfactual method does not require access to the data or the model. It only requires access to the model's prediction function, which would also work via a web API, for example. This is attractive for companies which are audited by third parties or which are offering explanations for users without disclosing the model or data. A company has an interest in protecting model and data because of trade secrets or data protection reasons. Counterfactual explanations offer a balance between explaining model predictions and protecting the interests of the model owner. The method works also with systems that do not use machine learning. We can create counterfactuals for any system that receives inputs and returns outputs. The system that predicts apartment rents could also consist of handwritten rules, and counterfactual explanations would still work. The counterfactual explanation method is relatively easy to implement, since it is essentially a loss function that can be optimized with standard optimizer libraries. Some additional details must be taken into account, such as limiting feature values to meaningful ranges (e.g. only positive apartment sizes). 6.1.4 Disadvantages For each instance you will usually find multiple counterfactual explanations (Rashomon effect). This is inconvenient -- most people prefer simple explanations over the complexity of the real world. It is also a practical challenge. Let us say we generated 23 counterfactual explanations for one instance. Are we reporting them all? Only the best? What if they are all relatively &quot;good&quot;, but very different? These questions must be answered anew for each project. It can also be advantageous to have multiple counterfactual explanations, because then humans can select the ones that correspond to their previous knowledge. There is no guarantee that for a given tolerance \\(\\epsilon\\) a counterfactual instance is found. That is not necessarily the fault of the method, but rather depends on the data. The proposed method does not handle categorical features with many different levels well. The authors of the method suggested running the method separately for each combination of feature values of the categorical features, but this will lead to a combinatorial explosion if you have multiple categorical features with many values. For example, 6 categorical features with 10 unique levels would mean 1 million runs. A solution for only categorical features was proposed by Martens et. al (2014)53. A solution that handles both numerical and categorical variables with a principled way of generating perturbations for categorical variables is implemented in the Python package Alibi. 6.1.5 Software and Alternatives Counterfactuals explanations are implemented in the Python package Alibi. Authors of the package implement a simple counterfactual method as well as an extended method that uses class prototypes to improve the interpretability and convergence of the algorithm outputs54. A very similar approach was proposed by Martens et. al (2014) for explaining document classifications. In their work, they focus on explaining why a document was or was not classified as a particular class. The difference to the method presented in this chapter is that Martens et. al (2014) focus on text classifiers, which have word occurrences as inputs. An alternative way to search counterfactuals is the Growing Spheres algorithm by Laugel et. al (2017)55. The method first draws a sphere around the point of interest, samples points within that sphere, checks whether one of the sampled points yields the desired prediction, contracts or expands the sphere accordingly until a (sparse) counterfactual is found and finally returned. They do not use the word counterfactual in their paper, but the method is quite similar. They also define a loss function that favors counterfactuals with as few changes in the feature values as possible. Instead of directly optimizing the function, they suggest the above-mentioned search with spheres. Anchors by Ribeiro et. al (2018)56 are the opposite of counterfactuals, see chapter about Scoped Rules (Anchors). Wachter, Sandra, Brent Mittelstadt, and Chris Russell. &quot;Counterfactual explanations without opening the black box: Automated decisions and the GDPR.&quot; (2017).↩ Martens, David, and Foster Provost. &quot;Explaining data-driven document classifications.&quot; (2014).↩ Van Looveren, Arnaud, and Janis Klaise. &quot;Interpretable Counterfactual Explanations Guided by Prototypes.&quot; arXiv preprint arXiv:1907.02584 (2019).↩ Laugel, Thibault, et al. &quot;Inverse classification for comparison-based interpretability in machine learning.&quot; arXiv preprint arXiv:1712.08443 (2017).↩ Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Anchors: High-precision model-agnostic explanations.&quot; AAAI Conference on Artificial Intelligence (2018).↩ "],["adversarial.html", "6.2 Adversarial Examples", " 6.2 Adversarial Examples An adversarial example is an instance with small, intentional feature perturbations that cause a machine learning model to make a false prediction. I recommend reading the chapter about Counterfactual Explanations first, as the concepts are very similar. Adversarial examples are counterfactual examples with the aim to deceive the model, not interpret it. Why are we interested in adversarial examples? Are they not just curious by-products of machine learning models without practical relevance? The answer is a clear &quot;no&quot;. Adversarial examples make machine learning models vulnerable to attacks, as in the following scenarios. A self-driving car crashes into another car because it ignores a stop sign. Someone had placed a picture over the sign, which looks like a stop sign with a little dirt for humans, but was designed to look like a parking prohibition sign for the sign recognition software of the car. A spam detector fails to classify an email as spam. The spam mail has been designed to resemble a normal email, but with the intention of cheating the recipient. A machine-learning powered scanner scans suitcases for weapons at the airport. A knife was developed to avoid detection by making the system think it is an umbrella. Let us take a look at some ways to create adversarial examples. 6.2.1 Methods and Examples There are many techniques to create adversarial examples. Most approaches suggest minimizing the distance between the adversarial example and the instance to be manipulated, while shifting the prediction to the desired (adversarial) outcome. Some methods require access to the gradients of the model, which of course only works with gradient based models such as neural networks, other methods only require access to the prediction function, which makes these methods model-agnostic. The methods in this section focus on image classifiers with deep neural networks, as a lot of research is done in this area and the visualization of adversarial images is very educational. Adversarial examples for images are images with intentionally perturbed pixels with the aim to deceive the model during application time. The examples impressively demonstrate how easily deep neural networks for object recognition can be deceived by images that appear harmless to humans. If you have not yet seen these examples, you might be surprised, because the changes in predictions are incomprehensible for a human observer. Adversarial examples are like optical illusions but for machines. Something is Wrong With My Dog Szegedy et. al (2013)57 used a gradient based optimization approach in their work &quot;Intriguing Properties of Neural Networks&quot; to find adversarial examples for deep neural networks. FIGURE 6.2: Adversarial examples for AlexNet by Szegedy et. al (2013). All images in the left column are correctly classified. The middle column shows the (magnified) error added to the images to produce the images in the right column all categorized (incorrectly) as &quot;Ostrich&quot;. &quot;Intriguing properties of neural networks&quot;, Figure 5 by Szegedy et. al. CC-BY 3.0. These adversarial examples were generated by minimizing the following function with respect to r: \\[loss(\\hat{f}(x+r),l)+c\\cdot|r|\\] In this formula, x is an image (represented as a vector of pixels), r is the changes to the pixels to create an adversarial image (x+r produces a new image), l is the desired outcome class, and the parameter c is used to balance the distance between images and the distance between predictions. The first term is the distance between the predicted outcome of the adversarial example and the desired class l, the second term measures the distance between the adversarial example and the original image. This formulation is almost identical to the loss function to generate counterfactual explanations. There are additional constraints for r so that the pixel values remain between 0 and 1. The authors suggest to solve this optimization problem with a box-constrained L-BFGS, an optimization algorithm that works with gradients. Disturbed panda: Fast gradient sign method Goodfellow et. al (2014)58 invented the fast gradient sign method for generating adversarial images. The gradient sign method uses the gradient of the underlying model to find adversarial examples. The original image x is manipulated by adding or subtracting a small error \\(\\epsilon\\) to each pixel. Whether we add or subtract \\(\\epsilon\\) depends on whether the sign of the gradient for a pixel is positive or negative. Adding errors in the direction of the gradient means that the image is intentionally altered so that the model classification fails. The following formula describes the core of the fast gradient sign method: \\[x^\\prime=x+\\epsilon\\cdot{}sign(\\bigtriangledown_x{}J(\\theta,x,y))\\] where \\(\\bigtriangledown_x{}J\\) is the gradient of the models loss function with respect to the original input pixel vector x, y is the true label vector for x and \\(\\theta\\) is the model parameter vector. From the gradient vector (which is as long as the vector of the input pixels) we only need the sign: The sign of the gradient is positive (+1) if an increase in pixel intensity increases the loss (the error the model makes) and negative (-1) if a decrease in pixel intensity increases the loss. This vulnerability occurs when a neural network treats a relationship between an input pixel intensity and the class score linearly. In particular, neural network architectures that favor linearity, such as LSTMs, maxout networks, networks with ReLU activation units or other linear machine learning algorithms such as logistic regression are vulnerable to the gradient sign method. The attack is carried out by extrapolation. The linearity between the input pixel intensity and the class scores leads to vulnerability to outliers, i.e. the model can be deceived by moving pixel values into areas outside the data distribution. I expected these adversarial examples to be quite specific to a given neural network architecture. But it turns out that you can reuse adversarial examples to deceive networks with a different architecture trained on the same task. Goodfellow et. al (2014) suggested adding adversarial examples to the training data to learn robust models. A jellyfish ... No, wait. A bathtub: 1-pixel attacks The approach presented by Goodfellow and colleagues (2014) requires many pixels to be changed, if only by a little. But what if you can only change a single pixel? Would you be able to deceive a machine learning model? Su et. al (2019) 59 showed that it is actually possible to deceive image classifiers by changing a single pixel. FIGURE 6.3: By intentionally changing a single pixel a neural network trained on ImageNet can be deceived to predict the wrong class instead of the original class. Similar to counterfactuals, the 1-pixel attack looks for a modified example x' which comes close to the original image x, but changes the prediction to an adversarial outcome. However, the definition of closeness differs: Only a single pixel may change. The 1-pixel attack uses differential evolution to find out which pixel is to be changed and how. Differential evolution is loosely inspired by biological evolution of species. A population of individuals called candidate solutions recombines generation by generation until a solution is found. Each candidate solution encodes a pixel modification and is represented by a vector of five elements: the x- and y-coordinates and the red, green and blue (RGB) values. The search starts with, for example, 400 candidate solutions (= pixel modification suggestions) and creates a new generation of candidate solutions (children) from the parent generation using the following formula: \\[x_{i}(g+1)=x_{r1}(g)+F\\cdot(x_{r2}(g)-x_{r3}(g))\\] where each \\(x_i\\) is an element of a candidate solution (either x-coordinate, y-coordinate, red, green or blue), g is the current generation, F is a scaling parameter (set to 0.5) and r1, r2 and r3 are different random numbers. Each new child candidate solution is in turn a pixel with the five attributes for location and color and each of those attributes is a mixture of three random parent pixels. The creation of children is stopped if one of the candidate solutions is an adversarial example, meaning it is classified as an incorrect class, or if the number of maximum iterations specified by the user is reached. Everything is a toaster: Adversarial patch One of my favorite methods brings adversarial examples into physical reality. Brown et. al (2017)60 designed a printable label that can be stuck next to objects to make them look like toasters for an image classifier. Brilliant work! FIGURE 6.4: A sticker that makes a VGG16 classifier trained on ImageNet categorize an image of a banana as a toaster. Work by Brown et. al (2017). This method differs from the methods presented so far for adversarial examples, since the restriction that the adversarial image must be very close to the original image is removed. Instead, the method completely replaces a part of the image with a patch that can take on any shape. The image of the patch is optimized over different background images, with different positions of the patch on the images, sometimes moved, sometimes larger or smaller and rotated, so that the patch works in many situations. In the end, this optimized image can be printed and used to deceive image classifiers in the wild. Never bring a 3D-printed turtle to a gunfight -- even if your computer thinks it is a good idea: Robust adversarial examples The next method is literally adding another dimension to the toaster: Athalye et. al (2017)61 3D-printed a turtle that was designed to look like a rifle to a deep neural network from almost all possible angles. Yeah, you read that right. A physical object that looks like a turtle to humans looks like a rifle to the computer! FIGURE 6.5: Athalye et. al (2017) created a 3D-printed that is recognized as a rifle by TensorFlow’s standard pre-trained InceptionV3 classifier. The authors have found a way to create an adversarial example in 3D for a 2D classifier that is adversarial over transformations, such as all possibilities to rotate the turtle, zoom in and so on. Other approaches such as the fast gradient method no longer work when the image is rotated or viewing angle changes. Athalye et. al (2017) propose the Expectation Over Transformation (EOT) algorithm, which is a method for generating adversarial examples that even work when the image is transformed. The main idea behind EOT is to optimize adversarial examples across many possible transformations. Instead of minimizing the distance between the adversarial example and the original image, EOT keeps the expected distance between the two below a certain threshold, given a selected distribution of possible transformations. The expected distance under transformation can be written as: \\[\\mathbb{E}_{t\\sim{}T}[d(t(x^\\prime),t(x))]\\] where x is the original image, t(x) the transformed image (e.g. rotated), x' the adversarial example and t(x') its transformed version. Apart from working with a distribution of transformations, the EOT method follows the familiar pattern of framing the search for adversarial examples as an optimization problem. We try to find an adversarial example x' that maximizes the probability for the selected class \\(y_t\\) (e.g. &quot;rifle&quot;) across the distribution of possible transformations T: \\[\\arg\\max_{x^\\prime}\\mathbb{E}_{t\\sim{}T}[log{}P(y_t|t(x^\\prime))]\\] With the constraint that the expected distance over all possible transformations between adversarial example x' and original image x remains below a certain threshold: \\[\\mathbb{E}_{t\\sim{}T}[d(t(x^\\prime),t(x))]&lt;\\epsilon\\quad\\text{and}\\quad{}x\\in[0,1]^d\\] I think we should be concerned about the possibilities this method enables. The other methods are based on the manipulation of digital images. However, these 3D-printed, robust adversarial examples can be inserted into any real scene and deceive a computer to wrongly classify an object. Let us turn it around: What if someone creates a rifle which looks like a turtle? The blindfolded adversary: Black box attack Imagine the following scenario: I give you access to my great image classifier via Web API. You can get predictions from the model, but you do not have access to the model parameters. From the convenience of your couch, you can send data and my service answers with the corresponding classifications. Most adversarial attacks are not designed to work in this scenario because they require access to the gradient of the underlying deep neural network to find adversarial examples. Papernot and colleagues (2017)62 showed that it is possible to create adversarial examples without internal model information and without access to the training data. This type of (almost) zero-knowledge attack is called black box attack. How it works: Start with a few images that come from the same domain as the training data, e.g. if the classifier to be attacked is a digit classifier, use images of digits. The knowledge of the domain is required, but not the access to the training data. Get predictions for the current set of images from the black box. Train a surrogate model on the current set of images (for example a neural network). Create a new set of synthetic images using a heuristic that examines for the current set of images in which direction to manipulate the pixels to make the model output have more variance. Repeat steps 2 to 4 for a predefined number of epochs. Create adversarial examples for the surrogate model using the fast gradient method (or similar). Attack the original model with adversarial examples. The aim of the surrogate model is to approximate the decision boundaries of the black box model, but not necessarily to achieve the same accuracy. The authors tested this approach by attacking image classifiers trained on various cloud machine learning services. These services train image classifiers on user uploaded images and labels. The software trains the model automatically -- sometimes with an algorithm unknown to the user -- and deploys it. The classifier then gives predictions for uploaded images, but the model itself cannot be inspected or downloaded. The authors were able to find adversarial examples for various providers, with up to 84% of the adversarial examples being misclassified. The method even works if the black box model to be deceived is not a neural network. This includes machine learning models without gradients such as a decision trees. 6.2.2 The Cybersecurity Perspective Machine learning deals with known unknowns: predicting unknown data points from a known distribution. The defense against attacks deals with unknown unknowns: robustly predicting unknown data points from an unknown distribution of adversarial inputs. As machine learning is integrated into more and more systems, such as autonomous vehicles or medical devices, they are also becoming entry points for attacks. Even if the predictions of a machine learning model on a test dataset are 100% correct, adversarial examples can be found to deceive the model. The defense of machine learning models against cyber attacks is a new part of the field of cybersecurity. Biggio et. al (2018)63 give a nice review of ten years of research on adversarial machine learning, on which this section is based. Cybersecurity is an arms-race in which attackers and defenders outwit each other time and again. There are three golden rules in cybersecurity: 1) know your adversary 2) be proactive and 3) protect yourself. Different applications have different adversaries. People who try to defraud other people via email for their money are adversary agents of users and providers of email services. The providers want to protect their users, so that they can continue using their mail program, the attackers want to get people to give them money. Knowing your adversaries means knowing their goals. Assuming you do not know that these spammers exist and the only abuse of the email service is sending pirated copies of music, then the defense would be different (e.g. scanning the attachments for copyrighted material instead of analyzing the text for spam indicators). Being proactive means actively testing and identifying weak points of the system. You are proactive when you actively try to deceive the model with adversarial examples and then defend against them. Using interpretation methods to understand which features are important and how features affect the prediction is also a proactive step in understanding the weaknesses of a machine learning model. As the data scientist, do you trust your model in this dangerous world without ever having looked beyond the predictive power on a test dataset? Have you analyzed how the model behaves in different scenarios, identified the most important inputs, checked the prediction explanations for some examples? Have you tried to find adversarial inputs? The interpretability of machine learning models plays a major role in cybersecurity. Being reactive, the opposite of proactive, means waiting until the system has been attacked and only then understanding the problem and installing some defensive measures. How can we protect our machine learning systems against adversarial examples? A proactive approach is the iterative retraining of the classifier with adversarial examples, also called adversarial training. Other approaches are based on game theory, such as learning invariant transformations of the features or robust optimization (regularization). Another proposed method is to use multiple classifiers instead of just one and have them vote the prediction (ensemble), but that has no guarantee to work, since they could all suffer from similar adversarial examples. Another approach that does not work well either is gradient masking, which constructs a model without useful gradients by using a nearest neighbor classifier instead of the original model. We can distinguish types of attacks by how much an attacker knows about the system. The attackers may have perfect knowledge (white box attack), meaning they know everything about the model like the type of model, the parameters and the training data; the attackers may have partial knowledge (gray box attack), meaning they might only know the feature representation and the type of model that was used, but have no access to the training data or the parameters; the attackers may have zero knowledge (black box attack), meaning they can only query the model in a black box manner but have no access to the training data or information about the model parameters. Depending on the level of information, the attackers can use different techniques to attack the model. As we have seen in the examples, even in the black box case adversarial examples can be created, so that hiding information about data and model is not sufficient to protect against attacks. Given the nature of the cat-and-mouse game between attackers and defenders, we will see a lot of development and innovation in this area. Just think of the many different types of spam emails that are constantly evolving. New methods of attacks against machine learning models are invented and new defensive measures are proposed against these new attacks. More powerful attacks are developed to evade the latest defenses and so on, ad infinitum. With this chapter I hope to sensitize you to the problem of adversarial examples and that only by proactively studying the machine learning models are we able to discover and remedy weaknesses. Szegedy, Christian, et al. &quot;Intriguing properties of neural networks.&quot; arXiv preprint arXiv:1312.6199 (2013).↩ Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. &quot;Explaining and harnessing adversarial examples.&quot; arXiv preprint arXiv:1412.6572 (2014).↩ Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. &quot;One pixel attack for fooling deep neural networks.&quot; IEEE Transactions on Evolutionary Computation (2019).↩ Brown, Tom B., et al. &quot;Adversarial patch.&quot; arXiv preprint arXiv:1712.09665 (2017).↩ Athalye, Anish, and Ilya Sutskever. &quot;Synthesizing robust adversarial examples.&quot; arXiv preprint arXiv:1707.07397 (2017).↩ Papernot, Nicolas, et al. &quot;Practical black-box attacks against machine learning.&quot; Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017).↩ Biggio, Battista, and Fabio Roli. &quot;Wild Patterns: Ten years after the rise of adversarial machine learning.&quot; Pattern Recognition 84 (2018): 317-331.↩ "],["proto.html", "6.3 Prototypes and Criticisms", " 6.3 Prototypes and Criticisms A prototype is a data instance that is representative of all the data. A criticism is a data instance that is not well represented by the set of prototypes. The purpose of criticisms is to provide insights together with prototypes, especially for data points which the prototypes do not represent well. Prototypes and criticisms can be used independently from a machine learning model to describe the data, but they can also be used to create an interpretable model or to make a black box model interpretable. In this chapter I use the expression &quot;data point&quot; to refer to a single instance, to emphasize the interpretation that an instance is also a point in a coordinate system where each feature is a dimension. The following figure shows a simulated data distribution, with some of the instances chosen as prototypes and some as criticisms. The small points are the data, the large points the criticisms and the large squares the prototypes. The prototypes are selected (manually) to cover the centers of the data distribution and the criticisms are points in a cluster without a prototype. Prototypes and criticisms are always actual instances from the data. FIGURE 6.6: Prototypes and criticisms for a data distribution with two features x1 and x2. I selected the prototypes manually, which does not scale well and probably leads to poor results. There are many approaches to find prototypes in the data. One of these is k-medoids, a clustering algorithm related to the k-means algorithm. Any clustering algorithm that returns actual data points as cluster centers would qualify for selecting prototypes. But most of these methods find only prototypes, but no criticisms. This chapter presents MMD-critic by Kim et. al (2016)64, an approach that combines prototypes and criticisms in a single framework. MMD-critic compares the distribution of the data and the distribution of the selected prototypes. This is the central concept for understanding the MMD-critic method. MMD-critic selects prototypes that minimize the discrepancy between the two distributions. Data points in areas with high density are good prototypes, especially when points are selected from different &quot;data clusters&quot;. Data points from regions that are not well explained by the prototypes are selected as criticisms. Let us delve deeper into the theory. 6.3.1 Theory The MMD-critic procedure on a high-level can be summarized briefly: Select the number of prototypes and criticisms you want to find. Find prototypes with greedy search. Prototypes are selected so that the distribution of the prototypes is close to the data distribution. Find criticisms with greedy search. Points are selected as criticisms where the distribution of prototypes differs from the distribution of the data. We need a couple of ingredients to find prototypes and criticisms for a dataset with MMD-critic. As the most basic ingredient, we need a kernel function to estimate the data densities. A kernel is a function that weighs two data points according to their proximity. Based on density estimates, we need a measure that tells us how different two distributions are so that we can determine whether the distribution of the prototypes we select is close to the data distribution. This is solved by measuring the maximum mean discrepancy (MMD). Also based on the kernel function, we need the witness function to tell us how different two distributions are at a particular data point. With the witness function, we can select criticisms, i.e. data points at which the distribution of prototypes and data diverges and the witness function takes on large absolute values. The last ingredient is a search strategy for good prototypes and criticisms, which is solved with a simple greedy search. Let us start with the maximum mean discrepancy (MMD), which measures the discrepancy between two distributions. The selection of prototypes creates a density distribution of prototypes. We want to evaluate whether the prototypes distribution differs from the data distribution. We estimate both with kernel density functions. The maximum mean discrepancy measures the difference between two distributions, which is the supremum over a function space of differences between the expectations according to the two distributions. All clear? Personally, I understand these concepts much better when I see how something is calculated with data. The following formula shows how to calculate the squared MMD measure (MMD2): \\[MMD^2=\\frac{1}{m^2}\\sum_{i,j=1}^m{}k(z_i,z_j)-\\frac{2}{mn}\\sum_{i,j=1}^{m,n}k(z_i,x_j)+\\frac{1}{n^2}\\sum_{i,j=1}^n{}k(x_i,x_j)\\] k is a kernel function that measures the similarity of two points, but more about this later. m is the number of prototypes z, and n is the number of data points x in our original dataset. The prototypes z are a selection of data points x. Each point is multidimensional, that is it can have multiple features. The goal of MMD-critic is to minimize MMD2. The closer MMD2 is to zero, the better the distribution of the prototypes fits the data. The key to bringing MMD2 down to zero is the term in the middle, which calculates the average proximity between the prototypes and all other data points (multiplied by 2). If this term adds up to the first term (the average proximity of the prototypes to each other) plus the last term (the average proximity of the data points to each other), then the prototypes explain the data perfectly. Try out what would happen to the formula if you used all n data points as prototypes. The following graphic illustrates the MMD2 measure. The first plot shows the data points with two features, whereby the estimation of the data density is displayed with a shaded background. Each of the other plots shows different selections of prototypes, along with the MMD2 measure in the plot titles. The prototypes are the large dots and their distribution is shown as contour lines. The selection of the prototypes that best covers the data in these scenarios (bottom left) has the lowest discrepancy value. FIGURE 6.7: The squared maximum mean discrepancy measure (MMD2) for a dataset with two features and different selections of prototypes. A choice for the kernel is the radial basis function kernel: \\[k(x,x^\\prime)=exp\\left(-\\gamma||x-x^\\prime||^2\\right)\\] where ||x-x'||2 is the Euclidean distance between two points and \\(\\gamma\\) is a scaling parameter. The value of the kernel decreases with the distance between the two points and ranges between zero and one: Zero when the two points are infinitely far apart; one when the two points are equal. We combine the MMD2 measure, the kernel and greedy search in an algorithm for finding prototypes: Start with an empty list of prototypes. While the number of prototypes is below the chosen number m: For each point in the dataset, check how much MMD2 is reduced when the point is added to the list of prototypes. Add the data point that minimizes the MMD2 to the list. Return the list of prototypes. The remaining ingredient for finding criticisms is the witness function, which tells us how much two density estimates differ at a particular point. It can be estimated using: \\[witness(x)=\\frac{1}{n}\\sum_{i=1}^nk(x,x_i)-\\frac{1}{m}\\sum_{j=1}^mk(x,z_j)\\] For two datasets (with the same features), the witness function gives you the means of evaluating in which empirical distribution the point x fits better. To find criticisms, we look for extreme values of the witness function in both negative and positive directions. The first term in the witness function is the average proximity between point x and the data, and, respectively, the second term is the average proximity between point x and the prototypes. If the witness function for a point x is close to zero, the density function of the data and the prototypes are close together, which means that the distribution of prototypes resembles the distribution of the data at point x. A negative witness function at point x means that the prototype distribution overestimates the data distribution (for example if we select a prototype but there are only few data points nearby); a positive witness function at point x means that the prototype distribution underestimates the data distribution (for example if there are many data points around x but we have not selected any prototypes nearby). To give you more intuition, let us reuse the prototypes from the plot beforehand with the lowest MMD2 and display the witness function for a few manually selected points. The labels in the following plot show the value of the witness function for various points marked as triangles. Only the point in the middle has a high absolute value and is therefore a good candidate for a criticism. FIGURE 6.8: Evaluations of the witness function at different points. The witness function allows us to explicitly search for data instances that are not well represented by the prototypes. Criticisms are points with high absolute value in the witness function. Like prototypes, criticisms are also found through greedy search. But instead of reducing the overall MMD2, we are looking for points that maximize a cost function that includes the witness function and a regularizer term. The additional term in the optimization function enforces diversity in the points, which is needed so that the points come from different clusters. This second step is independent of how the prototypes are found. I could also have handpicked some prototypes and used the procedure described here to learn criticisms. Or the prototypes could come from any clustering procedure, like k-medoids. That is it with the important parts of MMD-critic theory. One question remains: How can MMD-critic be used for interpretable machine learning? MMD-critic can add interpretability in three ways: By helping to better understand the data distribution; by building an interpretable model; by making a black box model interpretable. If you apply MMD-critic to your data to find prototypes and criticisms, it will improve your understanding of the data, especially if you have a complex data distribution with edge cases. But with MMD-critic you can achieve more! For example, you can create an interpretable prediction model: a so-called &quot;nearest prototype model&quot;. The prediction function is defined as: \\[\\hat{f}(x)=argmax_{i\\in{}S}k(x,x_i)\\] which means that we select the prototype i from the set of prototypes S that is closest to the new data point, in the sense that it yields the highest value of the kernel function. The prototype itself is returned as an explanation for the prediction. This procedure has three tuning parameters: The type of kernel, the kernel scaling parameter and the number of prototypes. All parameters can be optimized within a cross validation loop. The criticisms are not used in this approach. As a third option, we can use MMD-critic to make any machine learning model globally explainable by examining prototypes and criticisms along with their model predictions. The procedure is as follows: Find prototypes and criticisms with MMD-critic. Train a machine learning model as usual. Predict outcomes for the prototypes and criticisms with the machine learning model. Analyse the predictions: In which cases was the algorithm wrong? Now you have a number of examples that represent the data well and help you to find the weaknesses of the machine learning model. How does that help? Remember when Google's image classifier identified black people as gorillas? Perhaps they should have used the procedure described here before deploying their image recognition model. It is not enough just to check the performance of the model, because if it were 99% correct, this issue could still be in the 1%. And labels can also be wrong! Going through all the training data and performing a sanity check if the prediction is problematic might have revealed the problem, but would be infeasible. But the selection of -- say a few thousand -- prototypes and criticisms is feasible and could have revealed a problem with the data: It might have shown that there is a lack of images of people with dark skin, which indicates a problem with the diversity in the dataset. Or it could have shown one or more images of a person with dark skin as a prototype or (probably) as a criticism with the notorious &quot;gorilla&quot; classification. I do not promise that MMD-critic would certainly intercept these kind of mistakes, but it is a good sanity check. 6.3.2 Examples The following example of MMD-critic uses a handwritten digit dataset. Looking at the actual prototypes, you might notice that the number of images per digit is different. This is because a fixed number of prototypes were searched across the entire dataset and not with a fixed number per class. As expected, the prototypes show different ways of writing the digits. FIGURE 6.9: Prototypes for a handwritten digits dataset. 6.3.3 Advantages In a user study the authors of MMD-critic gave images to the participants, which they had to visually match to one of two sets of images, each representing one of two classes (e.g. two dog breeds). The participants performed best when the sets showed prototypes and criticisms instead of random images of a class. You are free to choose the number of prototypes and criticisms. MMD-critic works with density estimates of the data. This works with any type of data and any type of machine learning model. The algorithm is easy to implement. MMD-critic is very flexible in the way it is used to increase interpretability. It can be used to understand complex data distributions. It can be used to build an interpretable machine learning model. Or it can shed light on the decision making of a black box machine learning model. Finding criticisms is independent of the selection process of the prototypes. But it makes sense to select prototypes according to MMD-critic, because then both prototypes and criticisms are created using the same method of comparing prototypes and data densities. 6.3.4 Disadvantages While, mathematically, prototypes and criticisms are defined differently, their distinction is based on a cut-off value (the number of prototypes). Suppose you choose a too low number of prototypes to cover the data distribution. The criticisms would end up in the areas that are not that well explained. But if you were to add more prototypes they would also end up in the same areas. Any interpretation has to take into account that criticisms strongly depend on the existing prototypes and the (arbitrary) cut-off value for the number of prototypes. You have to choose the number of prototypes and criticisms. As much as this can be nice-to-have, it is also a disadvantage. How many prototypes and criticisms do we actually need? The more the better? The less the better? One solution is to select the number of prototypes and criticisms by measuring how much time humans have for the task of looking at the images, which depends on the particular application. Only when using MMD-critic to build a classifier do we have a way to optimize it directly. One solution could be a screeplot showing the number of prototypes on the x-axis and the MMD2 measure on the y-axis. We would choose the number of prototypes where the MMD2 curve flattens. The other parameters are the choice of the kernel and the kernel scaling parameter. We have the same problem as with the number of prototypes and criticisms: How do we select a kernel and its scaling parameter? Again, when we use MMD-critic as a nearest prototype classifier, we can tune the kernel parameters. For the unsupervised use cases of MMD-critic, however, it is unclear. (Maybe I am a bit harsh here, since all unsupervised methods have this problem.) It takes all the features as input, disregarding the fact that some features might not be relevant for predicting the outcome of interest. One solution is to use only relevant features, for example image embeddings instead of raw pixels. This works as long as we have a way to project the original instance onto a representation that contains only relevant information. There is some code available, but it is not yet implemented as nicely packaged and documented software. 6.3.5 Code and Alternatives An implementation of MMD-critic can be found here: https://github.com/BeenKim/MMD-critic. The simplest alternative to finding prototypes is k-medoids by Kaufman et. al (1987).65 Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. &quot;Examples are not enough, learn to criticize! Criticism for interpretability.&quot; Advances in Neural Information Processing Systems (2016).↩ Kaufman, Leonard, and Peter Rousseeuw. &quot;Clustering by means of medoids&quot;. North-Holland (1987).↩ "],["influential.html", "6.4 Influential Instances", " 6.4 Influential Instances Machine learning models are ultimately a product of training data and deleting one of the training instances can affect the resulting model. We call a training instance &quot;influential&quot; when its deletion from the training data considerably changes the parameters or predictions of the model. By identifying influential training instances, we can &quot;debug&quot; machine learning models and better explain their behaviors and predictions. This chapter shows you two approaches for identifying influential instances, namely deletion diagnostics and influence functions. Both approaches are based on robust statistics, which provides statistical methods that are less affected by outliers or violations of model assumptions. Robust statistics also provides methods to measure how robust estimates from data are (such as a mean estimate or the weights of a prediction model). Imagine you want to estimate the average income of the people in your city and ask ten random people on the street how much they earn. Apart from the fact that your sample is probably really bad, how much can your average income estimate be influenced by a single person? To answer this question, you can recalculate the mean value by omitting individual answers or derive mathematically via &quot;influence functions&quot; how the mean value can be influenced. With the deletion approach, we recalculate the mean value ten times, omitting one of the income statements each time, and measure how much the mean estimate changes. A big change means that an instance was very influential. The second approach upweights one of the persons by an infinitesimally small weight, which corresponds to the calculation of the first derivative of a statistic or model. This approach is also known as &quot;infinitesimal approach&quot; or &quot;influence function&quot;. The answer is, by the way, that your mean estimate can be very strongly influenced by a single answer, since the mean scales linearly with single values. A more robust choice is the median (the value at which one half of people earn more and the other half less), because even if the person with the highest income in your sample would earn ten times more, the resulting median would not change. Deletion diagnostics and influence functions can also be applied to the parameters or predictions of machine learning models to understand their behavior better or to explain individual predictions. Before we look at these two approaches for finding influential instances, we will examine the difference between an outlier and an influential instance. Outlier An outlier is an instance that is far away from the other instances in the dataset. &quot;Far away&quot; means that the distance, for example the Euclidean distance, to all the other instances is very large. In a dataset of newborns, a newborn weighting 6 kg would be considered an outlier. In a dataset of bank accounts with mostly checking accounts, a dedicated loan account (large negative balance, few transactions) would be considered an outlier. The following figure shows an outlier for a 1-dimensional distribution. FIGURE 6.10: Feature x follows a Gaussian distribution with an outlier at x=8. Outliers can be interesting data points (e.g. criticisms). When an outlier influences the model it is also an influential instance. Influential instance An influential instance is a data instance whose removal has a strong effect on the trained model. The more the model parameters or predictions change when the model is retrained with a particular instance removed from the training data, the more influential that instance is. Whether an instance is influential for a trained model also depends on its value for the target y. The following figure shows an influential instance for a linear regression model. FIGURE 6.11: A linear model with one feature. Trained once on the full data and once without the influential instance. Removing the influential instance changes the fitted slope (weight/coefficient) drastically. Why do influential instances help to understand the model? The key idea behind influential instances for interpretability is to trace model parameters and predictions back to where it all began: the training data. A learner, that is, the algorithm that generates the machine learning model, is a function that takes training data consisting of features X and target y and generates a machine learning model. For example, the learner of a decision tree is an algorithm that selects the split features and the values at which to split. A learner for a neural network uses backpropagation to find the best weights. FIGURE 6.12: A learner learns a model from training data (features plus target). The model makes predictions for new data. We ask how the model parameters or the predictions would change if we removed instances from the training data in the training process. This is in contrast to other interpretability approaches that analyze how the prediction changes when we manipulate the features of the instances to be predicted, such as partial dependence plots or feature importance. With influential instances, we do not treat the model as fixed, but as a function of the training data. Influential instances help us answer questions about global model behavior and about individual predictions. Which were the most influential instances for the model parameters or the predictions overall? Which were the most influential instances for a particular prediction? Influential instances tell us for which instances the model could have problems, which training instances should be checked for errors and give an impression of the robustness of the model. We might not trust a model if a single instance has a strong influence on the model predictions and parameters. At least that would make us investigate further. How can we find influential instances? We have two ways of measuring influence: Our first option is to delete the instance from the training data, retrain the model on the reduced training dataset and observe the difference in the model parameters or predictions (either individually or over the complete dataset). The second option is to upweight a data instance by approximating the parameter changes based on the gradients of the model parameters. The deletion approach is easier to understand and motivates the upweighting approach, so we start with the former. 6.4.1 Deletion Diagnostics Statisticians have already done a lot of research in the area of influential instances, especially for (generalized) linear regression models. When you search for &quot;influential observations&quot;, the first search results are about measures like DFBETA and Cook's distance. DFBETA measures the effect of deleting an instance on the model parameters. Cook's distance (Cook, 197766) measures the effect of deleting an instance on model predictions. For both measures we have to retrain the model repeatedly, omitting individual instances each time. The parameters or predictions of the model with all instances is compared with the parameters or predictions of the model with one of the instances deleted from the training data. DFBETA is defined as: \\[DFBETA_{i}=\\beta-\\beta^{(-i)}\\] where \\(\\beta\\) is the weight vector when the model is trained on all data instances, and \\(\\beta^{(-i)}\\) the weight vector when the model is trained without instance i. Quite intuitive I would say. DFBETA works only for models with weight parameters, such as logistic regression or neural networks, but not for models such as decision trees, tree ensembles, some support vector machines and so on. Cook's distance was invented for linear regression models and approximations for generalized linear regression models exist. Cook's distance for a training instance is defined as the (scaled) sum of the squared differences in the predicted outcome when the i-th instance is removed from the model training. \\[D_i=\\frac{\\sum_{j=1}^n(\\hat{y}_j-\\hat{y}_{j}^{(-i)})^2}{p\\cdot{}MSE}\\] where the numerator is the squared difference between prediction of the model with and without the i-th instance, summed over the dataset. The denominator is the number of features p times the mean squared error. The denominator is the same for all instances no matter which instance i is removed. Cook's distance tells us how much the predicted output of a linear model changes when we remove the i-th instance from the training. Can we use Cook's distance and DFBETA for any machine learning model? DFBETA requires model parameters, so this measure works only for parameterized models. Cook's distance does not require any model parameters. Interestingly, Cook's distance is usually not seen outside the context of linear models and generalized linear models, but the idea of taking the difference between model predictions before and after removal of a particular instance is very general. A problem with the definition of Cook's distance is the MSE, which is not meaningful for all types of prediction models (e.g. classification). The simplest influence measure for the effect on the model predictions can be written as follows: \\[\\text{Influence}^{(-i)}=\\frac{1}{n}\\sum_{j=1}^{n}\\left|\\hat{y}_j-\\hat{y}_{j}^{(-i)}\\right|\\] This expression is basically the numerator of Cook's distance, with the difference that the absolute difference is added up instead of the squared differences. This was a choice I made, because it makes sense for the examples later. The general form of deletion diagnostic measures consists of choosing a measure (such as the predicted outcome) and calculating the difference of the measure for the model trained on all instances and when the instance is deleted. We can easily break the influence down to explain for the prediction of instance j what the influence of the i-th training instance was: \\[\\text{Influence}_{j}^{(-i)}=\\left|\\hat{y}_j-\\hat{y}_{j}^{(-i)}\\right|\\] This would also work for the difference in model parameters or the difference in the loss. In the following example we will use these simple influence measures. Deletion diagnostics example In the following example, we train a support vector machine to predict cervical cancer given risk factors and measure which training instances were most influential overall and for a particular prediction. Since the prediction of cancer is a classification problem, we measure the influence as the difference in predicted probability for cancer. An instance is influential if the predicted probability strongly increases or decreases on average in the dataset when the instance is removed from model training. The measurement of the influence for all 858 training instances requires to train the model once on all data and retrain it 858 times (= size of training data) with one of the instances removed each time. The most influential instance has an influence measure of about 0.01. An influence of 0.01 means that if we remove the 540-th instance, the predicted probability changes by 1 percentage point on average. This is quite substantial considering the average predicted probability for cancer is 6.4%. The mean value of influence measures over all possible deletions is 0.2 percentage points. Now we know which of the data instances were most influential for the model. This is already useful to know for debugging the data. Is there a problematic instance? Are there measurement errors? The influential instances are the first ones that should be checked for errors, because each error in them strongly influences the model predictions. Apart from model debugging, can we learn something to better understand the model? Just printing out the top 10 most influential instances is not very useful, because it is just a table of instances with many features. All methods that return instances as output only make sense if we have a good way of representing them. But we can better understand what kind of instances are influential when we ask: What distinguishes an influential instance from a non-influential instance? We can turn this question into a regression problem and model the influence of an instance as a function of its feature values. We are free to choose any model from the chapter on Interpretable Machine Learning Models. For this example I chose a decision tree (following figure) that shows that data from women of age 35 and older were the most influential for the support vector machine. Of all the women in the dataset 153 out of 858 were older than 35. In the chapter on Partial Dependence Plots we have seen that after 40 there is a sharp increase in the predicted probability of cancer and the Feature Importance has also detected age as one of the most important features. The influence analysis tells us that the model becomes increasingly unstable when predicting cancer for higher ages. This in itself is valuable information. This means that errors in these instances can have a strong effect on the model. FIGURE 6.13: A decision tree that models the relationship between the influence of the instances and their features. The maximum depth of the tree is set to 2. This first influence analysis revealed the overall most influential instance. Now we select one of the instances, namely the 7-th instance, for which we want to explain the prediction by finding the most influential training data instances. It is like a counterfactual question: How would the prediction for instance 7 change if we omit instance i from the training process? We repeat this removal for all instances. Then we select the training instances that result in the biggest change in the prediction of instance 7 when they are omitted from the training and use them to explain the prediction of the model for that instance. I chose to explain the prediction for instance 7 because it is the instance with the highest predicted probability of cancer (7.35%), which I thought was an interesting case to analyze more deeply. We could return the, say, top 10 most influential instances for predicting the 7-th instance printed as a table. Not very useful, because we could not see much. Again, it makes more sense to find out what distinguishes the influential instances from the non-influential instances by analyzing their features. We use a decision tree trained to predict the influence given the features, but in reality we misuse it only to find a structure and not to actually predict something. The following decision tree shows which kind of training instances were most influential for predicting the 7-th instance. FIGURE 6.14: Decision tree that explains which instances were most influential for predicting the 7-th instance. Data from women who smoked for 18.5 years or longer had a large influence on the prediction of the 7-th instance, with an average change in absolute prediction by 11.7 percentage points of cancer probability. Data instances of women who smoked or have been smoking for 18.5 years or longer have a high influence on the prediction of the 7-th instance. The woman behind the 7-th instance smoked for 34 years. In the data, 12 women (1.40%) smoked 18.5 years or longer. Any mistake made in collecting the number of years of smoking of one of these women will have a huge impact on the predicted outcome for the 7-th instance. The most extreme change in the prediction happens when we remove instance number 663. The patient allegedly smoked for 22 years, aligned with the results from the decision tree. The predicted probability for the 7-th instance changes from 7.35% to 66.60% if we remove the instance 663! If we take a closer look at the features of the most influential instance, we can see another possible problem. The data say that the woman is 28 years old and has been smoking for 22 years. Either it is a really extreme case and she really started smoking at 6, or this is a data error. I tend to believe the latter. This is certainly a situation in which we must question the accuracy of the data. These examples showed how useful it is to identify influential instances to debug models. One problem with the proposed approach is that the model needs to be retrained for each training instance. The whole retraining can be quite slow, because if you have thousands of training instances, you will have to retrain your model thousands of times. Assuming the model takes one day to train and you have 1000 training instances, then the computation of influential instances -- without parallelization -- will take almost 3 years. Nobody has time for this. In the rest of this chapter, I will show you a method that does not require retraining the model. 6.4.2 Influence Functions You: I want to know the influence a training instance has on a particular prediction. Research: You can delete the training instance, retrain the model, and measure the difference in the prediction. You: Great! But do you have a method for me that works without retraining? It takes so much time. Research: Do you have a model with a loss function that is twice differentiable with respect to its parameters? You: I trained a neural network with the logistic loss. So yes. Research: Then you can approximate the influence of the instance on the model parameters and on the prediction with influence functions. The influence function is a measure of how strongly the model parameters or predictions depend on a training instance. Instead of deleting the instance, the method upweights the instance in the loss by a very small step. This method involves approximating the loss around the current model parameters using the gradient and Hessian matrix. Loss upweighting is similar to deleting the instance. You: Great, that's what I'm looking for! Koh and Liang (2017)67 suggested using influence functions, a method of robust statistics, to measure how an instance influences model parameters or predictions. As with deletion diagnostics, the influence functions trace the model parameters and predictions back to the responsible training instance. However, instead of deleting training instances, the method approximates how much the model changes when the instance is upweighted in the empirical risk (sum of the loss over the training data). The method of influence functions requires access to the loss gradient with respect to the model parameters, which only works for a subset of machine learning models. Logistic regression, neural networks and support vector machines qualify, tree-based methods like random forests do not. Influence functions help to understand the model behavior, debug the model and detect errors in the dataset. The following section explains the intuition and math behind influence functions. Math behind influence functions The key idea behind influence functions is to upweight the loss of a training instance by an infinitesimally small step \\(\\epsilon\\), which results in new model parameters: \\[\\hat{\\theta}_{\\epsilon,z}=\\arg\\min_{\\theta{}\\in\\Theta}(1-\\epsilon)\\frac{1}{n}\\sum_{i=1}^n{}L(z_i,\\theta)+\\epsilon{}L(z,\\theta)\\] where \\(\\theta\\) is the model parameter vector and \\(\\hat{\\theta}_{\\epsilon,z}\\) is the parameter vector after upweighting z by a very small number \\(\\epsilon\\). L is the loss function with which the model was trained, \\(z_i\\) is the training data and z is the training instance which we want to upweight to simulate its removal. The intuition behind this formula is: How much will the loss change if we upweight a particular instance \\(z_i\\) from the training data by a little (\\(\\epsilon\\)) and downweight the other data instances accordingly? What would the parameter vector look like to optimize this new combined loss? The influence function of the parameters, i.e. the influence of upweighting training instance z on the parameters, can be calculated as follows. \\[I_{\\text{up,params}}(z)=\\left.\\frac{d{}\\hat{\\theta}_{\\epsilon,z}}{d\\epsilon}\\right|_{\\epsilon=0}=-H_{\\hat{\\theta}}^{-1}\\nabla_{\\theta}L(z,\\hat{\\theta})\\] The last expression \\(\\nabla_{\\theta}L(z,\\hat{\\theta})\\) is the loss gradient with respect to the parameters for the upweighted training instance. The gradient is the rate of change of the loss of the training instance. It tells us how much the loss changes when we change the model parameters \\(\\hat{\\theta}\\) by a bit. A positive entry in the gradient vector means that a small increase in the corresponding model parameter increases the loss, a negative entry means that the increase of the parameter reduces the loss. The first part \\(H^{-1}_{\\hat{\\theta}}\\) is the inverse Hessian matrix (second derivative of the loss with respect to the model parameters). The Hessian matrix is the rate of change of the gradient, or expressed as loss, it is the rate of change of the rate of change of the loss. It can be estimated using: \\[H_{\\theta}=\\frac{1}{n}\\sum_{i=1}^n\\nabla^2_{\\hat{\\theta}}L(z_i,\\hat{\\theta})\\] More informally: The Hessian matrix records how curved the loss is at a certain point. The Hessian is a matrix and not just a vector, because it describes the curvature of the loss and the curvature depends on the direction in which we look. The actual calculation of the Hessian matrix is time-consuming if you have many parameters. Koh and Liang suggested some tricks to calculate it efficiently, which goes beyond the scope of this chapter. Updating the model parameters, as described by the above formula, is equivalent to taking a single Newton step after forming a quadratic expansion around the estimated model parameters. What intuition is behind this influence function formula? The formula comes from forming a quadratic expansion around the parameters \\(\\hat{\\theta}\\). That means we do not actually know, or it is too complex to calculate how exactly the loss of instance z will change when it is removed/upweighted. We approximate the function locally by using information about the steepness (= gradient) and the curvature (= Hessian matrix) at the current model parameter setting. With this loss approximation, we can calculate what the new parameters would approximately look like if we upweighted instance z: \\[\\hat{\\theta}_{-z}\\approx\\hat{\\theta}-\\frac{1}{n}I_{\\text{up,params}}(z)\\] The approximate parameter vector is basically the original parameter minus the gradient of the loss of z (because we want to decrease the loss) scaled by the curvature (= multiplied by the inverse Hessian matrix) and scaled by 1 over n, because that is the weight of a single training instance. The following figure shows how the upweighting works. The x-axis shows the value of the \\(\\theta\\) parameter and the y-axis the corresponding value of the loss with upweighted instance z. The model parameter here is 1-dimensional for demonstration purposes, but in reality it is usually high-dimensional. We move only 1 over n into the direction of improvement of the loss for instance z. We do not know how the loss would really change when we delete z, but with the first and second derivative of the loss, we create this quadratic approximation around our current model parameter and pretend that this is how the real loss would behave. FIGURE 6.15: Updating the model parameter (x-axis) by forming a quadratic expansion of the loss around the current model parameter, and moving 1/n into the direction in which the loss with upweighted instance z (y-axis) improves most. This upweighting of instance z in the loss approximates the parameter changes if we delete z and train the model on the reduced data. We do not actually need to calculate the new parameters, but we can use the influence function as a measure of the influence of z on the parameters. How do the predictions change when we upweight training instance z? We can either calculate the new parameters and then make predictions using the newly parameterized model, or we can also calculate the influence of instance z on the predictions directly, since we can calculate the influence by using the chain rule: \\[\\begin{align*}I_{up,loss}(z,z_{test})&amp;=\\left.\\frac{d{}L(z_{test},\\hat{\\theta}_{\\epsilon,z})}{d\\epsilon}\\right|_{\\epsilon=0}\\\\&amp;=\\left.\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T\\frac{d\\hat{\\theta}_{\\epsilon,z}}{d\\epsilon}\\right|_{\\epsilon=0}\\\\&amp;=-\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T{}H^{-1}_{\\theta}\\nabla_{\\theta}L(z,\\hat{\\theta})\\end{align*}\\] The first line of this equation means that we measure the influence of a training instance on a certain prediction \\(z_{test}\\) as a change in loss of the test instance when we upweight the instance z and get new parameters \\(\\hat{\\theta}_{\\epsilon,z}\\). For the second line of the equation, we have applied the chain rule of derivatives and get the derivative of the loss of the test instance with respect to the parameters times the influence of z on the parameters. In the third line, we replace the expression with the influence function for the parameters. The first term in the third line \\(\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T{}\\) is the gradient of the test instance with respect to the model parameters. Having a formula is great and the scientific and accurate way of showing things. But I think it is very important to get some intuition about what the formula means. The formula for \\(I_{\\text{up,loss}}\\) states that the influence function of the training instance z on the prediction of an instance \\(z_{test}\\) is &quot;how strongly the instance reacts to a change of the model parameters&quot; multiplied by &quot;how much the parameters change when we upweight the instance z&quot;. Another way to read the formula: The influence is proportional to how large the gradients for the training and test loss are. The higher the gradient of the training loss, the higher its influence on the parameters and the higher the influence on the test prediction. The higher the gradient of the test prediction, the more influenceable the test instance. The entire construct can also be seen as a measure of the similarity (as learned by the model) between the training and the test instance. That is it with theory and intuition. The next section explains how influence functions can be applied. Application of Influence Functions Influence functions have many applications, some of which have already been presented in this chapter. Understanding model behavior Different machine learning models have different ways of making predictions. Even if two models have the same performance, the way they make predictions from the features can be very different and therefore fail in different scenarios. Understanding the particular weaknesses of a model by identifying influential instances helps to form a &quot;mental model&quot; of the machine learning model behavior in your mind. Handling domain mismatches / Debugging model errors Handling domain mismatch is closely related to better understand the model behavior. Domain mismatch means that the distribution of training and test data is different, which can cause the model to perform poorly on the test data. Influence functions can identify training instances that caused the error. Suppose you have trained a prediction model for the outcome of patients who have undergone surgery. All these patients come from the same hospital. Now you use the model in another hospital and see that it does not work well for many patients. Of course, you assume that the two hospitals have different patients, and if you look at their data, you can see that they differ in many features. But what are the features or instances that have &quot;broken&quot; the model? Here too, influential instances are a good way to answer this question. You take one of the new patients, for whom the model has made a false prediction, find and analyze the most influential instances. For example, this could show that the second hospital has older patients on average and the most influential instances from the training data are the few older patients from the first hospital and the model simply lacked the data to learn to predict this subgroup well. The conclusion would be that the model needs to be trained on more patients who are older in order to work well in the second hospital. Fixing training data If you have a limit on how many training instances you can check for correctness, how do you make an efficient selection? The best way is to select the most influential instances, because -- by definition -- they have the most influence on the model. Even if you would have an instance with obviously incorrect values, if the instance is not influential and you only need the data for the prediction model, it is a better choice to check the influential instances. For example, you train a model for predicting whether a patient should remain in hospital or be discharged early. You really want to make sure that the model is robust and makes correct predictions, because a wrong release of a patient can have bad consequences. Patient records can be very messy, so you do not have perfect confidence in the quality of the data. But checking patient information and correcting it can be very time-consuming, because once you have reported which patients you need to check, the hospital actually needs to send someone to look at the records of the selected patients more closely, which might be handwritten and lying in some archive. Checking data for a patient could take an hour or more. In view of theses costs, it makes sense to check only a few important data instances. The best way is to select patients who have had a high influence on the prediction model. Koh and Liang (2017) showed that this type of selection works much better than random selection or the selection of those with the highest loss or wrong classification. 6.4.3 Advantages of Identifying Influential Instances The approaches of deletion diagnostics and influence functions are very different from the mostly feature-perturbation based approaches presented in the Model-Agnostic chapter. A look at influential instances emphasizes the role of training data in the learning process. This makes influence functions and deletion diagnostics one of the best debugging tools for machine learning models. Of the techniques presented in this book, they are the only ones that directly help to identify the instances which should be checked for errors. Deletion diagnostics are model-agnostic, meaning the approach can be applied to any model. Also influence functions based on the derivatives can be applied to a broad class of models. We can use these methods to compare different machine learning models and better understand their different behaviors, going beyond comparing only the predictive performance. We have not talked about this topic in this chapter, but influence functions via derivatives can also be used to create adversarial training data. These are instances that are manipulated in such a way that the model cannot predict certain test instances correctly when the model is trained on those manipulated instances. The difference to the methods in the Adversarial Examples chapter is that the attack takes place during training time, also known as poisoning attacks. If you are interested, read the paper by Koh and Liang (2017). For deletion diagnostics and influence functions, we considered the difference in the prediction and for the influence function the increase of the loss. But, really, the approach is generalizable to any question of the form: &quot;What happens to ... when we delete or upweight instance z?&quot;, where you can fill &quot;...&quot; with any function of your model of your desire. You can analyze how much a training instance influences the overall loss of the model. You can analyze how much a training instance influences the feature importance. You can analyze how much a training instance influences which feature is selected for the first split when training a decision tree. 6.4.4 Disadvantages of Identifying Influential Instances Deletion diagnostics are very expensive to calculate because they require retraining. But history has shown that computer resources are constantly increasing. A calculation that 20 years ago was unthinkable in terms of resources can easily be performed with your smartphone. You can train models with thousands of training instances and hundreds of parameters on a laptop in seconds/minutes. It is therefore not a big leap to assume that deletion diagnostics will work without problems even with large neural networks in 10 years. Influence functions are a good alternative to deletion diagnostics, but only for models with differentiable parameters, such as neural networks. They do not work for tree-based methods like random forests, boosted trees or decision trees. Even if you have models with parameters and a loss function, the loss may not be differentiable. But for the last problem, there is a trick: Use a differentiable loss as substitute for calculating the influence when, for example, the underlying model uses the Hinge loss instead of some differentiable loss. The loss is replaced by a smoothed version of the problematic loss for the influence functions, but the model can still be trained with the non-smooth loss. Influence functions are only approximate, because the approach forms a quadratic expansion around the parameters. The approximation can be wrong and the influence of an instance is actually higher or lower when removed. Koh and Liang (2017) showed for some examples that the influence calculated by the influence function was close to the influence measure obtained when the model was actually retrained after the instance was deleted. But there is no guarantee that the approximation will always be so close. There is no clear cutoff of the influence measure at which we call an instance influential or non-influential. It is useful to sort the instances by influence, but it would be great to have the means not only to sort the instances, but actually to distinguish between influential and non-influential. For example, if you identify the top 10 most influential training instances for a test instance, some of them may not be influential because, for example, only the top 3 were really influential. The influence measures only take into account the deletion of individual instances and not the deletion of several instances at once. Larger groups of data instances may have some interactions that strongly influence model training and prediction. But the problem lies in combinatorics: There are n possibilities to delete an individual instance from the data. There are n times (n-1) possibilities to delete two instances from the training data. There are n times (n-1) times (n-2) possibilities to delete three ... I guess you can see where this is going, there are just too many combinations. 6.4.5 Software and Alternatives Deletion diagnostics are very simple to implement. Take a look at the code I wrote for the examples in this chapter. For linear models and generalized linear models many influence measures like Cook's distance are implemented in R in the stats package. Koh and Liang published the Python code for influence functions from their paper in a repository. That is great! Unfortunately it is &quot;only&quot; the code of the paper and not a maintained and documented Python module. The code is focused on the Tensorflow library, so you cannot use it directly for black box models using other frameworks, like sci-kit learn. Keita Kurita wrote a great blog post for influence functions that helped me understand Koh and Liang's paper better. The blog post goes a little deeper into the mathematics behind influence functions for black box models and also talks about some of the mathematical 'tricks' with which the method is efficiently implemented. Cook, R. Dennis. &quot;Detection of influential observation in linear regression.&quot; Technometrics 19.1 (1977): 15-18.↩ Koh, Pang Wei, and Percy Liang. &quot;Understanding black-box predictions via influence functions.&quot; arXiv preprint arXiv:1703.04730 (2017).↩ "],["neural-networks.html", "Chapter 7 Neural Network Interpretation", " Chapter 7 Neural Network Interpretation This chapter is currently only available in this web version. ebook and print will follow. The following chapters focus on interpretation methods for neural networks. The methods visualize features and concepts learned by a neural network, explain individual predictions and simplify neural networks. Deep learning has been very successful, especially in tasks that involve images and texts such as image classification and language translation. The success story of deep neural networks began in 2012, when the ImageNet image classification challenge 68 was won by a deep learning approach. Since then, we have witnessed a Cambrian explosion of deep neural network architectures, with a trend towards deeper networks with more and more weight parameters. To make predictions with a neural network, the data input is passed through many layers of multiplication with the learned weights and through non-linear transformations. A single prediction can involve millions of mathematical operations depending on the architecture of the neural network. There is no chance that we humans can follow the exact mapping from data input to prediction. We would have to consider millions of weights that interact in a complex way to understand a prediction by a neural network. To interpret the behavior and predictions of neural networks, we need specific interpretation methods. The chapters assume that you are familiar with deep learning, including convolutional neural networks. We can certainly use model-agnostic methods, such as local models or partial dependence plots, but there are two reasons why it makes sense to consider interpretation methods developed specifically for neural networks: First, neural networks learn features and concepts in their hidden layers and we need special tools to uncover them. Second, the gradient can be utilized to implement interpretation methods that are more computationally efficient than model-agnostic methods that look at the model &quot;from the outside&quot;. Also most other methods in this book are intended for the interpretation of models for tabular data. Image and text data require different methods. The next chapters cover the following topics: Feature Visualization: What features has the neural network learned? Adversarial Examples from the Example-Based Explanations chapter are closely related to feature visualization : How can we manipulate the inputs to get a wrong classification? Concepts (IN PROGRESS): Which more abstract concepts has the neural network learned? Feature Attribution (IN PROGRESS): How did each input contribute to a particular prediction? Modell Distillation (IN PROGRESS): How can we explain a neural network with a simpler model? Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015↩ "],["cnn-features.html", "7.1 Learned Features", " 7.1 Learned Features This chapter is currently only available in this web version. ebook and print will follow. Convolutional neural networks learn abstract features and concepts from raw image pixels. Feature Visualization visualizes the learned features by activation maximization. Network Dissection labels neural network units (e.g. channels) with human concepts. Deep neural networks learn high-level features in the hidden layers. This is one of their greatest strengths and reduces the need for feature engineering. Assume you want to build an image classifier with a support vector machine. The raw pixel matrices are not the best input for training your SVM, so you create new features based on color, frequency domain, edge detectors and so on. With convolutional neural networks, the image is fed into the network in its raw form (pixels). The network transforms the image many times. First, the image goes through many convolutional layers. In those convolutional layers, the network learns new and increasingly complex features in its layers. Then the transformed image information goes through the fully connected layers and turns into a classification or prediction. FIGURE 7.1: Features learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from simple features in the lower convolutional layers (left) to more abstract features in the higher convolutional layers (right). Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/. The first convolutional layer(s) learn features such as edges and simple textures. Later convolutional layers learn features such as more complex textures and patterns. The last convolutional layers learn features such as objects or parts of objects. The fully connected layers learn to connect the activations from the high-level features to the individual classes to be predicted. Cool. But how do we actually get those hallucinatory images? 7.1.1 Feature Visualization The approach of making the learned features explicit is called Feature Visualization. Feature visualization for a unit of a neural network is done by finding the input that maximizes the activation of that unit. &quot;Unit&quot; refers either to individual neurons, channels (also called feature maps), entire layers or the final class probability in classification (or the corresponding pre-softmax neuron, which is recommended). Individual neurons are atomic units of the network, so we would get the most information by creating feature visualizations for each neuron. But there is a problem: Neural networks often contain millions of neurons. Looking at each neuron's feature visualization would take too long. The channels (sometimes called activation maps) as units are a good choice for feature visualization. We can go one step further and visualize an entire convolutional layer. Layers as a unit are used for Google's DeepDream, which repeatedly adds the visualized features of a layer to the original image, resulting in a dream-like version of the input. FIGURE 7.2: Feature visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron (or corresponding pre-softmax neuron) 7.1.1.1 Feature Visualization through Optimization In mathematical terms, feature visualization is an optimization problem. We assume that the weights of the neural network are fixed, which means that the network is trained. We are looking for a new image that maximizes the (mean) activation of a unit, here a single neuron: \\[img^*=\\arg\\max_{img}h_{n,x,y,z}(img)\\] The function \\(h\\) is the activation of a neuron, img the input of the network (an image), x and y describe the spatial position of the neuron, n specifies the layer and z is the channel index. For the mean activation of an entire channel z in layer n we maximize: \\[img^*=\\arg\\max_{img}\\sum_{x,y}h_{n,x,y,z}(img)\\] In this formula, all neurons in channel z are equally weighted. Alternatively, you can also maximize random directions, which means that the neurons would be multiplied by different parameters, including negative directions. In this way, we study how the neurons interact within the channel. Instead of maximizing the activation, you can also minimize the activation (which corresponds to maximizing the negative direction). Interestingly, when you maximize the negative direction you get very different features for the same unit: FIGURE 7.3: Positive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something which seems to have eyes yields a negative activation. Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb We can address this optimization problem in different ways. First, why should we generate new images? We could simply search through our training images and select those that maximize the activation. This is a valid approach, but using training data has the problem that elements on the images can be correlated and we can't see what the neural network is really looking for. If images that yield a high activation of a certain channel show a dog and a tennis ball, we don't know whether the neural network looks at the dog, the tennis ball or maybe at both. The other approach is to generate new images, starting from random noise. To obtain meaningful visualizations, there are usually constraints on the image, e.g. that only small changes are allowed. To reduce noise in the feature visualization, you can apply jittering, rotation or scaling to the image before the optimization step. Other regularization options include frequency penalization (e.g. reduce variance of neighboring pixels) or generating images with learned priors, e.g. with generative adversarial networks (GANs) 69 or denoising autoencoders 70. FIGURE 7.4: Iterative optimization from random image to maximizing activation. Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/. If you want to dive a lot deeper into feature visualization, take a look at the distill.pub online journal, especially the feature visualization post by Olah et al. 71, from which I used many of the images, and also about the building blocks of interpretability 72. 7.1.1.2 Connection to Adversarial Examples There is a connection between feature visualization and adversarial examples: Both techniques maximize the activation of a neural network unit. For adversarial examples, we look for the maximum activation of the neuron for the adversarial (= incorrect) class. One difference is the image we start with: For adversarial examples, it is the image for which we want to generate the adversarial image. For feature visualization it is, depending on the approach, random noise. 7.1.1.3 Text and Tabular Data The literature focuses on feature visualization for convolutional neural networks for image recognition. Technically, there is nothing to stop you from finding the input that maximally activates a neuron of a fully connected neural network for tabular data or a recurrent neural network for text data. You might not call it feature visualization any longer, since the &quot;feature&quot; would be a tabular data input or text. For credit default prediction, the inputs might be the number of prior credits, number of mobile contracts, address and dozens of other features. The learned feature of a neuron would then be a certain combination of the dozens of features. For recurrent neural networks, it is a bit nicer to visualize what the network learned: Karpathy et. al (2015)73 showed that recurrent neural networks indeed have neurons that learn interpretable features. They trained a character-level model, which predicts the next character in the sequence from the previous characters. Once an opening brace &quot;(&quot; occurred, one of the neurons got highly activated, and got de-activated when the matching closing bracket &quot;)&quot; occurred. Other neurons fired at the end of a line. Some neurons fired in URLs. The difference to the feature visualization for CNNs is that the examples were not found through optimization, but by studying neuron activations in the training data. Some of the images seem to show well-known concepts like dog snouts or buildings. But how can we be sure? The Network Dissection method links human concepts with individual neural network units. Spoiler alert: Network Dissection requires extra datasets that someone has labeled with human concepts. 7.1.2 Network Dissection The Network Dissection approach by Bau &amp; Zhou et al. (2017) 74 quantifies the interpretability of a unit of a convolutional neural network. It links highly activated areas of CNN channels with human concepts (objects, parts, textures, colors, ...). The channels of a convolutional neural network learn new features, as we saw in the chapter on Feature Visualization. But these visualizations do not prove that a unit has learned a certain concept. We also do not have a measure for how well a unit detects e.g. sky scrapers. Before we go into the details of Network Dissection, we have to talk about the big hypothesis that is behind that line of research. The hypothesis is: Units of a neural network (like convolutional channels) learn disentangled concepts. The Question of Disentangled Features Do (convolutional) neural networks learn disentangled features? Disentangled features mean that individual network units detect specific real world concepts. Convolutional channel 394 might detect sky scrapers, channel 121 dog snouts, channel 12 stripes at 30 degree angle ... The opposite of a disentangled network is a completely entangled network. In a completely entangled network, for example, there would be no individual unit for dog snouts. All channels would contribute to the recognition of dog snouts. Disentangled features imply that the network is highly interpretable. Let us assume we have a network with completely disentangled units that are labeled with known concepts. This would open up the possibility to track the network's decision making process. For example, we could analyze how the network classifies wolves against huskeys. First, we identify the &quot;huskey&quot;-unit. We can check whether this unit depends on the &quot;dog snout&quot;, &quot;fluffy fur&quot; and &quot;snow&quot;-units from the previous layer. If it does, we know that it will misclassify an image of a huskey with a snowy background as a wolf. In a disentangled network, we could identify problematic non-causal correlations. We could automatically list all highly activated units and their concepts to explain an individual prediction. We could easily detect bias in the neural network. For example, did the network learn a &quot;white skin&quot; feature to predict salary? Spoiler-alert: Convolutional neural networks are not perfectly disentangled. We will now look more closely at Network Dissection to find out how interpretable neural networks are. 7.1.2.1 Network Dissection Algorithm Network Dissection has three steps: Get images with human-labeled visual concepts, from stripes to skyscrapers. Measure the CNN channel activations for these images. Quantify the alignment of activations and labeled concepts. The following figure visualizes how an image is forwarded to a channel and matched with the labeled concepts. FIGURE 7.5: For a given input image and a trained network (fixed weights), we forward propagate the image up to the target layer, upscale the activations to match the original image size and compare the maximum activations with the ground truth pixel-wise segmentation. Figure originally from Bau &amp; Zhou et. al (2017). Step 1: Broden dataset The first difficult but crucial step is data collection. Network Dissection requires pixel-wise labeled images with concepts of different abstraction levels (from colors to street scenes). Bau &amp; Zhou et. al combined a couple of datasets with pixel-wise concepts. They called this new dataset 'Broden', which stands for broadly and densely labeled data. The Broden dataset is segmented to the pixel level mostly, for some datasets the whole image is labeled. Broden contains 60,000 images with over 1,000 visual concepts in different abstraction levels: 468 scenes, 585 objects, 234 parts, 32 materials, 47 textures and 11 colors. The following figure shows sample images from the Broden dataset. FIGURE 7.6: Example images from the Broden dataset. Figure originally from Bau &amp; Zhou et. al (2017). Step 2: Retrieve network activations Next, we create the masks of the top activated areas per channel and per image. At this point the concept labels are not yet involved. For each convolutional channel k: For each image x in the Broden dataset Forward propagate image x to the target layer containing channel k. Extract the pixel activations of convolutional channel k: \\(A_k(x)\\) Calculate distribution of pixel activations \\(\\alpha_k\\) over all images Determine the 0.005-quantile level \\(T_k\\) of activations \\(\\alpha_k\\). This means 0.5% of all activations of channel k for image x are greater than \\(T_k\\). For each image x in the Broden dataset: Scale the (possibly) lower-resolution activation map \\(A_k(x)\\) to the resolution of image x. We call the result \\(S_k(x)\\). Binarize the activation map: A pixel is either on or off, depending on whether it exceeds the activation threshold \\(T_k\\). The new mask is \\(M_k(x)=S_k(x)\\geq{}T_k(x)\\). Step 3: Activation-concept alignment After step 2 we have one activation mask per channel and image. These activation masks mark highly activated areas. For each channel we want to find the human concept that activates that channel. We find the concept by comparing the activation masks with all labeled concepts. We quantify the alignment between activation mask k and concept mask c with the Intersection over Union (IoU) score: \\[IoU_{k,c}=\\frac{\\sum|M_k(x)\\bigcap{}L_c(x)|}{\\sum|M_k(x)\\bigcup{}L_c(x)|}\\] where \\(|\\cdot|\\) is the cardinality of a set. Intersection over union compares the alignment between two areas. \\(IoU_{k,c}\\) can be interpreted as the accuracy with which unit k detects concept c. We call unit k a detector of concept c when \\(IoU_{k,c}&gt;0.04\\). This threshold was chosen by Bau &amp; Zhou et. al. The following figure illustrates intersection and union of activation mask and concept mask for a single image: FIGURE 7.7: The Intersection over Union (IoU) is computed by comparing the human ground truth annotation and the top activated pixels. The following figure shows a unit that detects dogs: FIGURE 7.8: Activation mask for inception_4e channel 750 which detects dogs with \\(IoU=0.203\\). Figure originally from Bau &amp; Zhou et. al (2017). 7.1.2.2 Experiments The Network Dissection authors trained different network architectures (AlexNet, VGG, GoogleNet, ResNet) from scratch on different datasets (ImageNet, Places205, Places365). ImageNet contains 1.6 million images from 1000 classes that focus on objects. Places205 and Places365 contain 2.4 million / 1.6 million images from 205 / 365 different scenes. They additionally trained AlexNet on self-supervised training tasks such as predicting video frame order or colorizing images. For many of these different settings, they counted the number of unique concept detectors as a measure of interpretability. Here are some of the findings: The networks detect lower-level concepts (colors, textures) at lower layers and higher-level concepts (parts, objects) at higher layers. We have already seen this in the Feature Visualizations. Batch normalization reduces the number of unique concept detectors. Many units detect the same concept. For example, there are 95 (!) dog channels in VGG trained on ImageNet when using \\(IoU \\geq 0.04\\) as detection cutoff (4 in conv4_3, 91 in conv5_3, see project website). Increasing the number of channels in a layer increases the number of interpretable units. Random initializations (training with different random seeds) result in slightly different numbers of interpretable units. ResNet is the network architecture with the largest number of unique detectors, followed by VGG, GoogleNet and AlexNet last. The largest number of unique concept detectors are learned for Places356, followed by Places205 and ImageNet last. The number of unique concept detectors increases with the number of training iterations. FIGURE 7.9: ResNet trained on Places365 has the highest number of unique detectors. AlexNet with random weights has the lowest number of unique detectors and serves as baseline. Figure originally from Bau &amp; Zhou et. al (2017). Networks trained on self-supervised tasks have fewer unique detectors compared to networks trained on supervised tasks. In transfer learning, the concept of a channel can change. For example, a dog detector became a waterfall detector. This happened in a model that was initially trained to classify objects and then fine-tuned to classify scenes. In one of the experiments, the authors projected the channels onto a new rotated basis. This was done for the VGG network trained on ImageNet. &quot;Rotated&quot; does not mean that the image was rotated. &quot;Rotated&quot; means that we take the 256 channels from the conv5 layer and compute new 256 channels as linear combinations of the original channels. In the process, the channels get entangled. Rotation reduces interpretability, i.e., the number of channels aligned with a concept decreases. The rotation was designed to keep the performance of the model the same. The first conclusion: Interpretability of CNNs is axis-dependent. This means that random combinations of channels are less likely to detect unique concepts. The second conclusion: Interpretability is independent of discriminative power. The channels can be transformed with orthogonal transformations while the discriminative power remains the same, but interpretability decreases. FIGURE 7.10: The number of unique concept detectors decreases when the 256 channels of AlexNet conv5 (trained on ImageNet) are gradually changed to a basis using a random orthogonal transformation. Figure originally from Bau &amp; Zhou et. al (2017). The authors also used Network Dissection for Generative Adversarial Networks (GANs). You can find Network Dissection for GANs on the project's website. 7.1.3 Advantages Feature visualizations give unique insight into the working of neural networks, especially for image recognition. Given the complexity and opacity of neural networks, feature visualization is an important step in analyzing and describing neural networks. Through feature visualization, we have learned that neural networks first learn simple edge and texture detectors and more abstract part and object detectors in higher layers. Network dissection expands those insights and makes interpretability of network units measurable. Network dissection allows us to automatically link units to concepts, which is very convenient. Feature visualization is a great tool to communicate in a non-technical way how neural networks work. With network dissection, we can also detect concepts beyond the classes in the classification task. But we need datasets that contain images with pixel-wise labeled concepts. Feature visualization can be combined with feature attribution methods, which explain which pixels were important for the classification. The combination of both methods allows to explain an individual classification along with local visualization of the learned features that were involved in the classification. See The Building Blocks of Interpretability from distill.pub. Finally, feature visualizations make great desktop wallpapers and T-Shirts prints. 7.1.4 Disadvantages Many feature visualization images are not interpretable at all, but contain some abstract features for which we have no words or mental concept. The display of feature visualizations along with training data can help. The images still might not reveal what the neural network reacted to and only show something like &quot;maybe there has to be something yellow in the images&quot;. Even with Network Dissection some channels are not linked to a human concept. For example, layer conv5_3 from VGG trained on ImageNet has 193 channels (out of 512) that could not be matched with a human concept. There are too many units to look at, even when &quot;only&quot; visualizing the channel activations. For Inception V1 there are already over 5000 channels from 9 convolutional layers. If you also want to show the negative activations plus a few images from the training data that maximally or minimally activate the channel (let's say 4 positive, 4 negative images), then you must already display more than 50 000 images. At least we know -- thanks to Network Dissection -- that we do not need to investigate random directions. Illusion of Interpretability? The feature visualizations can convey the illusion that we understand what the neural network is doing. But do we really understand what is going on in the neural network? Even if we look at hundreds or thousands of feature visualizations, we cannot understand the neural network. The channels interact in a complex way, positive and negative activations are unrelated, multiple neurons might learn very similar features and for many of the features we do not have equivalent human concepts. We must not fall into the trap of believing we fully understand neural networks just because we believe we saw that neuron 349 in layer 7 is activated by daisies. Network Dissection showed that architectures like ResNet or Inception have units that react to certain concepts. But the IoU is not that great and often many units respond to the same concept and some to no concept at all. They channels are not completely disentangled and we cannot interpret them in isolation. For Network Dissection, you need datasets that are labeled on the pixel level with the concepts. These datasets take a lot of effort to collect, since each pixel needs to be labeled, which usually works by drawing segments around objects on the image. Network Dissection only aligns human concepts with positive activations but not with negative activations of channels. As the feature visualizations showed, negative activations seem to be linked to concepts. This might be fixed by additionally looking at the lower quantile of activations. 7.1.5 Software and Further Material There is an open-source implementation of feature visualization called Lucid. You can conveniently try it in your browser by using the notebook links that are provided on the Lucid Github page. No additional software is required. Other implementations are tf_cnnvis for TensorFlow, Keras Filters for Keras and DeepVis for Caffe. Network Dissection has a great project website. Next to the publication, the website hosts additional material such as code, data and visualizations of activation masks. Nguyen, Anh, et al. &quot;Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.&quot; Advances in Neural Information Processing Systems. 2016.↩ Nguyen, Anh, et al. &quot;Plug &amp; play generative networks: Conditional iterative generation of images in latent space.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.↩ Olah, et al., &quot;Feature Visualization&quot;, Distill, 2017.↩ Olah, et al., &quot;The Building Blocks of Interpretability&quot;, Distill, 2018.↩ Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. &quot;Visualizing and understanding recurrent networks.&quot; arXiv preprint arXiv:1506.02078 (2015).↩ Bau, David, et al. &quot;Network dissection: Quantifying interpretability of deep visual representations.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.↩ "],["future.html", "Chapter 8 A Look into the Crystal Ball", " Chapter 8 A Look into the Crystal Ball What is the future of interpretable machine learning? This chapter is a speculative mental exercise and a subjective guess how interpretable machine learning will develop. I opened the book with rather pessimistic short stories and would like to conclude with a more optimistic outlook. I have based my &quot;predictions&quot; on three premises: Digitization: Any (interesting) information will be digitized. Think of electronic cash and online transactions. Think of e-books, music and videos. Think of all the sensory data about our environment, our human behavior, industrial production processes and so on. The drivers of the digitization of everything are: Cheap computers/sensors/storage, scaling effects (winner takes it all), new business models, modular value chains, cost pressure and much more. Automation: When a task can be automated and the cost of automation is lower than the cost of performing the task over time, the task will be automated. Even before the introduction of the computer we had a certain degree of automation. For example, the weaving machine automated weaving or the steam machine automated horsepower. But computers and digitization take automation to the next level. Simply the fact that you can program for-loops, write Excel macros, automate e-mail responses, and so on, show how much an individual can automate. Ticket machines automate the purchase of train tickets (no cashier needed any longer), washing machines automate laundry, standing orders automate payment transactions and so on. Automating tasks frees up time and money, so there is a huge economic and personal incentive to automate things. We are currently observing the automation of language translation, driving and, to a small degree, even scientific discovery. Misspecification: We are not able to perfectly specify a goal with all its constraints. Think of the genie in a bottle that always takes your wishes literally: &quot;I want to be the richest person in the world!&quot; -&gt; You become the richest person, but as a side effect the currency you hold crashes due to inflation. &quot;I want to be happy for the rest of my life!&quot; -&gt; The next 5 minutes you feel very happy, then the genie kills you. &quot;I wish for world peace!&quot; -&gt; The genie kills all humans. We specify goals incorrectly, either because we do not know all the constraints or because we cannot measure them. Let's look at corporations as an example of imperfect goal specification. A corporation has the simple goal of earning money for its shareholders. But this specification does not capture the true goal with all its constraints that we really strive for: For example, we do not appreciate a company killing people to make money, poisoning rivers, or simply printing its own money. We have invented laws, regulations, sanctions, compliance procedures, labor unions and more to patch up the imperfect goal specification. Another example that you can experience for yourself is Paperclips, a game in which you play a machine with the goal of producing as many paperclips as possible. WARNING: It is addictive. I do not want to spoil it too much, but let's say things get out of control really fast. In machine learning, the imperfections in the goal specification come from imperfect data abstractions (biased populations, measurement errors, ...), unconstrained loss functions, lack of knowledge of the constraints, shifting of the distribution between training and application data and much more. Digitization is driving automation. Imperfect goal specification conflicts with automation. I claim that this conflict is mediated partially by interpretation methods. The stage for our predictions is set, the crystal ball is ready, now we look at where the field could go! "],["the-future-of-machine-learning.html", "8.1 The Future of Machine Learning", " 8.1 The Future of Machine Learning Without machine learning there can be no interpretable machine learning. Therefore we have to guess where machine learning is heading before we can talk about interpretability. Machine learning (or &quot;AI&quot;) is associated with a lot of promises and expectations. But let's start with a less optimistic observation: While science develops a lot of fancy machine learning tools, in my experience it is quite difficult to integrate them into existing processes and products. Not because it is not possible, but simply because it takes time for companies and institutions to catch up. In the gold rush of the current AI hype, companies open up &quot;AI labs&quot;, &quot;Machine Learning Units&quot; and hire &quot;Data Scientists&quot;, &quot;Machine Learning Experts&quot;, &quot;AI engineers&quot;, and so on, but the reality is, in my experience, rather frustrating. Often companies do not even have data in the required form and the data scientists wait idle for months. Sometimes companies have such high expectation of AI and Data Science due to the media that data scientists could never fulfill them. And often nobody knows how to integrate data scientists into existing structures and many other problems. This leads to my first prediction. Machine learning will grow up slowly but steadily. Digitalization is advancing and the temptation to automate is constantly pulling. Even if the path of machine learning adoption is slow and stony, machine learning is constantly moving from science to business processes, products and real world applications. I believe we need to better explain to non-experts what types of problems can be formulated as machine learning problems. I know many highly paid data scientists who perform Excel calculations or classic business intelligence with reporting and SQL queries instead of applying machine learning. But a few companies are already successfully using machine learning, with the big Internet companies at the forefront. We need to find better ways to integrate machine learning into processes and products, train people and develop machine learning tools that are easy to use. I believe that machine learning will become much easier to use: We can already see that machine learning is becoming more accessible, for example through cloud services (&quot;Machine Learning as a service&quot; -- just to throw a few buzzwords around). Once machine learning has matured -- and this toddler has already taken its first steps -- my next prediction is: Machine learning will fuel a lot of things. Based on the principle &quot;Whatever can be automated will be automated&quot;, I conclude that whenever possible, tasks will be formulated as prediction problems and solved with machine learning. Machine learning is a form of automation or can at least be part of it. Many tasks currently performed by humans are replaced by machine learning. Here are some examples of tasks where machine learning is used to automate parts of it: Sorting / decision-making / completion of documents (e.g. in insurance companies, the legal sector or consulting firms) Data-driven decisions such as credit applications Drug discovery Quality controls in assembly lines Self-driving cars Diagnosis of diseases Translation. For this book, I used a translation service called (DeepL) powered by deep neural networks to improve my sentences by translating them from English into German and back into English. ... The breakthrough for machine learning is not only achieved through better computers / more data / better software, but also: Interpretability tools catalyze the adoption of machine learning. Based on the premise that the goal of a machine learning model can never be perfectly specified, it follows that interpretable machine learning is necessary to close the gap between the misspecified and the actual goal. In many areas and sectors, interpretability will be the catalyst for the adoption of machine learning. Some anecdotal evidence: Many people I have spoken to do not use machine learning because they cannot explain the models to others. I believe that interpretability will address this issue and make machine learning attractive to organisations and people who demand some transparency. In addition to the misspecification of the problem, many industries require interpretability, be it for legal reasons, due to risk aversion or to gain insight into the underlying task. Machine learning automates the modeling process and moves the human a bit further away from the data and the underlying task: This increases the risk of problems with experimental design, choice of training distribution, sampling, data encoding, feature engineering, and so on. Interpretation tools make it easier to identify these problems. "],["the-future-of-interpretability.html", "8.2 The Future of Interpretability", " 8.2 The Future of Interpretability Let us take a look at the possible future of machine learning interpretability. The focus will be on model-agnostic interpretability tools. It is much easier to automate interpretability when it is decoupled from the underlying machine learning model. The advantage of model-agnostic interpretability lies in its modularity. We can easily replace the underlying machine learning model. We can just as easily replace the interpretation method. For these reasons, model-agnostic methods will scale much better. That is why I believe that model-agnostic methods will become more dominant in the long term. But intrinsically interpretable methods will also have a place. Machine learning will be automated and, with it, interpretability. An already visible trend is the automation of model training. That includes automated engineering and selection of features, automated hyperparameter optimization, comparison of different models, and ensembling or stacking of the models. The result is the best possible prediction model. When we use model-agnostic interpretation methods, we can automatically apply them to any model that emerges from the automated machine learning process. In a way, we can automate this second step as well: Automatically compute the feature importance, plot the partial dependence, train a surrogate model, and so on. Nobody stops you from automatically computing all these model interpretations. The actual interpretation still requires people. Imagine: You upload a dataset, specify the prediction goal and at the push of a button the best prediction model is trained and the program spits out all interpretations of the model. There are already first products and I argue that for many applications it will be sufficient to use these automated machine learning services. Today anyone can build websites without knowing HTML, CSS and Javascript, but there are still many web developers around. Similarly, I believe that everyone will be able to train machine learning models without knowing how to program, and there will still be a need for machine learning experts. We do not analyze data, we analyze models. The raw data itself is always useless. (I exaggerate on purpose. The reality is that you need a deep understanding of the data to conduct a meaningful analysis.) I don't care about the data; I care about the knowledge contained in the data. Interpretable machine learning is a great way to distill knowledge from data. You can probe the model extensively, the model automatically recognizes if and how features are relevant for the prediction (many models have built-in feature selection), the model can automatically detect how relationships are represented, and -- if trained correctly -- the final model is a very good approximation of reality. Many analytical tools are already based on data models (because they are based on distribution assumptions): Simple hypothesis tests like Student's t-test. Hypothesis tests with adjustments for confounders (usually GLMs) Analysis of Variance (ANOVA) The correlation coefficient (the standardized linear regression coefficient is related to Pearson's correlation coefficient) ... What I am telling you here is actually nothing new. So why switch from analyzing assumption-based, transparent models to analyzing assumption-free black box models? Because making all these assumptions is problematic: They are usually wrong (unless you believe that most of the world follows a Gaussian distribution), difficult to check, very inflexible and hard to automate. In many domains, assumption-based models typically have a worse predictive performance on untouched test data than black box machine learning models. This is only true for big datasets, since interpretable models with good assumptions often perform better with small datasets than black box models. The black box machine learning approach requires a lot of data to work well. With the digitization of everything, we will have ever bigger datasets and therefore the approach of machine learning becomes more attractive. We do not make assumptions, we approximate reality as close as possible (while avoiding overfitting of the training data). I argue that we should develop all the tools that we have in statistics to answer questions (hypothesis tests, correlation measures, interaction measures, visualization tools, confidence intervals, p-values, prediction intervals, probability distributions) and rewrite them for black box models. In a way, this is already happening: Let us take a classical linear model: The standardized regression coefficient is already a feature importance measure. With the permutation feature importance measure, we have a tool that works with any model. In a linear model, the coefficients measures the effect of a single feature on the predicted outcome. The generalized version of this is the partial dependence plot. Test whether A or B is better: For this we can also use partial dependence functions. What we do not have yet (to the best of my best knowledge) are statistical tests for arbitrary black box models. The data scientists will automate themselves. I believe that data scientists will eventually automate themselves out of the job for many analysis and prediction tasks. For this to happen, the tasks must be well-defined and there must to be some processes and routines around them. Today, these routines and processes are missing, but data scientists and colleagues are working on them. As machine learning becomes an integral part of many industries and institutions, many of the tasks will be automated. Robots and programs will explain themselves. We need more intuitive interfaces to machines and programs that make heavy use of machine learning. Some examples: A self-driving car that reports why it stopped abruptly (&quot;70% probability that a kid will cross the road&quot;); A credit default program that explains to a bank employee why a credit application was rejected (&quot;Applicant has too many credit cards and is employed in an unstable job.&quot;); A robot arm that explains why it moved the item from the conveyor belt into the trash bin (&quot;The item has a craze at the bottom.&quot;). Interpretability could boost machine intelligence research. I can imagine that by doing more research on how programs and machines can explain themselves, we can improve improve our understanding of intelligence and become better at creating intelligent machines. In the end, all these predictions are speculations and we have to see what the future really brings. Form your own opinion and continue learning! "],["contribute.html", "Chapter 9 Contribute to the Book", " Chapter 9 Contribute to the Book Thank you for reading my book about Interpretable Machine Learning. The book is under continuous development. It will be improved over time and more chapters will be added. Very similar to how software is developed. All text and code for the book is open source and available at github.com. On the Github page you can suggest fixes and open issues if you find a mistake or if something is missing. "],["cite.html", "Chapter 10 Citing this Book", " Chapter 10 Citing this Book If you found this book useful for your blog post, research article or product, I would be grateful if you would cite this book. You can cite the book like this: Molnar, Christoph. &quot;Interpretable machine learning. A Guide for Making Black Box Models Explainable&quot;, 2019. https://christophm.github.io/interpretable-ml-book/. Or use the following bibtex entry: @book{molnar2019, title = {Interpretable Machine Learning}, author = {Christoph Molnar}, note = {\\url{https://christophm.github.io/interpretable-ml-book/}}, year = {2019}, subtitle = {A Guide for Making Black Box Models Explainable} } I am always curious about where and how interpretation methods are used in industry and research. If you use the book as a reference, it would be great if you wrote me a line and told me what for. This is of course optional and only serves to satisfy my own curiosity and to stimulate interesting exchanges. My mail is christoph.molnar.ai@gmail.com . "],["translations.html", "Chapter 11 Translations", " Chapter 11 Translations Interested in translating the book? This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This means that you are allowed to translate it and put it online. You have to mention me as original author and you are not allowed to sell the book. If you are interested in translating the book, you can write a message and I can link your translation here. My address is christoph.molnar.ai@gmail.com . List of translations Chinese: https://github.com/MingchaoZhu/InterpretableMLBook Complete translations by Mingchao Zhu. https://blog.csdn.net/wizardforcel/article/details/98992150 Translation of most chapters, by CSDN, an online community of programmers. https://zhuanlan.zhihu.com/p/63408696 Translation of some chapters by 知乎. The website also includes questions and answers from various users. Korean: https://tootouch.github.io/IML/taxonomy_of_interpretability_methods/ Complete Korean translation by TooTouch https://subinium.github.io/IML/ Partial Korean translation by An Subin Spanish https://fedefliguer.github.io/AAI/ First chapters translated by Federico Fliguer If you know of any other translation of the book or of individual chapters, I would be grateful to hear about it and list it here. You can reach me via email: christoph.molnar.ai@gmail.com . "],["acknowledgements.html", "Chapter 12 Acknowledgements", " Chapter 12 Acknowledgements Writing this book was (and still is) a lot of fun. But it is also a lot of work and I am very happy about the support I received. My biggest thank-you goes to Katrin who had the hardest job in terms of hours and effort: she proofread the book from beginning to end and discovered many spelling mistakes and inconsistencies that I would never have found. I am very grateful for her support. A big thanks goes to Verena Haunschmid for writing the section about LIME explanations for images. She works in data science and I recommend following her on Twitter: @ExpectAPatronum. I also want to thank all the early readers who contributed corrections on Github! Furthermore, I want to thank everyone who created illustrations: The cover was designed by my friend @YvonneDoinel. The graphics in the Shapley Value chapter were created by Heidi Seibold, as well as the turtle example in the adversarial examples chapter. Verena Haunschmid created the graphic in the RuleFit chapter. In at least three aspects, the way I published this book is unconventional. First, it is available both as website and as ebook/pdf. The software I used to create this book is called bookdown, written by Yihui Xie, who created many R packages that make it easy to combine R code and text. Thanks a lot! Secondly, I self-publish the book on the platform Leanpub, instead of working with a traditional publisher. And third, I published the book as in-progress book, which has helped me enormously to get feedback and to monetize it along the way. Many thanks to leanpub for making this possible and handling the royalties fairly. I would also like to thank you, dear reader, for reading this book without a big publisher name behind it. I am grateful for the funding of my research on interpretable machine learning by the Bavarian State Ministry of Science and the Arts in the framework of the Centre Digitisation.Bavaria (ZD.B). "],["references.html", "References", " References &quot;Definition of Algorithm.&quot; https://www.merriam-webster.com/dictionary/algorithm. (2017). Aamodt, Agnar, and Enric Plaza. &quot;Case-based reasoning: Foundational issues, methodological variations, and system approaches.&quot; AI communications 7.1 (1994): 39-59. Alberto, Túlio C, Johannes V Lochter, and Tiago A Almeida. &quot;Tubespam: comment spam filtering on YouTube.&quot; In Machine Learning and Applications (Icmla), Ieee 14th International Conference on, 138–43. IEEE. (2015). Alvarez-Melis, David, and Tommi S. Jaakkola. &quot;On the robustness of interpretability methods.&quot; arXiv preprint arXiv:1806.08049 (2018). Apley, Daniel W. &quot;Visualizing the effects of predictor variables in black box supervised learning models.&quot; arXiv preprint arXiv:1612.08468 (2016). Athalye, Anish, and Ilya Sutskever. &quot;Synthesizing robust adversarial examples.&quot; arXiv preprint arXiv:1707.07397 (2017). Bau, David, et al. &quot;Network dissection: Quantifying interpretability of deep visual representations.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. Biggio, Battista, and Fabio Roli. &quot;Wild Patterns: Ten years after the rise of adversarial machine learning.&quot; Pattern Recognition 84 (2018): 317-331. Breiman, Leo.“Random Forests.” Machine Learning 45 (1). Springer: 5-32 (2001). Brown, Tom B., et al. &quot;Adversarial patch.&quot; arXiv preprint arXiv:1712.09665 (2017). Cohen, William W. &quot;Fast effective rule induction.&quot; Machine Learning Proceedings (1995). 115-123. Cook, R. Dennis. &quot;Detection of influential observation in linear regression.&quot; Technometrics 19.1 (1977): 15-18. Doshi-Velez, Finale, and Been Kim. &quot;Towards a rigorous science of interpretable machine learning,&quot; no. Ml: 1–13. http://arxiv.org/abs/1702.08608 ( 2017). Emilie Kaufmann and Shivaram Kalyanakrishnan. “Information Complexity in Bandit Subset Selection”. Proceedings of Machine Learning Research (2013). Fanaee-T, Hadi, and Joao Gama. &quot;Event labeling combining ensemble detectors and background knowledge.&quot; Progress in Artificial Intelligence. Springer Berlin Heidelberg, 1–15. doi:10.1007/s13748-013-0040-3. (2013). Fernandes, Kelwin, Jaime S Cardoso, and Jessica Fernandes. &quot;Transfer learning with partial observability applied to cervical cancer screening.&quot; In Iberian Conference on Pattern Recognition and Image Analysis, 243–50. Springer. (2017). Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. “Model Class Reliance: Variable importance measures for any machine learning model class, from the ‘Rashomon’ perspective.” http://arxiv.org/abs/1801.01489 (2018). Fokkema, Marjolein, and Benjamin Christoffersen. &quot;Pre: Prediction rule ensembles&quot;. https://CRAN.R-project.org/package=pre (2017). Friedman, Jerome H, and Bogdan E Popescu. &quot;Predictive learning via rule ensembles.&quot; The Annals of Applied Statistics. JSTOR, 916–54. (2008). Friedman, Jerome H. &quot;Greedy function approximation: A gradient boosting machine.&quot; Annals of statistics (2001): 1189-1232. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. &quot;The elements of statistical learning&quot;. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009). Fürnkranz, Johannes, Dragan Gamberger, and Nada Lavrač. &quot;Foundations of rule learning.&quot; Springer Science &amp; Business Media, (2012). Goldstein, Alex, et al. &quot;Package ‘ICEbox’.&quot; (2017). Goldstein, Alex, et al. &quot;Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation.&quot; Journal of Computational and Graphical Statistics 24.1 (2015): 44-65. Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. &quot;Explaining and harnessing adversarial examples.&quot; arXiv preprint arXiv:1412.6572 (2014). Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. &quot;A simple and effective model-based variable importance measure.&quot; arXiv preprint arXiv:1805.04755 (2018). Heider, Fritz, and Marianne Simmel. &quot;An experimental study of apparent behavior.&quot; The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944). Holte, Robert C. &quot;Very simple classification rules perform well on most commonly used datasets.&quot; Machine learning 11.1 (1993): 63-90. Hooker, Giles. &quot;Discovering additive structure in black box functions.&quot; Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004). Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. &quot;Feature relevance quantification in explainable AI: A causal problem.&quot; International Conference on Artificial Intelligence and Statistics. PMLR, 2020. Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. &quot;Feature relevance quantification in explainable AI: A causality problem.&quot; arXiv preprint arXiv:1910.13413 (2019). Kahneman, Daniel, and Amos Tversky. &quot;The Simulation Heuristic.&quot; Stanford Univ CA Dept of Psychology. (1981). Kaufman, Leonard, and Peter Rousseeuw. &quot;Clustering by means of medoids&quot;. North-Holland (1987). Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. &quot;Examples are not enough, learn to criticize! Criticism for interpretability.&quot; Advances in Neural Information Processing Systems (2016). Kim, Been, et al. &quot;Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav).&quot; arXiv preprint arXiv:1711.11279 (2017). Koh, Pang Wei, and Percy Liang. &quot;Understanding black-box predictions via influence functions.&quot; arXiv preprint arXiv:1703.04730 (2017). Laugel, Thibault, et al. &quot;Inverse classification for comparison-based interpretability in machine learning.&quot; arXiv preprint arXiv:1712.08443 (2017). Letham, Benjamin, et al. &quot;Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model.&quot; The Annals of Applied Statistics 9.3 (2015): 1350-1371. Lipton, Peter. &quot;Contrastive explanation.&quot; Royal Institute of Philosophy Supplements 27 (1990): 247-266. Lipton, Zachary C. &quot;The mythos of model interpretability.&quot; arXiv preprint arXiv:1606.03490, (2016). Lundberg, Scott M., and Su-In Lee. &quot;A unified approach to interpreting model predictions.&quot; Advances in Neural Information Processing Systems. 2017. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. &quot;Anchors: High-Precision Model-Agnostic Explanations.&quot; AAAI Conference on Artificial Intelligence (AAAI), 2018 Martens, David, and Foster Provost. &quot;Explaining data-driven document classifications.&quot; (2014). Miller, Tim. &quot;Explanation in artificial intelligence: Insights from the social sciences.&quot; arXiv Preprint arXiv:1706.07269. (2017). Nguyen, Anh, et al. &quot;Plug &amp; play generative networks: Conditional iterative generation of images in latent space.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. Nguyen, Anh, et al. &quot;Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.&quot; Advances in Neural Information Processing Systems. 2016. Nickerson, Raymond S. &quot;Confirmation Bias: A ubiquitous phenomenon in many guises.&quot; Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998). Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015 Papernot, Nicolas, et al. &quot;Practical black-box attacks against machine learning.&quot; Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017). Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Anchors: High-precision model-agnostic explanations.&quot; AAAI Conference on Artificial Intelligence (2018). Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Model-agnostic interpretability of machine learning.&quot; ICML Workshop on Human Interpretability in Machine Learning. (2016). Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Why should I trust you?: Explaining the predictions of any classifier.&quot; Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016). Shapley, Lloyd S. &quot;A value for n-person games.&quot; Contributions to the Theory of Games 2.28 (1953): 307-317. Staniak, Mateusz, and Przemyslaw Biecek. &quot;Explanations of model predictions with live and breakDown packages.&quot; arXiv preprint arXiv:1804.01955 (2018). Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. &quot;One pixel attack for fooling deep neural networks.&quot; IEEE Transactions on Evolutionary Computation (2019). Sundararajan, Mukund, and Amir Najmi. &quot;The many Shapley values for model explanation.&quot; arXiv preprint arXiv:1908.08474 (2019). Sundararajan, Mukund, and Amir Najmi. &quot;The many Shapley values for model explanation.&quot; arXiv preprint arXiv:1908.08474 (2019). Szegedy, Christian, et al. &quot;Intriguing properties of neural networks.&quot; arXiv preprint arXiv:1312.6199 (2013). Van Looveren, Arnaud, and Janis Klaise. &quot;Interpretable Counterfactual Explanations Guided by Prototypes.&quot; arXiv preprint arXiv:1907.02584 (2019). Wachter, Sandra, Brent Mittelstadt, and Chris Russell. &quot;Counterfactual explanations without opening the black box: Automated decisions and the GDPR.&quot; (2017). Yang, Hongyu, Cynthia Rudin, and Margo Seltzer. &quot;Scalable Bayesian rule lists.&quot; Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. Zhao, Qingyuan, and Trevor Hastie. &quot;Causal interpretations of black-box models.&quot; Journal of Business &amp; Economic Statistics, to appear. (2017). Štrumbelj, Erik, and Igor Kononenko. &quot;A general method for visualizing and explaining black-box regression models.&quot; In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011). Štrumbelj, Erik, and Igor Kononenko. &quot;Explaining prediction models and individual predictions with feature contributions.&quot; Knowledge and information systems 41.3 (2014): 647-665. "],["r-packages-used-for-examples.html", "R Packages Used for Examples", " R Packages Used for Examples base. R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/. data.table. Matt Dowle and Arun Srinivasan (2020). data.table: Extension of data.frame. R package version 1.13.6. https://CRAN.R-project.org/package=data.table dplyr. Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.4. https://CRAN.R-project.org/package=dplyr ggplot2. Hadley Wickham, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani and Dewey Dunnington (2020). ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.3.3. https://CRAN.R-project.org/package=ggplot2 iml. Christoph Molnar and Patrick Schratz (2020). iml: Interpretable Machine Learning. R package version 0.10.1. https://CRAN.R-project.org/package=iml knitr. Yihui Xie (2021). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.31. https://CRAN.R-project.org/package=knitr libcoin. Torsten Hothorn (2021). libcoin: Linear Test Statistics for Permutation Inference. R package version 1.0-8. https://CRAN.R-project.org/package=libcoin memoise. Hadley Wickham, Jim Hester, Winston Chang, Kirill Müller and Daniel Cook (2021). memoise: Memoisation of Functions. R package version 2.0.0. https://CRAN.R-project.org/package=memoise mlr. Bernd Bischl, Michel Lang, Lars Kotthoff, Patrick Schratz, Julia Schiffner, Jakob Richter, Zachary Jones, Giuseppe Casalicchio and Mason Gallo (2020). mlr: Machine Learning in R. R package version 2.18.0. https://CRAN.R-project.org/package=mlr mvtnorm. Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi and Torsten Hothorn (2020). mvtnorm: Multivariate Normal and t Distributions. R package version 1.1-1. https://CRAN.R-project.org/package=mvtnorm NLP. Kurt Hornik (2020). NLP: Natural Language Processing Infrastructure. R package version 0.2-1. https://CRAN.R-project.org/package=NLP ParamHelpers. Bernd Bischl, Michel Lang, Jakob Richter, Jakob Bossek, Daniel Horn and Pascal Kerschke (2020). ParamHelpers: Helpers for Parameters in Black-Box Optimization, Tuning and Machine Learning. R package version 1.14. https://CRAN.R-project.org/package=ParamHelpers partykit. Torsten Hothorn and Achim Zeileis (2021). partykit: A Toolkit for Recursive Partytioning. R package version 1.2-12. https://CRAN.R-project.org/package=partykit pre. Marjolein Fokkema and Benjamin Christoffersen (2020). pre: Prediction Rule Ensembles. R package version 1.0.0. https://CRAN.R-project.org/package=pre readr. Hadley Wickham and Jim Hester (2020). readr: Read Rectangular Text Data. R package version 1.4.0. https://CRAN.R-project.org/package=readr rpart. Terry Therneau and Beth Atkinson (2019). rpart: Recursive Partitioning and Regression Trees. R package version 4.1-15. https://CRAN.R-project.org/package=rpart tidyr. Hadley Wickham (2020). tidyr: Tidy Messy Data. R package version 1.1.2. https://CRAN.R-project.org/package=tidyr tm. Ingo Feinerer and Kurt Hornik (2020). tm: Text Mining Package. R package version 0.7-8. https://CRAN.R-project.org/package=tm viridis. Simon Garnier (2018). viridis: Default Color Maps from 'matplotlib'. R package version 0.5.1. https://CRAN.R-project.org/package=viridis viridisLite. Simon Garnier (2018). viridisLite: Default Color Maps from 'matplotlib' (Lite Version). R package version 0.3.0. https://CRAN.R-project.org/package=viridisLite "]]
