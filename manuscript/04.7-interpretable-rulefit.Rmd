```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->

## RuleFit {#rulefit}
<!--The RuleFit algorithm by Friedman and Popescu (2008)[^Friedman2008] learns sparse linear models that include automatically detected interaction effects in the form of decision rules.-->

Friedman と Popescu (2008)[^Friedman2008] による RuleFit アルゴリズムは、相互作用効果を決定規則の形で自動的に検出したスパース線形モデルの学習に使われます。

<!--The linear regression model does not account for interactions between features.
Would it not be convenient to have a model that is as simple and interpretable as linear models, but also integrates feature interactions?
RuleFit fills this gap.
RuleFit learns a sparse linear model with the original features and also a number of new features that are decision rules.
These new features capture interactions between the original features.
RuleFit automatically generates these features from decision trees.
Each path through a tree can be transformed into a decision rule by combining the split decisions into a rule.
The node predictions are discarded and only the splits are used in the decision rules:
-->
線形回帰モデルは特徴間の相互作用を考慮していません。
線形モデルのようにシンプルで解釈しやすいモデルでありながら、特徴間の相互作用を統合したモデルがあれば便利ではないでしょうか？
実は、RuleFit はギャップを埋めることができます。
RuleFit は、元の特徴量と決定規則である多数の新しい特徴量を用いて、スパース線形モデルを学習します。
これらの新しい特徴量は、元の特徴量間の相互作用を説明します。
RuleFit は、決定木からこれらの特徴量を自動的に生成します。
分割された決定を結合し、規則にすることで、木を通る各パスを決定規則に変換できます。
ノードによる予測を破棄し、分割のみを決定規則に使用します。

<!--fig.cap="4 rules can be generated from a tree with 3 terminal nodes."-->

```{r rulefit-split, fig.cap="3つの終端ノードを持つ木から4つの規則を生成することができます。", out.width=500}
knitr::include_graphics("images/rulefit.jpg")
```

<!--
Where do those decision trees come from?
The trees are trained to predict the outcome of interest.
This ensures that the splits are meaningful for the prediction task.
Any algorithm that generates a lot of trees can be used for RuleFit, for example a random forest.
Each tree is decomposed into decision rules that are used as additional features in a sparse linear regression model (Lasso).
-->
これらの決定木はどこから来ているのでしょうか？
決定木は興味のある結果を予測するために学習されます。
これは、予測問題に対して分割が意味を持つことを保証しています。
ランダムフォレストのように、多数の木を生成するアルゴリズムを RuleFit に使うことができます。
それぞれの木は、スパース線形回帰モデル (Lasso) で使用される追加の特徴量である決定規則に分解されます。

<!--
The RuleFit paper uses the Boston housing data to illustrate this:
The goal is to predict the median house value of a Boston neighborhood.
One of the rules generated by RuleFit is:
IF `number of rooms > 6.64`  AND  `concentration of nitric oxide <0.67` THEN 1 ELSE 0.
-->
RuleFit が提案された論文では、ボストンの住宅データを使って説明しています。
目標は、ボストンの住宅の中央値を予測することです。
RuleFit で生成された規則の1つは、`部屋の数 > 6.64` かつ `一酸化炭素濃度 < 0.67` であるならば 1、そうでないならば 0 としています。

<!--
RuleFit also comes with a feature importance measure that helps to identify linear terms and rules that are important for the predictions.
Feature importance is calculated from the weights of the regression model.
The importance measure can be aggregated for the original features (which are used in their "raw" form and possibly in many decision rules).
-->
RuleFit は、予測に重要な線形項や規則を特定するための重要度の指標としても機能します。
特徴量の重要度は回帰モデルの回帰係数から計算されます。
重要度の指標は元の特徴量（"生"の形で使われ、多くの決定規則で使われる可能性があります）を集約したものになります。

<!--
RuleFit also introduces partial dependence plots to show the average change in prediction by changing a feature.
The partial dependence plot is a model-agnostic method that can be used with any model, and is explained in the [book chapter on partial dependence plots](#pdp).
-->
RuleFit は、特徴量を変更することで、予測値の平均的な変化を示す partial dependence plot を導入します。
partial dependence plot は、どんなモデルにも使うことのできるモデル診断の手法であり、[partial dependence plots](#pdp)の章で解説されています。

<!--### Interpretation and Example-->
### 解釈と例

<!--
Since RuleFit estimates a linear model in the end, the interpretation is the same as for "normal" [linear models](#limo).
The only difference is that the model has new features derived from decision rules.
Decision rules are binary features:
A value of 1 means that all conditions of the rule are met, otherwise the value is 0.
For linear terms in RuleFit, the interpretation is the same as in linear regression models:
If the feature increases by one unit, the predicted outcome changes by the corresponding feature weight.
-->
RuleFit は最終的には線形モデルを推定するので、解釈は"普通の"[線形モデル](#limo)と同じになります。
違いは、決定規則からなる新しい特徴量を有していることです。
値が 1 であることは全ての条件を満たしていることを示し、そうでない場合は値は 0 になります。
RuleFit における線形項の解釈は、線形回帰モデルと同様になります。ある特徴量が 1 増加すると、予測結果は特徴量の重みに応じて変化します。

```{r prepare-rulefit-example}
library(pre)
library(dplyr)

data("bike")

X = bike[bike.features.of.interest]

# round features so that table is better
X$temp = round(X$temp, 0)
X$hum = round(X$hum, 0)
X$windspeed = round(X$windspeed, 0)

y = bike[,'cnt']
dat = cbind(X, y)
mod = pre(y ~ ., data = dat, maxdepth = 2, ntrees = 100)
coefs <- coef(mod)
coefs$description[is.na(coefs$description)] = coefs$rule[is.na(coefs$description)]
coefs = left_join(coef(mod), pre::importance(mod, plot=FALSE)$baseimp)
coefs = coefs[!is.na(coefs$coefficient), ]
coefs$imp = round(coefs$imp, 1)
coefs$coefficient = round(coefs$coefficient, 1)
coefs$sd = round(coefs$sd, 2)
coefs$rule = NULL
coefs = coefs[!is.na(coefs$imp), ]
coefs = coefs[order(coefs$imp, decreasing = TRUE), ]
```

<!--
In this example, we use RuleFit to predict the number of [rented bicycles](#bike-data) on a given day.
The table shows five of the rules that were generated by RuleFit, along with their Lasso weights and importances.
The calculation is explained later in the chapter.
-->
この例では、RuleFit をある日付の[レンタル自転車数](#bike-data)を予測するために使用しています。
この表は、RuleFit によって生成された5つの規則と、それらの Lasso による重みと重要性を示しています。
計算については、この章の後半で説明します。

```{r rulefit-example}
# Making the table a bit prettier
coefs$description = gsub("\\%", "", coefs$description)
coefs$description = gsub("c\\(", "(", coefs$description)
kable(coefs[1:5, c('description', 'coefficient', 'imp')],
  col.names = c('Description', 'Weight', 'Importance'),
  row.names=FALSE, 
  digits = 0)

```

<!--
The most important rule was: "`r coefs[1, 'description']`" and the corresponding weight is `r round(coefs[1, 'coefficient'], 0)`.
The interpretation is:
If `r coefs[1, 'description']`, then the predicted number of bikes increases by `r round(coefs[1, 'coefficient'], 0)`, when all other feature values remain fixed.
In total, `r nrow(coef(mod))` such rules were created from the original `r ncol(X)` features.
Quite a lot!
But thanks to Lasso, only `r nrow(coefs[coefs$coefficient != 0,])` of the `r nrow(coef(mod))` have a weight different from 0.
-->
最も重要な規則は "`r coefs[1, 'description']`" であり、対応する重みは `r round(coefs[1, 'coefficient'], 0)` です。
このような規則は、元の `r ncol(X)` つの特徴量から合計 `r nrow(coef(mod))` 個が作成されました。
かなりの数です! 
しかし Lasso のおかげで、`r nrow(coef(mod))` のうち `r nrow(coefs[coefs$coefficient != 0,])` だけが 0 ではない重みを持っていることがわかります。

<!--
Computing the global feature importances reveals that temperature and time trend are the most important features:
-->
大域的な特徴量の重要性を計算すると、気温と時間の傾向が最も重要な特徴量であることがわかります。

<!--fig.cap = 'Feature importance measures for a RuleFit model predicting bike counts. The most important features for the predictions were temperature and time trend.'-->
```{r rulefit-importance, fig.cap = '自転車の数を予測する RuleFit モデルの特徴量重要度。予測のために最も重要な特徴量は気温と時間傾向でした。', dev.args = list(pointsize = 16)}
pre::importance(mod)
```

<!--
The feature importance measurement includes the importance of the raw feature term and all the decision rules in which the feature appears.
-->
特徴量重要度の尺度は、生の特徴量の重要度と、その特徴量が現れるすべての決定規則を含みます。

<!--**Interpretation template**-->
**解釈のテンプレート**

<!--
The interpretation is analogous to linear models:
The predicted outcome changes by $\beta_j$ if feature $x_j$ changes by one unit, provided all other features remain unchanged.
The weight interpretation of a decision rule is a special case:
If all conditions of a decision rule $r_k$ apply, the predicted outcome changes by $\alpha_k$ (the learned weight of rule $r_k$ in the linear model).
-->
解釈は、線形モデルと類似しています。
他の特徴量が固定されている場合、特徴量 $x_j$ が 1 変化すると、予測結果は $\beta_j$ だけ変化します。
決定規則に関する重みの解釈は特殊です。
決定規則 $r_k$ のすべての条件を満たすならば、予測結果は $\alpha_k$ (線形モデルで学習された規則 $r_k$ の重み) だけ変化します。

<!--
For classification (using logistic regression instead of linear regression):
If all conditions of the decision rule $r_k$ apply, the odds for event vs. no-event changes by a factor of $\alpha_k$.
-->
分類問題では（線形回帰ではなくロジスティック回帰を用いた場合）、
決定規則 $r_k$ の全ての条件を満たすなら、その事象が発生するかしないかのオッズが $\alpha_k$ 倍変化します。

<!--### Theory-->
### 理論

<!--
Let us dive deeper into the technical details of the RuleFit algorithm.
RuleFit consists of two components:
The first component creates "rules" from decision trees and the second component fits a linear model with the original features and the new rules as input (hence the name "RuleFit").
-->
RuleFit アルゴリズムの技術的な詳細について深く見ていくことにしましょう。
RuleFit は2つのコンポーネントで構成されています。
最初のコンポーネントは、決定木から"規則"を作成し、2番目のコンポーネントでは、元の特徴量と作成した規則を入力とする線形モデルを学習します（これが "RuleFit" という名前の由来です）。

<!--**Step 1: Rule generation**-->
**Step 1: 規則の生成**

<!--
What does a rule look like?
The rules generated by the algorithm have a simple form.
For example:
IF `x2 < 3` AND `x5 < 7` THEN 1 ELSE 0.
The rules are constructed by decomposing decision trees:
Any path to a node in a tree can be converted to a decision rule.
The trees used for the rules are fitted to predict the target outcome.
Therefore the splits and resulting rules are optimized to predict the outcome you are interested in.
You simply chain the binary decisions that lead to a certain node with "AND", and voilà, you have a rule.
It is desirable to generate a lot of diverse and meaningful rules.
Gradient boosting is used to fit an ensemble of decision trees by regressing or classifying y with your original features X.
Each resulting tree is converted into multiple rules.
Not only boosted trees, but any tree ensemble algorithm can be used to generate the trees for RuleFit.
A tree ensemble can be described with this general formula:
-->
規則とはどのようなものでしょうか？
アルゴリズムによって生成された規則は単純な形式になります。
例えば、IF `x2 < 3` AND `x5 < 7` THEN 1 ELSE 0 といったものです。
規則は、決定木を分解することで構築されます。
決定木上の任意のパスは、決定規則に変換できます。
規則のための木は、出力を予測するために利用されます。
したがって、分割や得られる規則は興味のある結果を得るために最適化されています。
特定のノードに至る二分決定を "AND" で連結させるだけで規則ができます。
多様かつ意味のある規則を多く生成することが望まれます。
勾配ブースティングでは、y を元の特徴量 X を使って回帰あるいは分類をすることで、決定木のアンサンブルを学習させます。
そして作成された各々の木は、複数の規則に変換されます。
ブースティングに限らず、任意の木のアンサンブルアルゴリズムに対して、RuleFit の木を生成できます。
木のアンサンブルは、次の一般的な式で記述できます。

$$f(x)=a_0+\sum_{m=1}^M{}a_m{}f_m(X)$$

<!--
M is the number of trees and $f_m(x)$ is the prediction function of the m-th tree.
The $a$'s are the weights.
Bagged ensembles, random forest, AdaBoost and MART produce tree ensembles  and can be used for RuleFit.
-->
M は木の数であり、$f_m(x)$ は m 番目の木の予測関数です。
$a$ は重みです。
Bagged ensembles、ランダムフォレスト、AdaBoost、そして MART は木のアンサンブルを生成し、RuleFit で使用されます。

<!--
We create the rules from all trees of the ensemble.
Each rule $r_m$ takes the form of:
-->
アンサンブルの全ての木から規則を作成します。
各規則 $r_m$ は次の形式で表されます。

$$r_m(x)=\prod_{j\in\text{T}_m}I(x_j\in{}s_{jm})$$

<!--
where $\text{T}_{m}$ is the set of features used in the m-th tree, I is the indicator function that is 1 when feature $x_j$ is in the specified subset of values s for the j-th feature (as specified by the tree splits) and 0 otherwise.
For numerical features, $s_{jm}$ is an interval in the value range of the feature.
The interval looks like one of the two cases:
-->
ここで、$\text{T}_{m}$ は、m 番目の木で利用される特徴量の集合です。
I は、特徴量 $x_j$ が j 番目の特徴量（木の分割で指定されたもの）に対する部分集合 s に含まれる場合に 1、それ以外の場合に 0 となる指示関数です。
量的特徴量の場合、$s_{jm}$ は特徴量の値の区間となります。
区間は次の2つの場合のいずれかのようになります。

$$x_{s_{jm},\text{lower}}<x_j$$

$$x_j<x_{s_{jm},upper}$$

<!--
Further splits in that feature possibly lead to more complicated intervals.
For categorical features the subset s contains some specific categories of the feature.
-->
特徴量を更に分割すると、より複雑な区間になる可能性があります。
カテゴリカル特徴量の場合、部分集合は特徴量の特定のカテゴリが含まれることになります。

<!--A made up example for the bike rental dataset:-->
自転車レンタルのデータセットの例を見てみましょう。

$$r_{17}(x)=I(x_{\text{temp}}<15)\cdot{}I(x_{\text{weather}}\in\{\text{good},\text{cloudy}\})\cdot{}I(10\leq{}x_{\text{windspeed}}<20)$$

<!--
This rule returns 1 if all three conditions are met, otherwise 0.
RuleFit extracts all possible rules from a tree, not only from the leaf nodes.
So another rule that would be created is:
-->
この規則は、3つの条件全てが満たされた場合に 1、それ以外は 0 を返します。
RuleFit は、葉だけではなく、木の全てのノードから規則を抽出します。
したがって、作成されるであろう規則は次のようになります。

$$r_{18}(x)=I(x_{\text{temp}}<15)\cdot{}I(x_{\text{weather}}\in\{\text{good},\text{cloudy}\}$$

<!--
Altogether, the number of rules created from an ensemble of M trees with $t_m$ terminal nodes each is:
-->
全体として、$t_m$ 個の葉をもつ M 個の木のアンサンブルから作成される規則の数は次式で与えられます。

$$K=\sum_{m=1}^M2(t_m-1)$$ 

<!--
A trick introduced by the RuleFit authors is to learn trees with random depth so that many diverse rules with different lengths are generated.
Note that we discard the predicted value in each node and only keep the conditions that lead us to a node and then we create a rule from it.
The weighting of the decision rules is done in step 2 of RuleFit.
-->
RuleFit の著者によって導入されたトリックは、ランダムな深さの木を学習することで、長さの異なる多種多様な規則を生成するというものです。
各ノードにおける予測値は破棄して、そのノードに至る条件のみを保持し、そこから規則を作るということに注意してください。
決定規則の重みづけは、RuleFit の第2ステップで行われます。

<!--
Another way to see step 1:
RuleFit generates a new set of features from your original features.
These features are binary and can represent quite complex interactions of your original features.
The rules are chosen to maximize the prediction task.
The rules are automatically generated from the covariates matrix X.
You can simply see the rules as new features based on your original features.
-->
ステップ1はこのように見ることもできます。
RuleFit は、元の特徴量から新しい特徴量の集合を生成します。
これらの特徴量は、二値であり、元の特徴量の極めて複雑な相互作用を表現できます。
規則は予測タスクで最良の結果が得られるように選択されます。
規則は、共変量行列Xから自動的に生成されます。
規則は元の特徴量に基づく新たな特徴量としてみなすことができます。

<!--**Step 2: Sparse linear model**-->
**Step 2: スパース線形モデル**

<!--
You get MANY rules in step 1.
Since the first step can be seen as only a feature transformation, you are still not done with fitting a model.
Also, you want to reduce the number of rules.
In addition to the rules, all your "raw" features from your original dataset will  also be used in the sparse linear model.
Every rule and every original feature becomes a feature in the linear model and gets a weight estimate.
The original raw features are added because trees fail at representing simple linear relationships between y and x.
Before we train a sparse linear model, we winsorize the original features so that they are more robust against outliers:
-->
ステップ1で、多くの規則を得ることができます。
この最初のステップは、単なる特徴量の変換にすぎないため、モデルへの適合はまだ終わっていません。
また、規則の数を減らしたいとも思うでしょう。
これらの規則に加えて、元のデータセットの全ての"生"の特徴量も、スパース線形モデルで利用することになります。
全ての規則と元の特徴量が線形モデルの特徴量となり、重みが推定値されます。
元の生の特徴量を追加するのは、木は y と x の間の単純な線形関係を表現するのに失敗するためです。
スパース線形モデルを学習する前に、元の特徴量の外れ値をクリッピング (winsorizing) し、外れ値に対してより頑健になるようにします。

$$l_j^*(x_j)=min(\delta_j^+,max(\delta_j^-,x_j))$$

<!--
where $\delta_j^-$ and $\delta_j^+$ are the $\delta$ quantiles of the data distribution of feature $x_j$.
A choice of 0.05 for $\delta$ means that any value of feature $x_j$ that is in the 5% lowest or 5% highest values will be set to the quantiles at 5% or 95% respectively.
As a rule of thumb, you can choose $\delta$ = 0.025.
In addition, the linear terms have to be normalized so that they have the same prior importance as a typical decision rule:
-->
ここで、$\delta_j^-$ と $\delta_j^+$ は、特徴量 $x_j$ のデータ分布の $\delta$ 分位数です。
$\delta$ に0.05を選択すると、上位 5％ または下位 5％ の特徴量 $x_j$ の値が、それぞれ 5％ または 95％ の分位数に設定されます。
経験則として、$\delta$ = 0.025 を選択できます。
さらに、線形項は、通常の決定規則と事前の重要性が同一となるように正規化する必要があります。

$$l_j(x_j)=0.4\cdot{}l^*_j(x_j)/std(l^*_j(x_j))$$

<!--
The $0.4$ is the average standard deviation of rules with a uniform support distribution of $s_k\sim{}U(0,1)$.
-->
$0.4$ は、$s_k\sim{}U(0,1)$ の一様なサポート分布を持つ規則の標準偏差の平均です。

<!--
We combine both types of features to generate a new feature matrix and train a sparse linear model with Lasso, with the following structure:
-->
両方のタイプの特徴量を組み合わせて、新たな特徴量行列を作成し、次の形式で Lasso を利用してスパース線形モデルを学習します。

$$\hat{f}(x)=\hat{\beta}_0+\sum_{k=1}^K\hat{\alpha}_k{}r_k(x)+\sum_{j=1}^p\hat{\beta}_j{}l_j(x_j)$$

<!--
where $\hat{\alpha}$ is the estimated weight vector for the rule features and $\hat{\beta}$ the weight vector for the original features.
Since RuleFit uses Lasso, the loss function gets the additional constraint that forces some of the weights to get a zero estimate:
-->
ここで、$\hat{\alpha}$ は、規則の特徴量に対して推定された重みベクトルであり、$\hat{\beta}$ は、元の特徴量に対する重みベクトルです。
RuleFit は Lasso を利用するため、損失関数は、一部の重みを 0 にするための制約が必要になります。

$$(\{\hat{\alpha}\}_1^K,\{\hat{\beta}\}_0^p)=argmin_{\{\hat{\alpha}\}_1^K,\{\hat{\beta}\}_0^p}\sum_{i=1}^n{}L(y^{(i)},f(x^{(i)}))+\lambda\cdot\left(\sum_{k=1}^K|\alpha_k|+\sum_{j=1}^p|b_j|\right)$$

<!--
The result is a linear model that has linear effects for all of the original features and for the rules.
The interpretation is the same as for linear models, the only difference is that some features are now binary rules.
-->
この結果は、元の全ての特徴量と規則に対して線形な効果をもつ線形モデルです。
解釈は、線形モデルの場合と同様ですが、唯一の違いは、一部の特徴量が二値の規則となっている点です。

<!--**Step 3 (optional): Feature importance**-->

**Step3（optional）: 特徴量重要度**

<!--
For the linear terms of the original features, the feature importance is measured with the standardized predictor:
-->
元の特徴量の線形項については、標準化された予測器を利用して特徴量重要度を測定します。

$$I_j=|\hat{\beta}_j|\cdot{}std(l_j(x_j))$$

<!--
where $\beta_j$ is the weight from the Lasso model and $std(l_j(x_j))$ is the standard deviation of the linear term over the data.
-->
ここで、$\beta_j$ は、Lasso モデルから得られた重みであり、$std(l_j(x_j))$ はデータ全体の線形項の標準偏差です。

<!--For the decision rule terms, the importance is calculated with the following formula:-->
決定規則の項の場合、重要度は次式で計算されます。

$$I_k=|\hat{\alpha}_k|\cdot\sqrt{s_k(1-s_k)}$$

<!--
where $\hat{\alpha}_k$ is the associated Lasso weight of the decision rule and $s_k$ is the support of the feature in the data, which is the percentage of data points to which the decision rule applies (where $r_k(x)=1$):
-->
ここで、$\hat{\alpha}_k$ は、決定規則の関連するLassoの重みであり、$s_k$ は、データにおける特徴量のサポートであり、決定規則が適用されるデータの割合です（ここで、$r_k(x)=1$ ）。

$$s_k=\frac{1}{n}\sum_{i=1}^n{}r_k(x^{(i)})$$

<!--
A feature occurs as a linear term and possibly also within many decision rules.
How do we measure the total importance of a feature?
The importance $J_j(x)$ of a feature can be measured for each individual prediction:
-->
特徴量は線形項として現れるだけでなく、場合によっては多くの決定規則の内部にも現れます。
どのように特徴量の重要度を測るべきでしょうか？
特徴量の重要度 $J_j(x)$ は、個々の予測ごとに測定できます。

$$J_j(x)=I_j(x)+\sum_{x_j\in{}r_k}I_k(x)/m_k$$

<!--
where $I_l$ is the importance of the linear term and $I_k$ the importance of the decision rules in which $x_j$ appears, and $m_k$ is the number of features constituting the rule $r_k$.
Adding the feature importance from all instances gives us the global feature importance:
-->
ここで、$I_l$ は線形項の重要度、$I_k$ は $x_j$ が現れる決定規則の重要度、$m_k$ は規則 $r_k$ を構成する特徴量の数です。
全ての事例から特徴量の重要度を足し合わせることで、大域的な重要度を得ることができます。

$$J_j(X)=\sum_{i=1}^n{}J_j(x^{(i)})$$

<!--
It is possible to select a subset of instances and calculate the feature importance for this group.
-->
事例の部分集合を選択して、そのグループの特徴量重要度を計算できます。

<!--### Advantages-->
### 長所

<!--
RuleFit automatically adds **feature interactions** to linear models.
Therefore, it solves the problem of linear models that you have to add interaction terms manually and it helps a bit with the issue of modeling nonlinear relationships.
-->
RuleFitは**特徴量間の相互作用**を線形モデルに自動で追加します。
したがって、相互作用項を手動で追加する必要のある線形モデルの問題を解決し、非線形関係をモデリングする問題にも少し役立ちます。

<!--
RuleFit can handle both classification and regression tasks.
-->
RuleFit は分類問題と回帰問題の両方を扱えます。

<!--
The rules created are easy to interpret, because they are binary decision rules.
Either the rule applies to an instance or not.
Good interpretability is only guaranteed if the number of conditions within a rule is not too large.
A rule with 1 to 3 conditions seems reasonable to me.
This means a maximum depth of 3 for the trees in the tree ensemble.
-->
作成される決定規則は二値であるため、規則が観測データに適用されるかどうかを調べることで簡単に解釈できます。
優れた解釈可能性は、決定規則内の条件の数が多すぎない場合にのみ保証されます。
個人的には、1〜3 個の条件の決定規則が合理的だと思います。
つまり、アンサンブルの木の最大の深さは 3 が良いということです。

<!--
Even if there are many rules in the model, they do not apply to every instance.
For an individual instance only a handful of rules apply (= have a non-zero weights).
This improves local interpretability.
-->
たとえモデルに多くの決定規則がある場合でも、それらがすべての観測データに適用されるわけではありません。
個々の観測データにはほんのひと握りの決定規則のみ（= 非ゼロの重みを持つ）が適用されます。
これにより、個々のデータに対する解釈可能性が向上します。

<!--
RuleFit proposes a bunch of useful diagnostic tools. 
These tools are model-agnostic, so you can find them in the model-agnostic section of the book: [feature importance](#feature-importance),  [partial dependence plots](#pdp) and [feature interactions](#interaction).
-->
RuleFitは便利な診断ツールを多数提供しています。
これらのツールはモデルに依存しないため、この本のモデル非依存 (model-agnostic) のセクションで紹介されています：[特徴量重要度](#feature-importance)、[partial dependence plots](#pdp)、[特徴量の相互作用](#interaction)。

<!--### Disadvantages-->
### 短所

<!--
Sometimes RuleFit creates many rules that get a non-zero weight in the Lasso model.
The interpretability degrades with increasing number of features in the model.
A promising solution is to force feature effects to be monotonic, meaning that an increase of a feature has to lead to an increase of the prediction.
-->
RuleFit は、Lasso モデルにおいて非ゼロな重みを得るたくさんの規則を作り出すことがあります。
解釈性は特徴量の数が増えるにつれ低下します。
有望な解決策としては特徴量の影響を単調にすることです。
つまり、特徴量が増加すると、予測結果も増加する必要があるということです。

<!--
An anecdotal drawback: The papers claim a good performance of RuleFit -- often close to the predictive performance of random forests! -- but in the few cases where I tried it personally, the performance was disappointing.
Just try it out for your problem and see how it performs.
-->
論文では度々 RuleFit の性能が、ランダムフォレストの予測性能に匹敵するほど良いと主張しています。
しかしながら、私が個人的に試したいくつかの場合において、がっかりするような性能でした。
まず、適用してみてどのような性能が出るかを確認しましょう。

<!--
The end product of the RuleFit procedure is a linear model with additional fancy features (the decision rules).
But since it is a linear model, the weight interpretation is still unintuitive.
It comes with the same "footnote" as a usual linear regression model:
"... given all features are fixed."
It gets a bit more tricky when you have overlapping rules.
For example, one decision rule (feature) for the bicycle prediction could be: "temp > 10" and another rule could be "temp > 15 & weather='GOOD'".
If the weather is good and the temperature is above 15 degrees, the temperature is automatically greater then 10.
In the cases where the second rule applies, the first rule applies as well.
The interpretation of the estimated weight for the second rule is:
"Assuming all other features remain fixed, the predicted number of bikes increases by $\beta_2$ when the weather is good and temperature above 15 degrees.".
But, now it becomes really clear that the 'all other feature fixed' is problematic, because if rule 2 applies, also rule 1 applies and the interpretation is nonsensical.
-->
RuleFit の手順をふんで得られる最終生成物は、追加の特徴（決定規則）を持つ線形モデルです。
しかし、線形モデルであるからこそ、重みの解釈が直感的ではありません。
通常の線形回帰モデルと同様に、"...他の全ての特徴量が固定されている場合に限る。"という"脚注"がついています。
また、規則が重複していると少し厄介になります。
例えば、自転車の数を予測するための1つの決定規則（特徴量）として "temp > 10" と "temp > 15 & weather='GOOD'" があるとします。
天気が良く、気温が15度以上であれば、自動的に気温が10度以上になります。
2つ目の規則が満たされているときに、1つ目の規則も満たされています。
2つ目の規則における推測された重みの解釈は"他の特徴量が固定され、天気が良く、気温が15度以上のとき、予測される自転車の数は $\beta_2$ 増加する。"となります。
しかしここで、"他の特徴量が固定された場合"というのが問題になってきます。
なぜなら、2つ目の規則が適合しているとき、1つ目の規則にも適合し、解釈が意味の無いものになってしまうからです。

<!--### Software and Alternative-->
### ソフトウェアと代替手法

<!--
The RuleFit algorithm is implemented in R by Fokkema and Christoffersen (2017)[^Fokkema] and you can find a [Python version on Github](https://github.com/christophM/rulefit).
-->
RuleFit アルゴリズムは R では Fokkema と Christoffersen (2017)[^Fokkema] によって実装されています。
Python 実装は [Github](https://github.com/christophM/rulefit) 上にもあります。

<!--
A very similar framework is [skope-rules](https://github.com/scikit-learn-contrib/skope-rules), a Python module that also extracts rules from ensembles.
It differs in the way it learns the final rules:
First, skope-rules remove low-performing rules, based on recall and precision thresholds.
Then, duplicate and similar rules are removed by performing a selection based on the diversity of logical terms (variable + larger/smaller operator) and performance (F1-score) of the rules.
This final step does not rely on using Lasso, but considers only the out-of-bag F1-score and the logical terms which form the rules.
-->
非常によく似たフレームワークは [skope-rules](https://github.com/scikit-learn-contrib/skope-rules) という Python のモジュールでアンサンブルから規則を抽出します。
これは最終的な規則を学習する方法が違います。
まず、skope-rules はパフォーマンスのよくない規則を、recall（再現性）とprecision（適合率）に基づいて除去します。
そして、重複あるいは似ている規則が、論理項（変数 + 大なり／小なり）の多様性や F1-score に基づいて除去します。
最後に Lasso を用いる代わりに、out-of-bag の F1-score や規則を構成する論理項を用います。

[^Friedman2008]: Friedman, Jerome H, and Bogdan E Popescu. "Predictive learning via rule ensembles." The Annals of Applied Statistics. JSTOR, 916–54. (2008).

[^Fokkema]: Fokkema, Marjolein, and Benjamin Christoffersen.  "Pre: Prediction rule ensembles". https://CRAN.R-project.org/package=pre (2017).
